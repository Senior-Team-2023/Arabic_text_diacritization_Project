{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":27137,"status":"ok","timestamp":1704154451892,"user":{"displayName":"Mark Test","userId":"07605231817608634761"},"user_tz":-120},"id":"4FmiJ0pSimk2","outputId":"95c06d80-e4b7-4fe0-dafe-b98df18b7971"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":7,"status":"ok","timestamp":1704154451892,"user":{"displayName":"Mark Test","userId":"07605231817608634761"},"user_tz":-120},"id":"8Lgy0bfSiqLu","outputId":"302bc820-fa03-4ce1-81b8-7b7548dd9216"},"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/.shortcut-targets-by-id/1zPjf1cHfdKqObemkPReffGbQHU_wotr2/NLP_Project\n"]}],"source":["%cd ./drive/MyDrive/Colab\\ Notebooks/NLP_Project/"]},{"cell_type":"code","execution_count":3,"metadata":{"id":"WR6a6DkN0d-3","executionInfo":{"status":"ok","timestamp":1704154457178,"user_tz":-120,"elapsed":5288,"user":{"displayName":"Mark Test","userId":"07605231817608634761"}}},"outputs":[],"source":["import os\n","import numpy as np\n","import pandas as pd\n","from tqdm import tqdm\n","import torch\n","from torch import nn\n","import random as rnd\n","from torch.optim.lr_scheduler import StepLR\n","import gc"]},{"cell_type":"code","source":["# import pandas as pd\n","\n","# # Replace these file paths with the actual paths to your CSV files\n","# file1_path = './Results/test_set_without_labels.csv'\n","# file2_path = './Results/result_compare_model_baseline.csv'\n","\n","# # Load CSV files into DataFrames\n","# df1 = pd.read_csv(file1_path)\n","# df2 = pd.read_csv(file2_path)\n","\n","# # Check if \"letter\" column exists in both DataFrames\n","# if 'letter' not in df1.columns:\n","#     print('Error: \"letter\" column not found in the first CSV file.')\n","#     exit()\n","\n","# if 'input' not in df2.columns:\n","#     print('Error: \"input\" column not found in the second CSV file.')\n","#     exit()\n","# C = 0\n","# # Compare \"letter\" column in the first CSV with \"input\" column in the second CSV using a for loop\n","# mismatched_indices = []\n","# for idx, letter_value in enumerate(df1['letter']):\n","#     # if df2['input'][idx] == df2['input'][13994]:\n","#     #   C+=1\n","#     if df1['letter'][idx] != df2['input'][idx]:\n","#       print(\"err in index :\", idx,\" & line : \", df1['line_number'][idx])\n","#       print(\"'\",df1['letter'][idx - 4] ,\"' --- '\", df2['input'][idx - 4],\"'\")\n","#       print(\"'\",df1['letter'][idx - 3] ,\"' --- '\", df2['input'][idx - 3],\"'\")\n","#       print(\"'\",df1['letter'][idx - 2] ,\"' --- '\", df2['input'][idx - 2],\"'\")\n","#       print(\"'\",df1['letter'][idx - 1] ,\"' --- '\", df2['input'][idx - 1],\"'\")\n","#       print(\"'\",df1['letter'][idx] ,\"' --- '\", df2['input'][idx],\"'\")\n","#       print(\"'\",df1['letter'][idx + 1] ,\"' --- '\", df2['input'][idx + 1],\"'\")\n","#       print(\"'\",df1['letter'][idx + 2] ,\"' --- '\", df2['input'][idx + 2],\"'\")\n","#       print(\"'\",df1['letter'][idx + 3] ,\"' --- '\", df2['input'][idx + 3],\"'\")\n","#       print(\"'\",df1['letter'][idx + 4] ,\"' --- '\", df2['input'][idx + 4],\"'\")\n","#       print(\"'\",df1['letter'][idx + 5] ,\"' --- '\", df2['input'][idx + 5],\"'\")\n","#       print(\"'\",df1['letter'][idx + 6] ,\"' --- '\", df2['input'][idx + 6],\"'\")\n","#       print(\"'\",df1['letter'][idx + 7] ,\"' --- '\", df2['input'][idx + 7],\"'\")\n","#       print(\"'\",df1['letter'][idx + 8] ,\"' --- '\", df2['input'][idx + 8],\"'\")\n","#       break\n","#     # for idx_in, input_value in enumerate(df1['input']):\n","\n","#       # if letter_value not in df2['input'].values:\n","#       #     mismatched_indices.append(idx)\n","\n","\n","# print(C)"],"metadata":{"id":"n0oM1zovpvga","executionInfo":{"status":"ok","timestamp":1704154457179,"user_tz":-120,"elapsed":14,"user":{"displayName":"Mark Test","userId":"07605231817608634761"}}},"execution_count":4,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"alY6U9G-dBSM"},"source":["# Constants"]},{"cell_type":"code","execution_count":5,"metadata":{"id":"CL1Yi6fYdAyA","executionInfo":{"status":"ok","timestamp":1704154457179,"user_tz":-120,"elapsed":13,"user":{"displayName":"Mark Test","userId":"07605231817608634761"}}},"outputs":[],"source":["MODEL = \"BI_LSTM\"\n","NUM_LAYERS = 2\n","EMBEDDING_SIZE = 300"]},{"cell_type":"code","execution_count":6,"metadata":{"id":"jS2AXWkhigQt","executionInfo":{"status":"ok","timestamp":1704154457179,"user_tz":-120,"elapsed":13,"user":{"displayName":"Mark Test","userId":"07605231817608634761"}}},"outputs":[],"source":["def get_vocab(vocab_path, tags_path):\n","    vocab = {}\n","    with open(vocab_path) as f:\n","        for i, l in enumerate(f.read().splitlines()):\n","            vocab[l] = i  # to avoid the 0\n","        # loading tags (we require this to map tags to their indices)\n","    vocab['<PAD>'] = len(vocab) # 35180\n","    tag_map = {}\n","    with open(tags_path) as f:\n","        for i, t in enumerate(f.read().splitlines()):\n","            tag_map[t] = i\n","\n","    return vocab, tag_map\n","\n","def get_params(vocab, tag_map, sentences_file, labels_file):\n","    sentences = []\n","    labels = []\n","\n","    with open(sentences_file) as f:\n","        for sentence in f.read().splitlines():\n","            # replace each token by its index if it is in vocab\n","            # else use index of UNK_WORD\n","            s = [vocab[token] if token in vocab\n","                 else vocab['UNK']\n","                 for token in sentence.split(' ')]\n","            sentences.append(s)\n","\n","    with open(labels_file) as f:\n","        for sentence in f.read().splitlines():\n","            # replace each label by its index\n","            s = sentence.split(' ')\n","            # remove empty strings\n","            s = list(filter(None, s))\n","            l = [tag_map[label] for label in s] # I added plus 1 here\n","            labels.append(l)\n","    return sentences, labels, len(sentences)\n"]},{"cell_type":"markdown","metadata":{"id":"_44BK5K82YwF"},"source":["# Importing and discovering the data"]},{"cell_type":"code","execution_count":7,"metadata":{"id":"ulSik2Sv1p1G","executionInfo":{"status":"ok","timestamp":1704154467833,"user_tz":-120,"elapsed":10666,"user":{"displayName":"Mark Test","userId":"07605231817608634761"}}},"outputs":[],"source":["\n","vocab, tag_map = get_vocab('./Dataset/new_new_characters/unique_chars.txt', './Dataset/new_new_characters/unique_labels.txt')\n","t_sentences, t_labels, t_size = get_params(vocab, tag_map, './Dataset/new_new_characters/t_chars.txt', './Dataset/new_new_characters/t_labels.txt')\n","v_sentences, v_labels, v_size = get_params(vocab, tag_map, './Dataset/new_new_characters/v_chars.txt', './Dataset/new_new_characters/v_labels.txt')\n","test_sentences1, test_labels1, test_size1 = get_params(vocab, tag_map, './Dataset/new_new_characters/test_chars.txt', './Dataset/new_new_characters/test_labels.txt')"]},{"cell_type":"code","source":["test_sentences2, test_labels2, test_size2 = get_params(vocab, tag_map, './Dataset/new_new_characters/test_no_diacritics_chars.txt', './Dataset/new_new_characters/test_no_diacritics_labels.txt')\n","test_sentences3, test_labels3, test_size3 = get_params(vocab, tag_map, './Dataset/new_new_characters/test2_chars.txt', './Dataset/new_new_characters/test2_labels.txt')"],"metadata":{"id":"Ae4mAnXHdrIJ","executionInfo":{"status":"ok","timestamp":1704154471303,"user_tz":-120,"elapsed":3482,"user":{"displayName":"Mark Test","userId":"07605231817608634761"}}},"execution_count":8,"outputs":[]},{"cell_type":"code","execution_count":9,"metadata":{"id":"b9NQB6k9h_Ig","executionInfo":{"status":"ok","timestamp":1704154471304,"user_tz":-120,"elapsed":16,"user":{"displayName":"Mark Test","userId":"07605231817608634761"}}},"outputs":[],"source":["# # NOTE: to increase the size of the dataset\n","# t_sentences  = t_sentences + v_sentences\n","# t_labels = t_labels + v_labels\n","# t_size = t_size + v_size"]},{"cell_type":"markdown","metadata":{"id":"wt3e4nxjFT3O"},"source":["# NERDataset\n","The class that impelements the dataset for NER"]},{"cell_type":"code","execution_count":10,"metadata":{"id":"29iM0u4-4YOV","executionInfo":{"status":"ok","timestamp":1704154471304,"user_tz":-120,"elapsed":16,"user":{"displayName":"Mark Test","userId":"07605231817608634761"}}},"outputs":[],"source":["class NERDataset(torch.utils.data.Dataset):\n","\n","  def __init__(self, x, y, pad):\n","    self.x = nn.utils.rnn.pad_sequence([torch.tensor(i) for i in x], padding_value=pad,batch_first = True)\n","    self.y = nn.utils.rnn.pad_sequence([torch.tensor(i) for i in y], padding_value=tag_map[\"pad\"],batch_first = True)\n","    print('The max length of the sentences is', self.x.shape[1])\n","    print('The max length of the labels is', self.y.shape[1])\n","  def __len__(self):\n","    return len(self.x)\n","\n","  def __getitem__(self, idx):\n","    return self.x[idx], self.y[idx]"]},{"cell_type":"markdown","metadata":{"id":"CQB6O7I7FbUh"},"source":["# Classifiers\n","The class that implementss the pytorch model for arabic diacritic classification"]},{"cell_type":"code","execution_count":11,"metadata":{"id":"xHeJcz1JuhYa","executionInfo":{"status":"ok","timestamp":1704154471304,"user_tz":-120,"elapsed":15,"user":{"displayName":"Mark Test","userId":"07605231817608634761"}}},"outputs":[],"source":["class ArabicDiacriticsClassifier(nn.Module):\n","  def __init__(self, vocab_size=len(t_sentences) + len(v_sentences) + len(v_sentences), num_layers = 3, embedding_dim = 512, hidden_size=256, n_classes=len(tag_map)):\n","    super(ArabicDiacriticsClassifier, self).__init__()\n","    # (1) Create the embedding layer\n","    self.embedding = nn.Embedding(vocab_size, embedding_dim)\n","    # import gensim.downloader as api\n","    # self.embedding.weight.data.copy_(torch.from_numpy(api.load('word2vec-google-news-300').vectors[:1000])) WORD2VEc\n","    # import fasttext.util\n","    # self.embedding.weight.data.copy_(torch.from_numpy(fasttext.util.download_model('en', if_exists='ignore').get_input_matrix()[:1000]))\n","\n","    # (2) Create an LSTM layer with hidden size = hidden_size and batch_first = True\n","    self.num_layers = num_layers\n","    self.hidden_size = hidden_size\n","\n","\n","    self.lstm = nn.LSTM(embedding_dim, hidden_size,num_layers, batch_first=True, bidirectional=True)\n","    self.linear = nn.Linear(2 * hidden_size, n_classes)\n","\n","    # (3) Create a linear layer with number of neorons = n_classes\n","    # self.linear = nn.Linear(hidden_size, n_classes)\n","\n","\n","  def forward(self, sentences):\n","    embeddings = self.embedding(sentences)\n","\n","    # BIDIRECTIONAL\n","    # Initialize hidden states for bidirectional LSTM\n","    # h0 = torch.zeros(self.num_layers*2, embeddings.size(0), self.hidden_size).to(sentences.device)\n","    # c0 = torch.zeros(self.num_layers*2, embeddings.size(0), self.hidden_size).to(sentences.device)\n","    # lstm_out, (a, b) = self.lstm(embeddings, (h0, c0))\n","\n","    # LSTM\n","    lstm1_out, (a,b) = self.lstm(embeddings)\n","    final_output = self.linear(lstm1_out)\n","    # final_output = self.linear(lstm_out[:, -1, :])\n","    return final_output"]},{"cell_type":"code","execution_count":12,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":15,"status":"ok","timestamp":1704154471304,"user":{"displayName":"Mark Test","userId":"07605231817608634761"},"user_tz":-120},"id":"lJJJF-qQA_wk","outputId":"d7cca685-9cb8-4628-e55f-fad854474d8a"},"outputs":[{"output_type":"stream","name":"stdout","text":["ArabicDiacriticsClassifier(\n","  (embedding): Embedding(128137, 512)\n","  (lstm): LSTM(512, 256, num_layers=3, batch_first=True, bidirectional=True)\n","  (linear): Linear(in_features=512, out_features=16, bias=True)\n",")\n"]}],"source":["model = ArabicDiacriticsClassifier()\n","print(model)"]},{"cell_type":"code","source":["test_sentences = test_sentences1\n","test_labels = test_labels1\n","print(len(test_sentences3))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1OFu3LEnjqor","executionInfo":{"status":"ok","timestamp":1704154631863,"user_tz":-120,"elapsed":3,"user":{"displayName":"Mark Test","userId":"07605231817608634761"}},"outputId":"b64116eb-fe93-4a63-c67e-80adde1738a4"},"execution_count":24,"outputs":[{"output_type":"stream","name":"stdout","text":["5770\n"]}]},{"cell_type":"code","execution_count":25,"metadata":{"id":"3BI7_ANkLf7G","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1704154632497,"user_tz":-120,"elapsed":635,"user":{"displayName":"Mark Test","userId":"07605231817608634761"}},"outputId":"bdc81f75-771f-47a3-e918-42e97a40cce2"},"outputs":[{"output_type":"stream","name":"stdout","text":["The max length of the sentences is 1936\n","The max length of the labels is 1936\n"]}],"source":["test_dataset = NERDataset(test_sentences, test_labels, vocab['<PAD>'])"]},{"cell_type":"code","source":["def load_model(model,model_name):\n","  model.load_state_dict(torch.load(f'./SavedModels/{model_name}'))\n","  return model\n","def load_baseline_epoch_model(model,model_name):\n","  model.load_state_dict(torch.load(f'./BaseLineModels/{model_name}'))\n","  return model"],"metadata":{"id":"b2Lj2SneWZUj","executionInfo":{"status":"ok","timestamp":1704154632497,"user_tz":-120,"elapsed":2,"user":{"displayName":"Mark Test","userId":"07605231817608634761"}}},"execution_count":26,"outputs":[]},{"cell_type":"code","source":["model_name = \"model_baseline_EPOCH6\"\n","# model_name = \"model_3_baseLine_batch256_lr0.001_embedding_512_epoch1\"\n","model = load_baseline_epoch_model(model, model_name)"],"metadata":{"id":"NtFqGvvgZVv-","executionInfo":{"status":"ok","timestamp":1704154707378,"user_tz":-120,"elapsed":6097,"user":{"displayName":"Mark Test","userId":"07605231817608634761"}}},"execution_count":34,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"TWJNO6mUXPRI"},"source":["# Evaluation"]},{"cell_type":"code","execution_count":35,"metadata":{"id":"Gz5mxUAJM1xS","executionInfo":{"status":"ok","timestamp":1704154707378,"user_tz":-120,"elapsed":14,"user":{"displayName":"Mark Test","userId":"07605231817608634761"}}},"outputs":[],"source":["diacritic_results = []\n","gold_results = []\n","test_input_list = []\n","\n","def evaluate(model, test_dataset, batch_size=64):\n","  # (1) create the test data loader\n","  test_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size)\n","\n","  # GPU Configuration\n","  use_cuda = torch.cuda.is_available()\n","  device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n","  if use_cuda:\n","    model = model.cuda()\n","\n","  total_acc_test = 0\n","\n","\n","  # (2) disable gradients\n","  with torch.no_grad():\n","\n","    for test_input, test_label in tqdm(test_dataloader):\n","      # (3) move the test input to the device\n","      test_label = test_label.to(device)\n","\n","      # (4) move the test label to the device\n","      test_input = test_input.to(device)\n","\n","      # (5) do the forward pass\n","      output = model(test_input)\n","      prediction = output.argmax(2)\n","\n","\n","      diacritic_results.extend(np.array(prediction.cpu().data).flatten())\n","      gold_results.extend(np.array(test_label.cpu().data).flatten())\n","      test_input_list.extend(np.array(test_input.cpu().data).flatten())"]},{"cell_type":"markdown","metadata":{"id":"DMsZQ4ITWAEl"},"source":["### Evaluatio and Save"]},{"cell_type":"code","execution_count":36,"metadata":{"id":"bHY019YvKejN","executionInfo":{"status":"ok","timestamp":1704154707378,"user_tz":-120,"elapsed":13,"user":{"displayName":"Mark Test","userId":"07605231817608634761"}}},"outputs":[],"source":["def DER():\n","  der = 0\n","  total_size = 0\n","  for i in range(len(diacritic_results)):\n","    if test_input_list[i] != vocab['<PAD>']: # Do not include padding in DER calculations\n","      if diacritic_results[i] != gold_results[i] : # Miss Classification\n","        der += 1\n","      total_size += 1\n","  der /= total_size\n","  der *= 100\n","  print(\"DER = \",der,\"%\")\n","  print(\"Accuracy = \",100 - der,\"%\")"]},{"cell_type":"code","execution_count":37,"metadata":{"id":"LLtlWNm6YVG2","executionInfo":{"status":"ok","timestamp":1704154707378,"user_tz":-120,"elapsed":13,"user":{"displayName":"Mark Test","userId":"07605231817608634761"}}},"outputs":[],"source":["filtered_diacritic_results = [] # diacrtic results without paddings\n","filtered_inputs = [] # inputs without paddings\n","def PerpareForExportToCSV():\n","  # Prepare the data that will be written in the CSV file\n","  # these list are sorted as mentioned by the TA\n","  LIST_OF_DIACRITICS = [\n","      \"FATHA\",\n","      \"FATHATAN\",\n","      \"DAMMA\",\n","      \"DAMMATAN\",\n","      \"KASRA\",\n","      \"KASRATAN\",\n","      \"SUKUN\",\n","      \"SHADDA\",\n","      \"SHADDA_FATHA\",\n","      \"SHADDA_FATHATAN\",\n","      \"SHADDA_DAMMA\",\n","      \"SHADDA_DAMMATAN\",\n","      \"SHADDA_KASRA\",\n","      \"SHADDA_KASRATAN\",\n","      \"_\"\n","  ]\n","  LIST_OF_ARABIC_LETTERS = list(vocab.keys())\n","\n","\n","\n","  for i in range(len(diacritic_results)):\n","    if test_input_list[i] != vocab['<PAD>']:\n","      filtered_diacritic_results.append(diacritic_results[i])\n","      filtered_inputs.append(test_input_list[i])\n","\n","  index = len(filtered_diacritic_results)\n","\n","  inputs = [LIST_OF_ARABIC_LETTERS[filtered_inputs[i]] for i in range(index)]\n","  model_prediction = [LIST_OF_DIACRITICS[filtered_diacritic_results[i]] for i in range(index)]"]},{"cell_type":"code","source":["def ExportToCSV(model_name):\n","  data_length = len(filtered_diacritic_results)\n","  assert data_length == 417359, f\"Expected data length to be 417359, but got {data_length}.\"\n","  df = pd.DataFrame(\n","      {\n","      'ID': range(len(filtered_diacritic_results[0: data_length])),\n","      'label': filtered_diacritic_results[0: data_length],\n","      })\n","\n","  df.to_csv(f'./Results/result_{model_name}.csv', index=False)"],"metadata":{"id":"vzd36snRjME9","executionInfo":{"status":"ok","timestamp":1704154707378,"user_tz":-120,"elapsed":12,"user":{"displayName":"Mark Test","userId":"07605231817608634761"}}},"execution_count":38,"outputs":[]},{"cell_type":"code","source":["evaluate(model, test_dataset)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"88xgvIOwjJ9J","executionInfo":{"status":"ok","timestamp":1704154756083,"user_tz":-120,"elapsed":48717,"user":{"displayName":"Mark Test","userId":"07605231817608634761"}},"outputId":"14981012-7ca5-4c6c-97c7-565b1a2f22c8"},"execution_count":39,"outputs":[{"output_type":"stream","name":"stderr","text":["100%|██████████| 96/96 [00:48<00:00,  1.97it/s]\n"]}]},{"cell_type":"code","execution_count":40,"metadata":{"id":"GbOfNxV4b-eC","executionInfo":{"status":"ok","timestamp":1704154760069,"user_tz":-120,"elapsed":4000,"user":{"displayName":"Mark Test","userId":"07605231817608634761"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"50963300-889c-4fc7-8ddb-33076414fae2"},"outputs":[{"output_type":"stream","name":"stdout","text":["Calculating DER for our test set\n","DER =  3.018976760327245 %\n","Accuracy =  96.98102323967275 %\n"]}],"source":["if len(test_sentences2) != len(test_sentences): # If we are testing on our test sets\n","  print(\"Calculating DER for our test set\")\n","  DER()\n","else:\n","  print(\"Exporting to CSV ...\")\n","  model_name = f\"model_baseline\"\n","  PerpareForExportToCSV()\n","  ExportToCSV(model_name)\n","  print(\"Exported Successfully\")"]},{"cell_type":"code","source":[],"metadata":{"id":"JXIAp6PCiBjs"},"execution_count":null,"outputs":[]}],"metadata":{"accelerator":"GPU","colab":{"provenance":[],"gpuType":"T4"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.5"}},"nbformat":4,"nbformat_minor":0}