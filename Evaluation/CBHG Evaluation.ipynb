{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"id":"4FmiJ0pSimk2","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1704155112672,"user_tz":-120,"elapsed":16893,"user":{"displayName":"Mark Test","userId":"07605231817608634761"}},"outputId":"2d95f6a5-e646-4429-dec9-84536223f527"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8Lgy0bfSiqLu","executionInfo":{"status":"ok","timestamp":1704155113025,"user_tz":-120,"elapsed":360,"user":{"displayName":"Mark Test","userId":"07605231817608634761"}},"outputId":"892723ed-f80c-4628-dea5-ea2307849c5a"},"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/.shortcut-targets-by-id/1zPjf1cHfdKqObemkPReffGbQHU_wotr2/NLP_Project\n"]}],"source":["%cd ./drive/MyDrive/Colab\\ Notebooks/NLP_Project/"]},{"cell_type":"code","execution_count":3,"metadata":{"id":"WR6a6DkN0d-3","executionInfo":{"status":"ok","timestamp":1704155117711,"user_tz":-120,"elapsed":4689,"user":{"displayName":"Mark Test","userId":"07605231817608634761"}}},"outputs":[],"source":["import os\n","import numpy as np\n","import pandas as pd\n","from tqdm import tqdm\n","import torch\n","from torch import nn\n","import random as rnd\n","from torch.optim.lr_scheduler import StepLR\n","import gc"]},{"cell_type":"code","execution_count":4,"metadata":{"id":"WrtpcAxjScxO","executionInfo":{"status":"ok","timestamp":1704155117711,"user_tz":-120,"elapsed":3,"user":{"displayName":"Mark Test","userId":"07605231817608634761"}}},"outputs":[],"source":["# os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'max_split_size_mb:512'"]},{"cell_type":"markdown","metadata":{"id":"alY6U9G-dBSM"},"source":["# Constants"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CL1Yi6fYdAyA"},"outputs":[],"source":["MODEL = \"BI_LSTM\"\n","NUM_LAYERS = 2\n","EMBEDDING_SIZE = 300"]},{"cell_type":"code","execution_count":5,"metadata":{"id":"jS2AXWkhigQt","executionInfo":{"status":"ok","timestamp":1704155117711,"user_tz":-120,"elapsed":3,"user":{"displayName":"Mark Test","userId":"07605231817608634761"}}},"outputs":[],"source":["def get_vocab(vocab_path, tags_path):\n","    vocab = {}\n","    with open(vocab_path) as f:\n","        for i, l in enumerate(f.read().splitlines()):\n","            vocab[l] = i  # to avoid the 0\n","        # loading tags (we require this to map tags to their indices)\n","    vocab['<PAD>'] = len(vocab) # 35180\n","    tag_map = {}\n","    with open(tags_path) as f:\n","        for i, t in enumerate(f.read().splitlines()):\n","            tag_map[t] = i\n","\n","    return vocab, tag_map\n","\n","def get_params(vocab, tag_map, sentences_file, labels_file):\n","    sentences = []\n","    labels = []\n","\n","    with open(sentences_file) as f:\n","        for sentence in f.read().splitlines():\n","            # replace each token by its index if it is in vocab\n","            # else use index of UNK_WORD\n","            s = [vocab[token] if token in vocab\n","                 else vocab['UNK']\n","                 for token in sentence.split(' ')]\n","            sentences.append(s)\n","\n","    with open(labels_file) as f:\n","        for sentence in f.read().splitlines():\n","            # replace each label by its index\n","            s = sentence.split(' ')\n","            # remove empty strings\n","            s = list(filter(None, s))\n","            l = [tag_map[label] for label in s] # I added plus 1 here\n","            labels.append(l)\n","    return sentences, labels, len(sentences)\n"]},{"cell_type":"markdown","metadata":{"id":"_44BK5K82YwF"},"source":["# Importing and discovering the data"]},{"cell_type":"code","execution_count":6,"metadata":{"id":"ulSik2Sv1p1G","executionInfo":{"status":"ok","timestamp":1704155131578,"user_tz":-120,"elapsed":13869,"user":{"displayName":"Mark Test","userId":"07605231817608634761"}}},"outputs":[],"source":["vocab, tag_map = get_vocab('./Dataset/new_new_characters/unique_chars.txt', './Dataset/new_new_characters/unique_labels.txt')\n","t_sentences, t_labels, t_size = get_params(vocab, tag_map, './Dataset/new_new_characters/t_chars.txt', './Dataset/new_new_characters/t_labels.txt')\n","v_sentences, v_labels, v_size = get_params(vocab, tag_map, './Dataset/new_new_characters/v_chars.txt', './Dataset/new_new_characters/v_labels.txt')\n","test_sentences1, test_labels1, test_size1 = get_params(vocab, tag_map, './Dataset/new_new_characters/test_chars.txt', './Dataset/new_new_characters/test_labels.txt')\n","test_sentences2, test_labels2, test_size2 = get_params(vocab, tag_map, './Dataset/new_new_characters/test_no_diacritics_chars.txt', './Dataset/new_new_characters/test_no_diacritics_labels.txt')\n","test_sentences3, test_labels3, test_size3 = get_params(vocab, tag_map, './Dataset/new_new_characters/test2_chars.txt', './Dataset/new_new_characters/test2_labels.txt')"]},{"cell_type":"markdown","metadata":{"id":"wt3e4nxjFT3O"},"source":["# NERDataset\n","The class that impelements the dataset for NER"]},{"cell_type":"code","execution_count":7,"metadata":{"id":"29iM0u4-4YOV","executionInfo":{"status":"ok","timestamp":1704155131581,"user_tz":-120,"elapsed":9,"user":{"displayName":"Mark Test","userId":"07605231817608634761"}}},"outputs":[],"source":["class NERDataset(torch.utils.data.Dataset):\n","\n","  def __init__(self, x, y, pad):\n","    self.x = nn.utils.rnn.pad_sequence([torch.tensor(i) for i in x], padding_value=pad,batch_first = True)\n","    self.y = nn.utils.rnn.pad_sequence([torch.tensor(i) for i in y], padding_value=tag_map[\"pad\"],batch_first = True)\n","    print('The max length of the sentences is', self.x.shape[1])\n","    print('The max length of the labels is', self.y.shape[1])\n","  def __len__(self):\n","    return len(self.x)\n","\n","  def __getitem__(self, idx):\n","    return self.x[idx], self.y[idx]"]},{"cell_type":"markdown","metadata":{"id":"CQB6O7I7FbUh"},"source":["# Classifiers\n","The class that implementss the pytorch model for arabic diacritic classification"]},{"cell_type":"code","execution_count":8,"metadata":{"id":"o6t7yYNKZpmT","executionInfo":{"status":"ok","timestamp":1704155131581,"user_tz":-120,"elapsed":8,"user":{"displayName":"Mark Test","userId":"07605231817608634761"}}},"outputs":[],"source":["import torch\n","from torch import nn\n","from typing import Any\n","\n","\n","class BatchNormConv1d(nn.Module):\n","    \"\"\"\n","    A nn.Conv1d followed by an optional activation function, and nn.BatchNorm1d\n","    \"\"\"\n","\n","    def __init__(\n","        self,\n","        in_dim: int,\n","        out_dim: int,\n","        kernel_size: int,\n","        stride: int,\n","        padding: int,\n","        activation: Any = None,\n","    ):\n","        super().__init__()\n","        self.conv1d = nn.Conv1d(\n","            in_dim,\n","            out_dim,\n","            kernel_size=kernel_size,\n","            stride=stride,\n","            padding=padding,\n","            bias=False,\n","        )\n","        self.bn = nn.BatchNorm1d(out_dim)\n","        self.activation = activation\n","\n","    def forward(self, x: Any):\n","        x = self.conv1d(x)\n","        if self.activation is not None:\n","            x = self.activation(x)\n","        return self.bn(x)\n","\n","\n","class LinearNorm(torch.nn.Module):\n","    def __init__(self, in_dim, out_dim, bias=True, w_init_gain='linear'):\n","        super().__init__()\n","        self.linear_layer = torch.nn.Linear(in_dim, out_dim, bias=bias)\n","\n","        torch.nn.init.xavier_uniform_(\n","            self.linear_layer.weight,\n","            gain=torch.nn.init.calculate_gain(w_init_gain))\n","\n","    def forward(self, x):\n","        return self.linear_layer(x)\n","\n","\n","class ConvNorm(torch.nn.Module):\n","    def __init__(self, in_channels, out_channels, kernel_size=1, stride=1,\n","                 padding=None, dilation=1, bias=True, w_init_gain='linear'):\n","        super().__init__()\n","        if padding is None:\n","            assert(kernel_size % 2 == 1)\n","            padding = int(dilation * (kernel_size - 1) / 2)\n","\n","        self.conv = torch.nn.Conv1d(in_channels, out_channels,\n","                                    kernel_size=kernel_size, stride=stride,\n","                                    padding=padding, dilation=dilation,\n","                                    bias=bias)\n","\n","        torch.nn.init.xavier_uniform_(\n","            self.conv.weight, gain=torch.nn.init.calculate_gain(w_init_gain))\n","\n","    def forward(self, signal):\n","        conv_signal = self.conv(signal)\n","        return conv_signal\n"]},{"cell_type":"code","execution_count":9,"metadata":{"id":"xHeJcz1JuhYa","executionInfo":{"status":"ok","timestamp":1704155131581,"user_tz":-120,"elapsed":8,"user":{"displayName":"Mark Test","userId":"07605231817608634761"}}},"outputs":[],"source":["\"\"\"\n","Some custom modules that are used by the TTS model\n","\"\"\"\n","from typing import List\n","import torch\n","from torch import nn\n","\n","\n","\n","class Prenet(nn.Module):\n","    \"\"\"\n","    A prenet is a collection of linear layers with dropout(0.5),\n","    and RELU activation function\n","    Args:\n","    config: the hyperparameters object\n","    in_dim (int): the input dim\n","    \"\"\"\n","\n","    def __init__(\n","        self, in_dim: int, prenet_depth: List[int] = [256, 128], dropout: int = 0.5\n","    ):\n","        \"\"\" Initializing the prenet module \"\"\"\n","        super().__init__()\n","        in_sizes = [in_dim] + prenet_depth[:-1]\n","        self.layers = nn.ModuleList(\n","            [\n","                nn.Linear(in_size, out_size)\n","                for (in_size, out_size) in zip(in_sizes, prenet_depth)\n","            ]\n","        )\n","        self.relu = nn.ReLU()\n","        self.dropout = nn.Dropout(dropout)\n","\n","    def forward(self, inputs: torch.Tensor):\n","        \"\"\"Calculate forward propagation\n","        Args:\n","        inputs (batch_size, seqLen): the inputs to the prenet, the input shapes could\n","        be different as it is being used in both encoder and decoder.\n","        Returns:\n","        Tensor: the output of  the forward propagation\n","        \"\"\"\n","        for linear in self.layers:\n","            inputs = self.dropout(self.relu(linear(inputs)))\n","        return inputs\n","\n","\n","class Highway(nn.Module):\n","    \"\"\"Highway Networks were developed by (Srivastava et al., 2015)\n","    to overcome the difficulty of training deep neural networks\n","    (https://arxiv.org/abs/1507.06228).\n","    Args:\n","    in_size (int): the input size\n","    out_size (int): the output size\n","    \"\"\"\n","\n","    def __init__(self, in_size, out_size):\n","        \"\"\"\n","        Initializing Highway networks\n","        \"\"\"\n","        super().__init__()\n","        self.H = nn.Linear(in_size, out_size)\n","        self.H.bias.data.zero_()\n","        self.T = nn.Linear(in_size, out_size)\n","        self.T.bias.data.fill_(-1)\n","        self.relu = nn.ReLU()\n","        self.sigmoid = nn.Sigmoid()\n","\n","    def forward(self, inputs: torch.Tensor):\n","        \"\"\"Calculate forward propagation\n","        Args:\n","        inputs (Tensor):\n","        \"\"\"\n","        H = self.relu(self.H(inputs))\n","        T = self.sigmoid(self.T(inputs))\n","        return H * T + inputs * (1.0 - T)\n","\n","\n","class CBHG(nn.Module):\n","    \"\"\"The CBHG module (1-D Convolution Bank + Highway network + Bidirectional GRU)\n","    was proposed by (Lee et al., 2017, https://www.aclweb.org/anthology/Q17-1026)\n","    for a character-level NMT model.\n","    It was adapted by (Wang et al., 2017) for building the Tacotron.\n","    It is used in both the encoder and decoder  with different parameters.\n","    \"\"\"\n","\n","    def __init__(\n","        self,\n","        in_dim: int,\n","        out_dim: int,\n","        K: int,\n","        projections: List[int],\n","        highway_layers: int = 4,\n","    ):\n","        \"\"\"Initializing the CBHG module\n","        Args:\n","        in_dim (int): the input size\n","        out_dim (int): the output size\n","        k (int): number of filters\n","        \"\"\"\n","        super().__init__()\n","\n","        self.in_dim = in_dim\n","        self.out_dim = out_dim\n","        self.relu = nn.ReLU()\n","        self.conv1d_banks = nn.ModuleList(\n","            [\n","                BatchNormConv1d(\n","                    in_dim,\n","                    in_dim,\n","                    kernel_size=k,\n","                    stride=1,\n","                    padding=k // 2,\n","                    activation=self.relu,\n","                )\n","                for k in range(1, K + 1)\n","            ]\n","        )\n","        self.max_pool1d = nn.MaxPool1d(kernel_size=2, stride=1, padding=1)\n","\n","        in_sizes = [K * in_dim] + projections[:-1]\n","        activations = [self.relu] * (len(projections) - 1) + [None]\n","        self.conv1d_projections = nn.ModuleList(\n","            [\n","                BatchNormConv1d(\n","                    in_size, out_size, kernel_size=3, stride=1, padding=1, activation=ac\n","                )\n","                for (in_size, out_size, ac) in zip(in_sizes, projections, activations)\n","            ]\n","        )\n","\n","        self.pre_highway = nn.Linear(projections[-1], in_dim, bias=False)\n","        self.highways = nn.ModuleList([Highway(in_dim, in_dim) for _ in range(4)])\n","\n","        self.gru = nn.GRU(in_dim, out_dim, 1, batch_first=True, bidirectional=True)\n","\n","    def forward(self, inputs, input_lengths=None):\n","        # (B, T_in, in_dim)\n","        x = inputs\n","        x = x.transpose(1, 2)\n","        T = x.size(-1)\n","\n","        # (B, in_dim*K, T_in)\n","        # Concat conv1d bank outputs\n","        x = torch.cat([conv1d(x)[:, :, :T] for conv1d in self.conv1d_banks], dim=1)\n","        assert x.size(1) == self.in_dim * len(self.conv1d_banks)\n","        x = self.max_pool1d(x)[:, :, :T]\n","\n","        for conv1d in self.conv1d_projections:\n","            x = conv1d(x)\n","\n","        # (B, T_in, in_dim)\n","        # Back to the original shape\n","        x = x.transpose(1, 2)\n","\n","        if x.size(-1) != self.in_dim:\n","            x = self.pre_highway(x)\n","\n","        # Residual connection\n","        x += inputs\n","        for highway in self.highways:\n","            x = highway(x)\n","\n","        if input_lengths is not None:\n","            x = nn.utils.rnn.pack_padded_sequence(x, input_lengths, batch_first=True)\n","\n","        # (B, T_in, in_dim*2)\n","        self.gru.flatten_parameters()\n","        outputs, _ = self.gru(x)\n","\n","        if input_lengths is not None:\n","            outputs, _ = nn.utils.rnn.pad_packed_sequence(outputs, batch_first=True)\n","\n","        return outputs\n"]},{"cell_type":"code","execution_count":10,"metadata":{"id":"znAS0rzpqM8f","executionInfo":{"status":"ok","timestamp":1704155131581,"user_tz":-120,"elapsed":8,"user":{"displayName":"Mark Test","userId":"07605231817608634761"}}},"outputs":[],"source":["\"\"\"\n","The CBHG model implementation\n","\"\"\"\n","from typing import List, Optional\n","\n","from torch import nn\n","import torch\n","\n","\n","\n","class CBHGModel(nn.Module):\n","    \"\"\"CBHG model implementation as described in the paper:\n","     https://ieeexplore.ieee.org/document/9274427\n","\n","    Args:\n","    inp_vocab_size (int): the number of the input symbols\n","    targ_vocab_size (int): the number of the target symbols (diacritics)\n","    embedding_dim (int): the embedding  size\n","    use_prenet (bool): whether to use prenet or not\n","    prenet_sizes (List[int]): the sizes of the prenet networks\n","    cbhg_gru_units (int): the number of units of the CBHG GRU, which is the last\n","    layer of the CBHG Model.\n","    cbhg_filters (int): number of filters used in the CBHG module\n","    cbhg_projections: projections used in the CBHG module\n","\n","    Returns:\n","    diacritics Dict[str, Tensor]:\n","    \"\"\"\n","\n","    def __init__(\n","        self,\n","        inp_vocab_size: int,\n","        targ_vocab_size: int,\n","        embedding_dim: int = 256,\n","        use_prenet: bool = True,\n","        prenet_sizes: List[int] = [256, 256],\n","        cbhg_gru_units: int = 256,\n","        cbhg_filters: int = 16,\n","        cbhg_projections: List[int] = [128, 256],\n","        post_cbhg_layers_units: List[int] = [256, 256],\n","        post_cbhg_use_batch_norm: bool = True\n","    ):\n","        super().__init__()\n","        self.use_prenet = use_prenet\n","        self.embedding = nn.Embedding(inp_vocab_size, embedding_dim)\n","        if self.use_prenet:\n","            self.prenet = Prenet(embedding_dim, prenet_depth=prenet_sizes)\n","\n","        self.cbhg = CBHG(\n","            prenet_sizes[-1] if self.use_prenet else embedding_dim,\n","            cbhg_gru_units,\n","            K=cbhg_filters,\n","            projections=cbhg_projections,\n","        )\n","\n","        layers = []\n","        post_cbhg_layers_units = [cbhg_gru_units] + post_cbhg_layers_units\n","\n","        for i in range(1, len(post_cbhg_layers_units)):\n","            layers.append(\n","                nn.LSTM(\n","                    post_cbhg_layers_units[i - 1] * 2,\n","                    post_cbhg_layers_units[i],\n","                    bidirectional=True,\n","                    batch_first=True,\n","                )\n","            )\n","            if post_cbhg_use_batch_norm:\n","                layers.append(nn.BatchNorm1d(post_cbhg_layers_units[i] * 2))\n","\n","        self.post_cbhg_layers = nn.ModuleList(layers)\n","        self.projections = nn.Linear(post_cbhg_layers_units[-1] * 2, targ_vocab_size)\n","        self.post_cbhg_layers_units = post_cbhg_layers_units\n","        self.post_cbhg_use_batch_norm = post_cbhg_use_batch_norm\n","\n","\n","    def forward(\n","        self,\n","        src: torch.Tensor,\n","        lengths: Optional[torch.Tensor] = None,\n","        target: Optional[torch.Tensor] = None,  # not required in this model\n","    ):\n","        \"\"\"Compute forward propagation\"\"\"\n","\n","        # src = [batch_size, src len]\n","        # lengths = [batch_size]\n","        # target = [batch_size, trg len]\n","\n","        embedding_out = self.embedding(src)\n","        # embedding_out; [batch_size, src_len, embedding_dim]\n","\n","        cbhg_input = embedding_out\n","        if self.use_prenet:\n","            cbhg_input = self.prenet(embedding_out)\n","\n","            # cbhg_input = [batch_size, src_len, prenet_sizes[-1]]\n","\n","        outputs = self.cbhg(cbhg_input, lengths)\n","\n","        hn = torch.zeros((2, 2, 2))\n","        cn = torch.zeros((2, 2, 2))\n","\n","        for i, layer in enumerate(self.post_cbhg_layers):\n","            if isinstance(layer, nn.BatchNorm1d):\n","                outputs = layer(outputs.permute(0, 2, 1))\n","                outputs = outputs.permute(0, 2, 1)\n","                continue\n","            if i > 0:\n","                outputs, (hn, cn) = layer(outputs, (hn, cn))\n","            else:\n","                outputs, (hn, cn) = layer(outputs)\n","\n","\n","        predictions = self.projections(outputs)\n","\n","        # predictions = [batch_size, src len, targ_vocab_size]\n","\n","        output = {\"diacritics\": predictions}\n","\n","        return output\n"]},{"cell_type":"code","execution_count":22,"metadata":{"id":"lJJJF-qQA_wk","executionInfo":{"status":"ok","timestamp":1704155166568,"user_tz":-120,"elapsed":1353,"user":{"displayName":"Mark Test","userId":"07605231817608634761"}}},"outputs":[],"source":["model = CBHGModel(\n","      inp_vocab_size = len(t_sentences) + len(v_sentences),\n","      targ_vocab_size = len(tag_map),\n","        )"]},{"cell_type":"markdown","metadata":{"id":"PLHx_oHpFlSX"},"source":["# Training"]},{"cell_type":"code","source":["test_sentences = test_sentences3\n","test_labels = test_labels3\n","print(len(test_sentences3))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"SaCzOv51QNxw","executionInfo":{"status":"ok","timestamp":1704155166568,"user_tz":-120,"elapsed":3,"user":{"displayName":"Mark Test","userId":"07605231817608634761"}},"outputId":"1b28e2ce-f46a-4647-8dd4-a68d6d8ba0de"},"execution_count":23,"outputs":[{"output_type":"stream","name":"stdout","text":["5770\n"]}]},{"cell_type":"code","execution_count":24,"metadata":{"id":"3BI7_ANkLf7G","executionInfo":{"status":"ok","timestamp":1704155166861,"user_tz":-120,"elapsed":295,"user":{"displayName":"Mark Test","userId":"07605231817608634761"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"424aa957-672b-4f8d-9ed0-8897a112b743"},"outputs":[{"output_type":"stream","name":"stdout","text":["The max length of the sentences is 1234\n","The max length of the labels is 1234\n"]}],"source":["test_dataset = NERDataset(test_sentences, test_labels, vocab['<PAD>'])"]},{"cell_type":"code","source":["def load_model(model,model_name):\n","  model.load_state_dict(torch.load(f'./SavedModels/{model_name}'))\n","  return model"],"metadata":{"id":"wyNn2-huagI_","executionInfo":{"status":"ok","timestamp":1704155166862,"user_tz":-120,"elapsed":5,"user":{"displayName":"Mark Test","userId":"07605231817608634761"}}},"execution_count":25,"outputs":[]},{"cell_type":"code","source":["model_name = f\"model_cbhg_all_data_lr0.001_batch32_epoch5\"\n","model = load_model(model, model_name)"],"metadata":{"id":"_dG2XE7HaQ5K","executionInfo":{"status":"ok","timestamp":1704156708396,"user_tz":-120,"elapsed":2796,"user":{"displayName":"Mark Test","userId":"07605231817608634761"}}},"execution_count":33,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"TWJNO6mUXPRI"},"source":["# Evaluation"]},{"cell_type":"code","execution_count":34,"metadata":{"id":"Gz5mxUAJM1xS","executionInfo":{"status":"ok","timestamp":1704156708396,"user_tz":-120,"elapsed":7,"user":{"displayName":"Mark Test","userId":"07605231817608634761"}}},"outputs":[],"source":["diacritic_results = []\n","gold_results = []\n","test_input_list = []\n","\n","def evaluate(model, test_dataset, batch_size = 32):\n","  # (1) create the test data loader\n","  test_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size)\n","\n","  # GPU Configuration\n","  use_cuda = torch.cuda.is_available()\n","  device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n","  if use_cuda:\n","    model = model.cuda()\n","\n","  total_acc_test = 0\n","\n","\n","  # (2) disable gradients\n","  with torch.no_grad():\n","\n","    for test_input, test_label in tqdm(test_dataloader):\n","      # (3) move the test input to the device\n","      test_label = test_label.to(device)\n","\n","      # (4) move the test label to the device\n","      test_input = test_input.to(device)\n","\n","      # (5) do the forward pass\n","      output = model(test_input)\n","      prediction = output['diacritics'].argmax(2)\n","\n","\n","      diacritic_results.extend(np.array(prediction.cpu().data).flatten())\n","      gold_results.extend(np.array(test_label.cpu().data).flatten())\n","      test_input_list.extend(np.array(test_input.cpu().data).flatten())"]},{"cell_type":"markdown","metadata":{"id":"DMsZQ4ITWAEl"},"source":["### Evaluatio and Save"]},{"cell_type":"code","execution_count":35,"metadata":{"id":"bHY019YvKejN","executionInfo":{"status":"ok","timestamp":1704156708396,"user_tz":-120,"elapsed":6,"user":{"displayName":"Mark Test","userId":"07605231817608634761"}}},"outputs":[],"source":["def DER():\n","  der = 0\n","  total_size = 0\n","  for i in range(len(diacritic_results)):\n","    if test_input_list[i] != vocab['<PAD>']: # Do not include padding in DER calculations\n","      if diacritic_results[i] != gold_results[i] : # Miss Classification\n","        der += 1\n","      total_size += 1\n","  der /= total_size\n","  der *= 100\n","  print(\"DER = \",der,\"%\")\n","  print(\"Accuracy = \",100 - der,\"%\")"]},{"cell_type":"code","execution_count":36,"metadata":{"id":"LLtlWNm6YVG2","executionInfo":{"status":"ok","timestamp":1704156708397,"user_tz":-120,"elapsed":7,"user":{"displayName":"Mark Test","userId":"07605231817608634761"}}},"outputs":[],"source":["filtered_diacritic_results = [] # diacrtic results without paddings\n","filtered_inputs = [] # inputs without paddings\n","def PerpareForExportToCSV():\n","  # Prepare the data that will be written in the CSV file\n","  # these list are sorted as mentioned by the TA\n","  LIST_OF_DIACRITICS = [\n","      \"FATHA\",\n","      \"FATHATAN\",\n","      \"DAMMA\",\n","      \"DAMMATAN\",\n","      \"KASRA\",\n","      \"KASRATAN\",\n","      \"SUKUN\",\n","      \"SHADDA\",\n","      \"SHADDA_FATHA\",\n","      \"SHADDA_FATHATAN\",\n","      \"SHADDA_DAMMA\",\n","      \"SHADDA_DAMMATAN\",\n","      \"SHADDA_KASRA\",\n","      \"SHADDA_KASRATAN\",\n","      \"_\"\n","  ]\n","  LIST_OF_ARABIC_LETTERS = list(vocab.keys())\n","\n","\n","\n","  for i in range(len(diacritic_results)):\n","    if test_input_list[i] != vocab['<PAD>']:\n","      filtered_diacritic_results.append(diacritic_results[i])\n","      filtered_inputs.append(test_input_list[i])\n","\n","  index = len(filtered_diacritic_results)\n","\n","  inputs = [LIST_OF_ARABIC_LETTERS[filtered_inputs[i]] for i in range(index)]\n","  model_prediction = [LIST_OF_DIACRITICS[filtered_diacritic_results[i]] for i in range(index)]"]},{"cell_type":"code","source":["def ExportToCSV(model_name):\n","  data_length = len(filtered_diacritic_results)\n","  assert data_length == 417359, f\"Expected data length to be 417359, but got {data_length}.\"\n","  df = pd.DataFrame(\n","      {\n","      'ID': range(len(filtered_diacritic_results[0: data_length])),\n","      'label': filtered_diacritic_results[0: data_length],\n","      })\n","\n","  df.to_csv(f'./Results/result_{model_name}.csv', index=False)"],"metadata":{"id":"vzd36snRjME9","executionInfo":{"status":"ok","timestamp":1704156708397,"user_tz":-120,"elapsed":7,"user":{"displayName":"Mark Test","userId":"07605231817608634761"}}},"execution_count":37,"outputs":[]},{"cell_type":"code","source":["evaluate(model, test_dataset)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"88xgvIOwjJ9J","executionInfo":{"status":"ok","timestamp":1704156770869,"user_tz":-120,"elapsed":62479,"user":{"displayName":"Mark Test","userId":"07605231817608634761"}},"outputId":"3f194fa5-c50a-4d28-ba38-e6e2fae2049a"},"execution_count":38,"outputs":[{"output_type":"stream","name":"stderr","text":["100%|██████████| 181/181 [01:02<00:00,  2.89it/s]\n"]}]},{"cell_type":"code","execution_count":39,"metadata":{"id":"GbOfNxV4b-eC","executionInfo":{"status":"ok","timestamp":1704156771875,"user_tz":-120,"elapsed":1017,"user":{"displayName":"Mark Test","userId":"07605231817608634761"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"a3106e9a-30f9-435a-e00d-c339d8216d6f"},"outputs":[{"output_type":"stream","name":"stdout","text":["Calculating DER for our test set\n","DER =  3.366434809073372 %\n","Accuracy =  96.63356519092663 %\n"]}],"source":["if len(test_sentences2) != len(test_sentences): # If we are testing on our test sets\n","  print(\"Calculating DER for our test set\")\n","  DER()\n","else:\n","  print(\"Exporting to CSV ...\")\n","  model_name = f\"model_cbhg\"\n","  PerpareForExportToCSV()\n","  ExportToCSV(model_name)\n","  print(\"Exported Successfully\")"]}],"metadata":{"accelerator":"GPU","colab":{"provenance":[],"gpuType":"T4"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.5"}},"nbformat":4,"nbformat_minor":0}