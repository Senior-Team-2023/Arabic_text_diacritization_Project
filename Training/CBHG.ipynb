{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":16072,"status":"ok","timestamp":1704147673229,"user":{"displayName":"Bemoi E. Ayad","userId":"07844534677848232307"},"user_tz":-120},"id":"4FmiJ0pSimk2","outputId":"c4b8f242-2d65-4241-a668-99665e0d6909"},"outputs":[{"name":"stdout","output_type":"stream","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":7,"status":"ok","timestamp":1704147673230,"user":{"displayName":"Bemoi E. Ayad","userId":"07844534677848232307"},"user_tz":-120},"id":"8Lgy0bfSiqLu","outputId":"8ed28ad5-254f-4c11-a231-64b85b4e70f1"},"outputs":[{"name":"stdout","output_type":"stream","text":["/content/drive/.shortcut-targets-by-id/1zPjf1cHfdKqObemkPReffGbQHU_wotr2/NLP_Project\n"]}],"source":["%cd ./drive/MyDrive/Colab\\ Notebooks/NLP_Project/"]},{"cell_type":"markdown","metadata":{"id":"-F37reSNmk9u"},"source":["# Import Modules\n"]},{"cell_type":"code","execution_count":14,"metadata":{"id":"WR6a6DkN0d-3"},"outputs":[],"source":["import os\n","import numpy as np\n","import pandas as pd\n","from tqdm import tqdm\n","import torch\n","from torch import nn\n","import random as rnd\n","from typing import Any, List, Optional\n","from torch.optim.lr_scheduler import StepLR\n","import gc"]},{"cell_type":"code","execution_count":3,"metadata":{"id":"jS2AXWkhigQt"},"outputs":[],"source":["def get_vocab(vocab_path, tags_path):\n","    vocab = {}\n","    with open(vocab_path) as f:\n","        for i, l in enumerate(f.read().splitlines()):\n","            vocab[l] = i  # to avoid the 0\n","    # loading tags (we require this to map tags to their indices)\n","    vocab['<PAD>'] = len(vocab) # 35180\n","    tag_map = {}\n","    with open(tags_path) as f:\n","        for i, t in enumerate(f.read().splitlines()):\n","            tag_map[t] = i\n","\n","    return vocab, tag_map\n","\n","def get_params(vocab, tag_map, sentences_file, labels_file):\n","    sentences = []\n","    labels = []\n","\n","    with open(sentences_file) as f:\n","        for sentence in f.read().splitlines():\n","            # replace each token by its index if it is in vocab\n","            # else use index of UNK_WORD\n","            s = [vocab[token] if token in vocab\n","                 else vocab['UNK']\n","                 for token in sentence.split(' ')]\n","            sentences.append(s)\n","\n","    with open(labels_file) as f:\n","        for sentence in f.read().splitlines():\n","            # replace each label by its index\n","            s = sentence.split(' ')\n","            # remove empty strings\n","            s = list(filter(None, s))\n","            l = [tag_map[label] for label in s] # I added plus 1 here\n","            labels.append(l)\n","    return sentences, labels, len(sentences)"]},{"cell_type":"markdown","metadata":{"id":"_44BK5K82YwF"},"source":["# Importing and discovering the data\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ulSik2Sv1p1G"},"outputs":[],"source":["vocab, tag_map = get_vocab('./Dataset/new_new_characters/unique_chars.txt', './Dataset/new_new_characters/unique_labels.txt')\n","t_sentences, t_labels, t_size = get_params(vocab, tag_map, './Dataset/new_new_characters/t_chars.txt', './Dataset/new_new_characters/t_labels.txt')\n","v_sentences, v_labels, v_size = get_params(vocab, tag_map, './Dataset/new_new_characters/v_chars.txt', './Dataset/new_new_characters/v_labels.txt')\n","test_sentences, test_labels, test_size = get_params(vocab, tag_map, './Dataset/new_new_characters/test_chars.txt', './Dataset/new_new_characters/test_labels.txt')\n","test_sentences2, test_labels2, test_size2 = get_params(vocab, tag_map, './Dataset/new_new_characters/test2_chars.txt', './Dataset/new_new_characters/test2_labels.txt')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":30,"status":"ok","timestamp":1704147686136,"user":{"displayName":"Bemoi E. Ayad","userId":"07844534677848232307"},"user_tz":-120},"id":"oWLR2Oxp28K6","outputId":"5cc60569-f37f-49aa-f941-6a4a3131be1e"},"outputs":[{"name":"stdout","output_type":"stream","text":["The number of outputs is tag_map 16\n","Num of vocabulary words: 39\n","The vocab size is 39\n","The training size is 116323\n","The validation size is 5907\n","An example of the first sentence is [24, 17, 0, 33, 34, 17, 24, 13, 32, 18, 0, 34, 17, 0, 10, 25, 33, 27, 0, 9, 24, 18, 0, 18, 0, 11, 15, 4, 2, 10, 18, 5, 1, 32, 15, 26, 16, 24, 17, 0, 33, 5, 0, 26, 22, 10, 24, 29, 12, 10, 33, 4, 27, 1, 4, 18, 15, 19, 10, 15, 6, 25, 10, 30, 5, 18, 0, 27, 23, 0, 18, 31, 17, 14, 17, 5, 31, 18, 32, 0, 31, 17, 14, 17, 5, 33, 31, 1, 18, 0, 25, 10, 1, 12, 15, 17, 15, 16, 4, 27, 0, 24, 18, 21, 31, 7, 6, 26, 5, 24, 28, 15, 17, 2, 25, 11, 1, 18, 15, 18, 5, 1, 32, 15, 26, 16, 24, 17, 0, 18, 5, 1, 2, 18, 23, 34, 17, 5, 26, 32, 0, 10, 29, 12, 31, 1, 33, 33, 17, 4, 0, 5, 23, 18, 0, 11, 1, 18, 15, 17, 27, 0, 24, 18, 21, 18, 0, 31, 7, 6, 26, 26, 10, 7, 15, 10, 6, 18, 0, 1, 14, 18, 23, 16, 17, 18, 0, 23, 14, 17, 25, 0, 0, 7, 1, 31, 17, 1, 6, 17, 28, 0, 4, 17, 23, 6, 15, 31, 6, 31, 25, 24, 17, 0, 31, 18, 0, 4, 17, 34, 7, 6, 18, 5, 33, 34, 1, 18, 0, 23, 18, 6, 15, 4, 18, 26, 15, 5, 18, 0, 0, 33, 29, 32, 18, 0, 20, 24, 18, 0, 31, 18, 0, 4, 33, 17, 4, 18, 0, 11, 1, 25, 10, 24, 27, 28, 18, 32, 31, 0, 18, 0, 23, 6, 15, 5, 1, 26, 23, 33, 24, 29, 0, 17, 0, 31, 10, 23, 29, 29, 5]\n","An example of its corresponding label is [0, 6, 2, 2, 0, 6, 0, 0, 0, 14, 6, 0, 8, 2, 0, 0, 2, 14, 0, 6, 0, 14, 0, 14, 14, 8, 6, 0, 4, 10, 14, 6, 2, 0, 0, 0, 0, 0, 6, 2, 2, 4, 0, 6, 5, 0, 6, 0, 4, 14, 14, 0, 4, 6, 0, 14, 4, 0, 6, 4, 0, 4, 14, 5, 4, 14, 6, 4, 6, 0, 14, 4, 2, 2, 14, 0, 0, 14, 2, 4, 0, 2, 2, 14, 2, 2, 4, 6, 14, 14, 12, 14, 4, 0, 2, 14, 0, 1, 0, 4, 6, 0, 14, 4, 2, 6, 0, 5, 4, 0, 0, 5, 0, 0, 12, 2, 8, 14, 5, 14, 6, 2, 0, 0, 0, 0, 0, 6, 2, 14, 6, 4, 0, 14, 5, 0, 6, 4, 4, 6, 5, 0, 0, 0, 8, 2, 2, 2, 0, 0, 2, 6, 4, 14, 14, 10, 8, 14, 4, 0, 4, 6, 0, 14, 4, 14, 6, 2, 6, 0, 4, 4, 14, 0, 4, 14, 4, 14, 14, 8, 0, 14, 0, 4, 0, 14, 14, 10, 2, 14, 4, 4, 14, 8, 0, 4, 0, 0, 6, 4, 0, 4, 0, 0, 4, 6, 5, 2, 0, 8, 3, 0, 6, 2, 0, 14, 4, 5, 0, 0, 6, 0, 14, 4, 4, 0, 8, 14, 14, 8, 14, 4, 0, 0, 14, 4, 3, 4, 0, 14, 8, 4, 0, 0, 14, 0, 14, 0, 14, 0, 0, 14, 4, 3, 2, 0, 0, 14, 14, 12, 6, 4, 14, 4, 14, 0, 14, 0, 4, 0, 14, 14, 12, 6, 0, 4, 0, 6, 4, 4, 2, 4, 0, 0, 0, 6, 2, 6, 0, 0, 6]\n"]},{"data":{"text/plain":["True"]},"execution_count":12,"metadata":{},"output_type":"execute_result"}],"source":["# Exploring information about the data\n","print('The number of outputs is tag_map', len(tag_map))\n","# The number of vocabulary tokens (including <PAD>)\n","g_vocab_size = len(vocab)\n","print(f\"Num of vocabulary words: {g_vocab_size}\")\n","print('The vocab size is', len(vocab))\n","print('The training size is', t_size)\n","print('The validation size is', v_size)\n","print('An example of the first sentence is', t_sentences[0])\n","print('An example of its corresponding label is', t_labels[0])\n","len(t_sentences[0])==len( t_labels[0])"]},{"cell_type":"markdown","metadata":{"id":"wt3e4nxjFT3O"},"source":["# NERDataset\n","\n","The class that impelements the dataset for NER\n"]},{"cell_type":"code","execution_count":5,"metadata":{"id":"29iM0u4-4YOV"},"outputs":[],"source":["class NERDataset(torch.utils.data.Dataset):\n","\n","  def __init__(self, x, y, pad):\n","    self.x = nn.utils.rnn.pad_sequence([torch.tensor(i) for i in x], padding_value=pad,batch_first = True)\n","    self.y = nn.utils.rnn.pad_sequence([torch.tensor(i) for i in y], padding_value=tag_map[\"pad\"],batch_first = True)\n","    print('The max length of the sentences is', self.x.shape[1])\n","    print('The max length of the labels is', self.y.shape[1])\n","  def __len__(self):\n","    return len(self.x)\n","\n","  def __getitem__(self, idx):\n","    return self.x[idx], self.y[idx]"]},{"cell_type":"markdown","metadata":{"id":"CQB6O7I7FbUh"},"source":["# Classifiers\n","\n","The class that implementss the pytorch model for arabic diacritic classification\n"]},{"cell_type":"code","execution_count":15,"metadata":{"id":"xHeJcz1JuhYa"},"outputs":[],"source":["class BatchNormConv1d(nn.Module):\n","    \"\"\"\n","    A nn.Conv1d followed by an optional activation function, and nn.BatchNorm1d\n","    \"\"\"\n","    def __init__(\n","        self,\n","        input_dim: int,\n","        output_dim: int,\n","        kernel_size: int,\n","        stride: int,\n","        padding: int,\n","        activation: Any = None,\n","    ):\n","        super().__init__()\n","        self.conv1d = nn.Conv1d(\n","            input_dim,\n","            output_dim,\n","            kernel_size=kernel_size,\n","            stride=stride,\n","            padding=padding,\n","            bias=False,\n","        )\n","        self.bn = nn.BatchNorm1d(output_dim)\n","        self.activation = activation\n","\n","    def forward(self, x: Any):\n","        x = self.conv1d(x)\n","        if self.activation is not None:\n","            x = self.activation(x)\n","        return self.bn(x)\n","\n","class Prenet(nn.Module):\n","    \"\"\"\n","    A prenet is a collection of linear layers with dropout(0.5), and RELU activation function\n","    \"\"\"\n","    def __init__(self, input_dim: int, prenet_depth: List[int] = [256, 128], dropout: int = 0.5):\n","        super().__init__()\n","        in_sizes = [input_dim] + prenet_depth[:-1]\n","        self.layers = nn.ModuleList(\n","            [\n","                nn.Linear(in_size, out_size)\n","                for (in_size, out_size) in zip(in_sizes, prenet_depth)\n","            ]\n","        )\n","        self.relu = nn.ReLU()\n","        self.dropout = nn.Dropout(dropout)\n","\n","    def forward(self, inputs: torch.Tensor):\n","        for linear in self.layers:\n","            inputs = self.dropout(self.relu(linear(inputs)))\n","        return inputs\n","\n","\n","class Highway(nn.Module):\n","    \"\"\"\n","      To overcome the difficulty of training deep neural networks\n","    \"\"\"\n","    def __init__(self, in_size, out_size):\n","        super().__init__()\n","        self.H = nn.Linear(in_size, out_size)\n","        self.H.bias.data.zero_()\n","        self.T = nn.Linear(in_size, out_size)\n","        self.T.bias.data.fill_(-1)\n","        self.relu = nn.ReLU()\n","        self.sigmoid = nn.Sigmoid()\n","\n","    def forward(self, inputs: torch.Tensor):\n","        H = self.relu(self.H(inputs))\n","        T = self.sigmoid(self.T(inputs))\n","        return H * T + inputs * (1.0 - T)\n","\n","\n","class CBHG(nn.Module):\n","    \"\"\"\n","    The CBHG module (1-D Convolution Bank + Highway network + Bidirectional GRU)\n","    \"\"\"\n","    def __init__(\n","        self,\n","        input_dim: int,\n","        output_dim: int,\n","        K: int,\n","        projections: List[int],\n","    ):\n","        \"\"\"\n","        input_dim (int): the input size\n","        output_dim (int): the output size\n","        k (int): number of filters\n","        \"\"\"\n","        super().__init__()\n","\n","        self.input_dim = input_dim\n","        self.output_dim = output_dim\n","        self.relu = nn.ReLU()\n","        self.conv1d_banks = nn.ModuleList(\n","            [\n","                BatchNormConv1d(\n","                    input_dim,\n","                    input_dim,\n","                    kernel_size=k,\n","                    stride=1,\n","                    padding=k // 2,\n","                    activation=self.relu,\n","                )\n","                for k in range(1, K + 1)\n","            ]\n","        )\n","        self.max_pool1d = nn.MaxPool1d(kernel_size=2, stride=1, padding=1)\n","\n","        in_sizes = [K * input_dim] + projections[:-1]\n","        activations = [self.relu] * (len(projections) - 1) + [None]\n","        self.conv1d_projections = nn.ModuleList(\n","            [\n","                BatchNormConv1d(\n","                    in_size, out_size, kernel_size=3, stride=1, padding=1, activation=ac\n","                )\n","                for (in_size, out_size, ac) in zip(in_sizes, projections, activations)\n","            ]\n","        )\n","\n","        self.pre_highway = nn.Linear(projections[-1], input_dim, bias=False)\n","        self.highways = nn.ModuleList([Highway(input_dim, input_dim) for _ in range(4)])\n","\n","        self.gru = nn.GRU(input_dim, output_dim, 1, batch_first=True, bidirectional=True)\n","\n","    def forward(self, inputs, input_lengths=None):\n","        x = inputs\n","        x = x.transpose(1, 2)\n","        T = x.size(-1)\n","\n","        # Concat conv1d bank outputs\n","        x = torch.cat([conv1d(x)[:, :, :T] for conv1d in self.conv1d_banks], dim=1)\n","        assert x.size(1) == self.input_dim * len(self.conv1d_banks)\n","        x = self.max_pool1d(x)[:, :, :T]\n","\n","        for conv1d in self.conv1d_projections:\n","            x = conv1d(x)\n","\n","        # Back to the original shape\n","        x = x.transpose(1, 2)\n","\n","        if x.size(-1) != self.input_dim:\n","            x = self.pre_highway(x)\n","\n","        # Residual connection\n","        x += inputs\n","        for highway in self.highways:\n","            x = highway(x)\n","\n","        if input_lengths is not None:\n","            x = nn.utils.rnn.pack_padded_sequence(x, input_lengths, batch_first=True)\n","\n","        self.gru.flatten_parameters()\n","        outputs, _ = self.gru(x)\n","\n","        if input_lengths is not None:\n","            outputs, _ = nn.utils.rnn.pad_packed_sequence(outputs, batch_first=True)\n","\n","        return outputs\n"]},{"cell_type":"code","execution_count":16,"metadata":{"id":"znAS0rzpqM8f"},"outputs":[],"source":["class CBHGModel(nn.Module):\n","    def __init__(\n","        self,\n","        input_vocab_size: int,\n","        target_vocab_size: int,\n","        embedding_dim: int = 256,\n","        use_prenet: bool = True,\n","        prenet_sizes: List[int] = [256, 256],\n","        gru_units: int = 256,\n","        cbhg_filters: int = 16,\n","        cbhg_projections: List[int] = [128, 256],\n","        post_cbhg_layers_units: List[int] = [256, 256],\n","        post_cbhg_use_batch_norm: bool = True\n","    ):\n","        super().__init__()\n","        self.use_prenet = use_prenet\n","        self.embedding = nn.Embedding(input_vocab_size, embedding_dim)\n","        if self.use_prenet:\n","            self.prenet = Prenet(embedding_dim, prenet_depth=prenet_sizes)\n","\n","        self.cbhg = CBHG(\n","            prenet_sizes[-1] if self.use_prenet else embedding_dim,\n","            gru_units,\n","            K=cbhg_filters,\n","            projections=cbhg_projections,\n","        )\n","\n","        layers = []\n","        post_cbhg_layers_units = [gru_units] + post_cbhg_layers_units\n","\n","        for i in range(1, len(post_cbhg_layers_units)):\n","            layers.append(\n","                nn.LSTM(\n","                    post_cbhg_layers_units[i - 1] * 2,\n","                    post_cbhg_layers_units[i],\n","                    bidirectional=True,\n","                    batch_first=True,\n","                )\n","            )\n","            if post_cbhg_use_batch_norm:\n","                layers.append(nn.BatchNorm1d(post_cbhg_layers_units[i] * 2))\n","\n","        self.post_cbhg_layers = nn.ModuleList(layers)\n","        self.projections = nn.Linear(post_cbhg_layers_units[-1] * 2, target_vocab_size)\n","        self.post_cbhg_layers_units = post_cbhg_layers_units\n","        self.post_cbhg_use_batch_norm = post_cbhg_use_batch_norm\n","\n","\n","    def forward(\n","        self,\n","        src: torch.Tensor,\n","        lengths: Optional[torch.Tensor] = None,\n","    ):\n","        embedding_out = self.embedding(src)\n","        cbhg_input = embedding_out\n","        if self.use_prenet:\n","            cbhg_input = self.prenet(embedding_out)\n","        outputs = self.cbhg(cbhg_input, lengths)\n","\n","        hn = torch.zeros((2, 2, 2))\n","        cn = torch.zeros((2, 2, 2))\n","\n","        for i, layer in enumerate(self.post_cbhg_layers):\n","            if isinstance(layer, nn.BatchNorm1d):\n","                outputs = layer(outputs.permute(0, 2, 1))\n","                outputs = outputs.permute(0, 2, 1)\n","                continue\n","            if i > 0:\n","                outputs, (hn, cn) = layer(outputs, (hn, cn))\n","            else:\n","                outputs, (hn, cn) = layer(outputs)\n","\n","\n","        predictions = self.projections(outputs)\n","        output = {\"diacritics\": predictions}\n","        return output\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lJJJF-qQA_wk"},"outputs":[],"source":["# Create an instance of CBHG Model\n","model = CBHGModel(\n","      input_vocab_size = len(t_sentences) + len(v_sentences),\n","      target_vocab_size = len(tag_map),\n","        )\n","# print(model)"]},{"cell_type":"markdown","metadata":{"id":"PLHx_oHpFlSX"},"source":["# Training\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4hFDKbOeUk8S"},"outputs":[],"source":["model_name = f\"model_cbhg_all_data_lr0.001_batch32\""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-yvaq8i2CCLD"},"outputs":[],"source":["def train(model,train_dataset, start_epoch=0,batch_size = 32, epochs = 10, learning_rate = 0.001):\n","  # (1) create the dataloader of the training set (make the shuffle=True)\n","  train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n","\n","\n","  # (2) make the criterion cross entropy loss\n","  # criterion = torch.nn.CrossEntropyLoss()\n","  criterion = torch.nn.CrossEntropyLoss(ignore_index=tag_map[\"pad\"])\n","\n","  # (3) create the optimizer (Adam)\n","  optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n","\n","  # define your scheduler (to gradually decrease learning rate)\n","  # scheduler is used to decrease the learning rate by a factor of [gamma] every [step_size] epochs\n","  # scheduler = StepLR(optimizer, step_size=5, gamma=0.0001)\n","\n","  # GPU configuration\n","  use_cuda = torch.cuda.is_available()\n","  device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n","  if use_cuda:\n","    model = model.cuda()\n","    criterion = criterion.cuda()\n","\n","  for epoch_num in range(start_epoch,start_epoch+epochs):\n","    total_acc_train = 0\n","    total_loss_train = 0\n","    prev_acc = -1\n","    for train_input, train_label in tqdm(train_dataloader):\n","\n","      # (4) move the train input to the device\n","      train_label = train_label.to(device)\n","\n","      # (5) move the train label to the device\n","      train_input = train_input.to(device)\n","\n","      # (6) do the forward pass\n","      output = model(train_input)\n","\n","      # (7) loss calculation\n","      batch_loss = criterion(output['diacritics'].view(-1, output['diacritics'].shape[-1]), train_label.view(-1))\n","\n","      # (8) append the batch loss to the total_loss_train\n","      total_loss_train += batch_loss.item()\n","\n","      # (9) calculate the batch accuracy (just add the number of correct predictions)\n","      acc = (output['diacritics'].argmax(2) == train_label).sum().item()\n","\n","      total_acc_train += acc\n","\n","      # (10) zero your gradients\n","      optimizer.zero_grad()\n","\n","\n","      # (11) do the backward pass\n","      batch_loss.backward()\n","\n","\n","      # (12) update the weights with your optimizer\n","      optimizer.step()\n","\n","      # update the learning rate\n","      # scheduler.step()\n","\n","    # epoch loss\n","    epoch_loss = total_loss_train / len(train_dataset)\n","\n","    # (13) calculate the accuracyS\n","    epoch_acc = total_acc_train / (len(train_dataset) * train_dataset[0][0].shape[0])\n","    if prev_acc >= epoch_acc:\n","      print(\"Finish training because too many epochs\")\n","      return\n","    else:\n","      prev_acc = epoch_acc\n","\n","    print(\n","        f'Epochs: {epoch_num + 1} | Train Loss: {epoch_loss} \\\n","        | Train Accuracy: {epoch_acc}\\n')\n","\n","    # clear the cache each epoch\n","    torch.cuda.empty_cache()\n","    torch.save(model.state_dict(), f'./SavedModels/{model_name}_epoch{epoch_num + 1}')\n","\n","  ##############################################################################################################"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3BI7_ANkLf7G"},"outputs":[],"source":["train_dataset = NERDataset(t_sentences, t_labels, vocab['<PAD>'])\n","test_dataset = NERDataset(test_sentences, test_labels, vocab['<PAD>'])\n","test_dataset2 = NERDataset(test_sentences2, test_labels2, vocab['<PAD>'])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4W5Ns2PlpEV1"},"outputs":[],"source":["def load_model(model,model_name):\n","  model.load_state_dict(torch.load(f'./SavedModels/{model_name}'))\n","  return model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LMXjDv51LU6k"},"outputs":[],"source":["start_epoch = 4\n","if start_epoch != 0:\n","  model = load_model(model,\"model_cbhg_all_data_lr0.001_batch32_epoch4\") # start from a previously trained model with some epochs\n","\n","train(model, train_dataset,start_epoch = start_epoch,epochs = 3)"]},{"cell_type":"markdown","metadata":{"id":"QiUOk7vmohDd"},"source":["# Save The Model\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HJSK7IZ2ogSR"},"outputs":[],"source":["# Save the Diacritic Classifier Model\n","torch.save(model.state_dict(), f'./SavedModels/{model_name}')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pVfRdrLJNW4C"},"outputs":[],"source":["del train_dataset\n","gc.collect()"]},{"cell_type":"markdown","metadata":{"id":"TWJNO6mUXPRI"},"source":["# Evaluation\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Gz5mxUAJM1xS"},"outputs":[],"source":["diacritic_results = []\n","gold_results = []\n","test_input_list = []\n","def evaluate(model, test_dataset, batch_size = 32):\n","  # (1) create the test data loader\n","  test_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size)\n","\n","  # GPU Configuration\n","  use_cuda = torch.cuda.is_available()\n","  device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n","  if use_cuda:\n","    model = model.cuda()\n","\n","  total_acc_test = 0\n","\n","\n","  # (2) disable gradients\n","  with torch.no_grad():\n","\n","    for test_input, test_label in tqdm(test_dataloader):\n","      # (3) move the test input to the device\n","      test_label = test_label.to(device)\n","\n","      # (4) move the test label to the device\n","      test_input = test_input.to(device)\n","\n","      # (5) do the forward pass\n","      output = model(test_input)\n","      prediction = output['diacritics'].argmax(2)\n","\n","\n","      diacritic_results.extend(np.array(prediction.cpu().data).flatten())\n","      gold_results.extend(np.array(test_label.cpu().data).flatten())\n","      test_input_list.extend(np.array(test_input.cpu().data).flatten())"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6FD8JNcHWmMY"},"outputs":[],"source":["evaluate(model, test_dataset)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lW8Nqgj9E146"},"outputs":[],"source":["der = 0\n","total_size = 0\n","for i in range(len(diacritic_results)):\n","  if test_input_list[i] != vocab['<PAD>']: # Do not include padding in DER calculations\n","    if diacritic_results[i] != gold_results[i] : # Miss Classification\n","      der += 1\n","    total_size += 1\n","der /= total_size\n","der *= 100\n","print(\"DER = \",der,\"%\")\n","print(\"Accuracy = \",100 - der,\"%\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LLtlWNm6YVG2"},"outputs":[],"source":["# these list are sorted as mentioned by the TA\n","LIST_OF_DIACRITICS = [\n","    \"FATHA\",\n","    \"FATHATAN\",\n","    \"DAMMA\",\n","    \"DAMMATAN\",\n","    \"KASRA\",\n","    \"KASRATAN\",\n","    \"SUKUN\",\n","    \"SHADDA\",\n","    \"SHADDA_FATHA\",\n","    \"SHADDA_FATHATAN\",\n","    \"SHADDA_DAMMA\",\n","    \"SHADDA_DAMMATAN\",\n","    \"SHADDA_KASRA\",\n","    \"SHADDA_KASRATAN\",\n","    \"_\"\n","]\n","LIST_OF_ARABIC_LETTERS = ['آ' ,'ض','ف','ص','أ','ت','ق','ث','ا','ه','غ','ة','ج','ك','م','ن','ي','ب','د','س','و','ل','ؤ','ش','إ','ط','ئ','ظ','ز','ى','ء','ر','ع','ذ','ح','خ','UNK','<pad>']"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0QXdBygXdT0-"},"outputs":[],"source":["# Prepare the data that will be written in the CSV file\n","index = 100\n","filtered_diacritic_results = [] # diacrtic results without paddings\n","filtered_gold_results = [] # diacrtic gold results without paddings\n","filtered_inputs = [] # inputs without paddings\n","state = [] # This marks the incorrect results in the CSV file\n","\n","for i in range(len(diacritic_results)):\n","  if test_input_list[i] != vocab['<PAD>']:\n","    filtered_diacritic_results.append(diacritic_results[i])\n","    filtered_gold_results.append(gold_results[i])\n","    filtered_inputs.append(test_input_list[i])\n","    if diacritic_results[i] != gold_results[i]:\n","      state.append(\"Incorrect\")\n","    else:\n","      state.append(\"\")\n","\n","\n","inputs = [LIST_OF_ARABIC_LETTERS[filtered_inputs[i]] for i in range(index)]\n","model_prediction = [LIST_OF_DIACRITICS[filtered_diacritic_results[i]] for i in range(index)]\n","gold_out = [LIST_OF_DIACRITICS[filtered_gold_results[i]] for i in range(index)]\n","state = state[0:index]\n","\n","print(len(inputs))\n","print(len(model_prediction))\n","print(len(gold_out))\n","print(len(state))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DNhBQoIbEYpp"},"outputs":[],"source":["# Output the accuracy to CSV file\n","model_name = f\"CBHG\"\n","df = pd.DataFrame(\n","    {\n","     'ID': range(len(filtered_diacritic_results[0:index])),\n","     'label': filtered_diacritic_results[0:index],\n","     # The following columns are provided for illustrative purposes only and are not necessary (will be commented when submitting the CSV file).\n","     'input':  inputs,\n","     'diactric':  model_prediction,\n","     'gold out': gold_out,\n","     'state' : state\n","     })\n","\n","df.to_csv(f'./Results/result_{model_name}.csv', index=False)"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.5"}},"nbformat":4,"nbformat_minor":0}
