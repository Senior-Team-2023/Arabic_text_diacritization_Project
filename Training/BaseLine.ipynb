{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":21044,"status":"ok","timestamp":1704148222794,"user":{"displayName":"peter test","userId":"00789745063192476581"},"user_tz":-120},"id":"4FmiJ0pSimk2","outputId":"f845f5d7-84d2-4fe4-e220-ef98a1f10760"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":13,"status":"ok","timestamp":1704148222796,"user":{"displayName":"peter test","userId":"00789745063192476581"},"user_tz":-120},"id":"8Lgy0bfSiqLu","outputId":"21f95a75-528e-430a-85f0-95d08dbbc2fa"},"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/.shortcut-targets-by-id/1zPjf1cHfdKqObemkPReffGbQHU_wotr2/NLP_Project\n"]}],"source":["\n","%cd ./drive/MyDrive/Colab\\ Notebooks/NLP_Project/"]},{"cell_type":"code","execution_count":12,"metadata":{"id":"WR6a6DkN0d-3","executionInfo":{"status":"ok","timestamp":1704149065090,"user_tz":-120,"elapsed":9,"user":{"displayName":"peter test","userId":"00789745063192476581"}}},"outputs":[],"source":["import os\n","import numpy as np\n","import pandas as pd\n","from tqdm import tqdm\n","import torch\n","from torch import nn\n","import random as rnd\n","from torch.optim.lr_scheduler import StepLR\n","import gc"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WrtpcAxjScxO"},"outputs":[],"source":["# os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'max_split_size_mb:512'"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"n0oM1zovpvga"},"outputs":[],"source":["# import pandas as pd\n","\n","# # Replace these file paths with the actual paths to your CSV files\n","# file1_path = './Results/test_set_without_labels.csv'\n","# file2_path = './Results/result_compare_model_baseline.csv'\n","\n","# # Load CSV files into DataFrames\n","# df1 = pd.read_csv(file1_path)\n","# df2 = pd.read_csv(file2_path)\n","\n","# # Check if \"letter\" column exists in both DataFrames\n","# if 'letter' not in df1.columns:\n","#     print('Error: \"letter\" column not found in the first CSV file.')\n","#     exit()\n","\n","# if 'input' not in df2.columns:\n","#     print('Error: \"input\" column not found in the second CSV file.')\n","#     exit()\n","# C = 0\n","# # Compare \"letter\" column in the first CSV with \"input\" column in the second CSV using a for loop\n","# mismatched_indices = []\n","# for idx, letter_value in enumerate(df1['letter']):\n","#     # if df2['input'][idx] == df2['input'][13994]:\n","#     #   C+=1\n","#     if df1['letter'][idx] != df2['input'][idx]:\n","#       print(\"err in index :\", idx,\" & line : \", df1['line_number'][idx])\n","#       print(\"'\",df1['letter'][idx - 4] ,\"' --- '\", df2['input'][idx - 4],\"'\")\n","#       print(\"'\",df1['letter'][idx - 3] ,\"' --- '\", df2['input'][idx - 3],\"'\")\n","#       print(\"'\",df1['letter'][idx - 2] ,\"' --- '\", df2['input'][idx - 2],\"'\")\n","#       print(\"'\",df1['letter'][idx - 1] ,\"' --- '\", df2['input'][idx - 1],\"'\")\n","#       print(\"'\",df1['letter'][idx] ,\"' --- '\", df2['input'][idx],\"'\")\n","#       print(\"'\",df1['letter'][idx + 1] ,\"' --- '\", df2['input'][idx + 1],\"'\")\n","#       print(\"'\",df1['letter'][idx + 2] ,\"' --- '\", df2['input'][idx + 2],\"'\")\n","#       print(\"'\",df1['letter'][idx + 3] ,\"' --- '\", df2['input'][idx + 3],\"'\")\n","#       print(\"'\",df1['letter'][idx + 4] ,\"' --- '\", df2['input'][idx + 4],\"'\")\n","#       print(\"'\",df1['letter'][idx + 5] ,\"' --- '\", df2['input'][idx + 5],\"'\")\n","#       print(\"'\",df1['letter'][idx + 6] ,\"' --- '\", df2['input'][idx + 6],\"'\")\n","#       print(\"'\",df1['letter'][idx + 7] ,\"' --- '\", df2['input'][idx + 7],\"'\")\n","#       print(\"'\",df1['letter'][idx + 8] ,\"' --- '\", df2['input'][idx + 8],\"'\")\n","#       break\n","#     # for idx_in, input_value in enumerate(df1['input']):\n","\n","#       # if letter_value not in df2['input'].values:\n","#       #     mismatched_indices.append(idx)\n","\n","\n","# print(C)"]},{"cell_type":"markdown","metadata":{"id":"alY6U9G-dBSM"},"source":["# Constants"]},{"cell_type":"code","execution_count":4,"metadata":{"id":"CL1Yi6fYdAyA","executionInfo":{"status":"ok","timestamp":1704148227136,"user_tz":-120,"elapsed":8,"user":{"displayName":"peter test","userId":"00789745063192476581"}}},"outputs":[],"source":["MODEL = \"BI_LSTM\"\n","NUM_LAYERS = 2\n","EMBEDDING_SIZE = 300"]},{"cell_type":"code","execution_count":5,"metadata":{"id":"jS2AXWkhigQt","executionInfo":{"status":"ok","timestamp":1704148227137,"user_tz":-120,"elapsed":8,"user":{"displayName":"peter test","userId":"00789745063192476581"}}},"outputs":[],"source":["def get_vocab(vocab_path, tags_path):\n","    vocab = {}\n","    with open(vocab_path) as f:\n","        for i, l in enumerate(f.read().splitlines()):\n","            vocab[l] = i  # to avoid the 0\n","        # loading tags (we require this to map tags to their indices)\n","    vocab['<PAD>'] = len(vocab) # 35180\n","    tag_map = {}\n","    with open(tags_path) as f:\n","        for i, t in enumerate(f.read().splitlines()):\n","            tag_map[t] = i\n","\n","    return vocab, tag_map\n","\n","def get_params(vocab, tag_map, sentences_file, labels_file):\n","    sentences = []\n","    labels = []\n","\n","    with open(sentences_file) as f:\n","        for sentence in f.read().splitlines():\n","            # replace each token by its index if it is in vocab\n","            # else use index of UNK_WORD\n","            s = [vocab[token] if token in vocab\n","                 else vocab['UNK']\n","                 for token in sentence.split(' ')]\n","            sentences.append(s)\n","\n","    with open(labels_file) as f:\n","        for sentence in f.read().splitlines():\n","            # replace each label by its index\n","            s = sentence.split(' ')\n","            # remove empty strings\n","            s = list(filter(None, s))\n","            l = [tag_map[label] for label in s] # I added plus 1 here\n","            labels.append(l)\n","    return sentences, labels, len(sentences)\n"]},{"cell_type":"markdown","metadata":{"id":"_44BK5K82YwF"},"source":["# Importing and discovering the data"]},{"cell_type":"code","execution_count":6,"metadata":{"id":"ulSik2Sv1p1G","executionInfo":{"status":"ok","timestamp":1704148982251,"user_tz":-120,"elapsed":6906,"user":{"displayName":"peter test","userId":"00789745063192476581"}}},"outputs":[],"source":["vocab, tag_map = get_vocab('./Dataset/new_new_characters/unique_chars.txt', './Dataset/new_new_characters/unique_labels.txt')\n","t_sentences, t_labels, t_size = get_params(vocab, tag_map, './Dataset/new_new_characters/t_chars.txt', './Dataset/new_new_characters/t_labels.txt')\n","v_sentences, v_labels, v_size = get_params(vocab, tag_map, './Dataset/new_new_characters/v_chars.txt', './Dataset/new_new_characters/v_labels.txt')\n","test_sentences, test_labels, test_size = get_params(vocab, tag_map, './Dataset/new_new_characters/test_chars.txt', './Dataset/new_new_characters/test_labels.txt')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Ae4mAnXHdrIJ"},"outputs":[],"source":["# # test_sentences, test_labels, test_size = get_params(vocab, tag_map, './Dataset/new_new_characters/test_no_diacritics_chars.txt', './Dataset/new_new_characters/test_no_diacritics_labels.txt')\n","# test_sentences, test_labels, test_size = get_params(vocab, tag_map, './Dataset/new_new_characters/test2_chars.txt', './Dataset/new_new_characters/test2_labels.txt')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"b9NQB6k9h_Ig"},"outputs":[],"source":["\n","# NOTE: to increase the size of the dataset\n","# t_sentences  = t_sentences + v_sentences\n","# t_labels = t_labels + v_labels\n","# t_size = t_size + v_size"]},{"cell_type":"code","execution_count":7,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":6,"status":"ok","timestamp":1704148982252,"user":{"displayName":"peter test","userId":"00789745063192476581"},"user_tz":-120},"id":"IB_1MhtP1rLL","outputId":"ad1a3f08-9dad-4da0-f897-2984532016ef"},"outputs":[{"output_type":"stream","name":"stdout","text":["padded token: 38\n"]}],"source":["# # vocab translates from a word to a unique number\n","# print('vocab[\"الأعم\"]:', vocab[\"الأعم\"])\n","# Pad token\n","print('padded token:', vocab[\"<PAD>\"])"]},{"cell_type":"code","execution_count":8,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4,"status":"ok","timestamp":1704148982252,"user":{"displayName":"peter test","userId":"00789745063192476581"},"user_tz":-120},"id":"PagjN4rl22Fr","outputId":"072dbc9c-26d6-4900-8eb8-26f0712e9d73"},"outputs":[{"output_type":"stream","name":"stdout","text":["{'FATHA': 0, 'FATHATAN': 1, 'DAMMA': 2, 'DAMMATAN': 3, 'KASRA': 4, 'KASRATAN': 5, 'SUKUN': 6, 'SHADDA': 7, 'SHADDA_FATHA': 8, 'SHADDA_FATHATAN': 9, 'SHADDA_DAMMA': 10, 'SHADDA_DAMMATAN': 11, 'SHADDA_KASRA': 12, 'SHADDA_KASRATAN': 13, '_': 14, 'pad': 15}\n"]}],"source":["# The possible tags\n","print(tag_map)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"oWLR2Oxp28K6"},"outputs":[],"source":["\n","# # Exploring information about the data\n","# print('The number of outputs is tag_map', len(tag_map))\n","# # The number of vocabulary tokens (including <PAD>)\n","# g_vocab_size = len(vocab)\n","# print(f\"Num of vocabulary words: {g_vocab_size}\")\n","# print('The vocab size is', len(vocab))\n","# print('The training size is', t_size)\n","# print('The validation size is', v_size)\n","# print('An example of the first sentence is', t_sentences[0])\n","# print('An example of its corresponding label is', t_labels[0])\n","# len(t_sentences[0])==len( t_labels[0])"]},{"cell_type":"markdown","metadata":{"id":"wt3e4nxjFT3O"},"source":["# NERDataset\n","The class that impelements the dataset for NER"]},{"cell_type":"code","execution_count":9,"metadata":{"id":"29iM0u4-4YOV","executionInfo":{"status":"ok","timestamp":1704148984946,"user_tz":-120,"elapsed":5,"user":{"displayName":"peter test","userId":"00789745063192476581"}}},"outputs":[],"source":["class NERDataset(torch.utils.data.Dataset):\n","\n","  def __init__(self, x, y, pad):\n","    self.x = nn.utils.rnn.pad_sequence([torch.tensor(i) for i in x], padding_value=pad,batch_first = True)\n","    self.y = nn.utils.rnn.pad_sequence([torch.tensor(i) for i in y], padding_value=tag_map[\"pad\"],batch_first = True)\n","    print('The max length of the sentences is', self.x.shape[1])\n","    print('The max length of the labels is', self.y.shape[1])\n","  def __len__(self):\n","    return len(self.x)\n","\n","  def __getitem__(self, idx):\n","    return self.x[idx], self.y[idx]"]},{"cell_type":"markdown","metadata":{"id":"CQB6O7I7FbUh"},"source":["# Classifiers\n","The class that implementss the pytorch model for arabic diacritic classification"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":24,"status":"ok","timestamp":1704135284655,"user":{"displayName":"Mark Test2","userId":"06729466490863137750"},"user_tz":-120},"id":"bxZ1tumDgF7N","outputId":"754ec9da-af0d-4254-e4ac-2b75ebbf8a25"},"outputs":[{"name":"stdout","output_type":"stream","text":["{'ل': 0, 'ن': 1, 'ش': 2, 'آ': 3, 'ك': 4, 'ب': 5, 'ح': 6, 'ص': 7, 'ئ': 8, 'خ': 9, 'ي': 10, 'ز': 11, 'ض': 12, 'ط': 13, 'ج': 14, 'ر': 15, 'ة': 16, 'و': 17, 'ا': 18, 'غ': 19, 'ى': 20, 'ء': 21, 'ظ': 22, 'س': 23, 'ق': 24, 'د': 25, 'ف': 26, 'إ': 27, 'ذ': 28, 'ت': 29, 'ث': 30, 'م': 31, 'ع': 32, 'ه': 33, 'أ': 34, 'ؤ': 35, 'UNK': 36, '<pad>': 37, '<PAD>': 38}\n"]}],"source":["print(vocab)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"o6t7yYNKZpmT"},"outputs":[],"source":["# # GRU\n","\n","# class ArabicDiacriticsClassifier(nn.Module):\n","#     def __init__(self, vocab_size=len(t_sentences) + len(test_sentences) + len(v_sentences), num_layers=2, embedding_dim=300, hidden_size=50, n_classes=len(tag_map)):\n","#         MODEL = \"BI_GRU\"\n","#         super(ArabicDiacriticsClassifier, self).__init__()\n","#         # (1) Create the embedding layer\n","#         self.embedding = nn.Embedding(vocab_size, embedding_dim)\n","\n","#         # (2) Create a GRU layer with hidden size = hidden_size and batch_first = True\n","#         self.gru = nn.GRU(embedding_dim, hidden_size, num_layers, batch_first=True, bidirectional=True)\n","#         self.num_layers = num_layers\n","#         self.hidden_size = hidden_size\n","\n","#         # (3) Create a linear layer with the number of neurons = n_classes\n","#         self.linear = nn.Linear(2 * hidden_size, n_classes)\n","\n","#     def forward(self, sentences):\n","#         embeddings = self.embedding(sentences)\n","\n","#         # Initialize hidden states for bidirectional GRU\n","#         h0 = torch.zeros(self.num_layers*2, embeddings.size(0), self.hidden_size).to(sentences.device)\n","#         gru_out, _ = self.gru(embeddings, h0)\n","\n","#         final_output = self.linear(gru_out)\n","#         return final_output"]},{"cell_type":"code","execution_count":18,"metadata":{"id":"xHeJcz1JuhYa","executionInfo":{"status":"ok","timestamp":1704149591653,"user_tz":-120,"elapsed":9,"user":{"displayName":"peter test","userId":"00789745063192476581"}}},"outputs":[],"source":["class ArabicDiacriticsClassifier(nn.Module):\n","  def __init__(self, vocab_size=len(t_sentences) + len(v_sentences)+len(v_sentences), num_layers = 3, embedding_dim = 512, hidden_size=256, n_classes=len(tag_map)):\n","    super(ArabicDiacriticsClassifier, self).__init__()\n","    # (1) Create the embedding layer\n","    self.embedding = nn.Embedding(vocab_size, embedding_dim)\n","    # import gensim.downloader as api\n","    # self.embedding.weight.data.copy_(torch.from_numpy(api.load('word2vec-google-news-300').vectors[:1000])) WORD2VEc\n","    # import fasttext.util\n","    # self.embedding.weight.data.copy_(torch.from_numpy(fasttext.util.download_model('en', if_exists='ignore').get_input_matrix()[:1000]))\n","\n","    # (2) Create an LSTM layer with hidden size = hidden_size and batch_first = True\n","    self.num_layers = num_layers\n","    self.hidden_size = hidden_size\n","\n","\n","    self.lstm = nn.LSTM(embedding_dim, hidden_size,num_layers, batch_first=True, bidirectional=True)\n","    self.linear = nn.Linear(2 * hidden_size, n_classes)\n","\n","    # (3) Create a linear layer with number of neorons = n_classes\n","    # self.linear = nn.Linear(hidden_size, n_classes)\n","\n","\n","  def forward(self, sentences):\n","    embeddings = self.embedding(sentences)\n","\n","    # BIDIRECTIONAL\n","    # Initialize hidden states for bidirectional LSTM\n","    # h0 = torch.zeros(self.num_layers*2, embeddings.size(0), self.hidden_size).to(sentences.device)\n","    # c0 = torch.zeros(self.num_layers*2, embeddings.size(0), self.hidden_size).to(sentences.device)\n","    # lstm_out, (a, b) = self.lstm(embeddings, (h0, c0))\n","\n","    # LSTM\n","    lstm1_out, (a,b) = self.lstm(embeddings)\n","    final_output = self.linear(lstm1_out)\n","    # final_output = self.linear(lstm_out[:, -1, :])\n","    return final_output"]},{"cell_type":"code","execution_count":19,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":923,"status":"ok","timestamp":1704149595553,"user":{"displayName":"peter test","userId":"00789745063192476581"},"user_tz":-120},"id":"lJJJF-qQA_wk","outputId":"294d24d8-7391-4bf3-ae94-6d5570d521a4"},"outputs":[{"output_type":"stream","name":"stdout","text":["ArabicDiacriticsClassifier(\n","  (embedding): Embedding(128137, 512)\n","  (lstm): LSTM(512, 256, num_layers=3, batch_first=True, bidirectional=True)\n","  (linear): Linear(in_features=512, out_features=16, bias=True)\n",")\n"]}],"source":["model = ArabicDiacriticsClassifier()\n","print(model)"]},{"cell_type":"markdown","metadata":{"id":"PLHx_oHpFlSX"},"source":["# Training"]},{"cell_type":"code","execution_count":13,"metadata":{"id":"-yvaq8i2CCLD","executionInfo":{"status":"ok","timestamp":1704149089035,"user_tz":-120,"elapsed":273,"user":{"displayName":"peter test","userId":"00789745063192476581"}}},"outputs":[],"source":["def train(model, train_dataset,start_epoch=0, batch_size=64, epochs=10, learning_rate=0.001):\n","  # (1) create the dataloader of the training set (make the shuffle=True)\n","  train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n","\n","  # (2) make the criterion cross entropy loss\n","  criterion = torch.nn.CrossEntropyLoss(ignore_index=tag_map[\"pad\"])\n","\n","  # (3) create the optimizer (Adam)\n","  optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n","  # define your scheduler\n","  # scheduler is used to decrease the learning rate by a factor of [gamma] every [step_size] epochs\n","  scheduler = StepLR(optimizer, step_size=1, gamma=0.00001)\n","  # GPU configuration\n","  use_cuda = torch.cuda.is_available()\n","  device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n","  if use_cuda:\n","    model = model.cuda()\n","    torch.cuda.empty_cache()\n","    criterion = criterion.cuda()\n","    torch.cuda.empty_cache()\n","\n","  print(\"Resuming from epoch \", start_epoc)\n","\n","  for epoch_num in range(start_epoch,epochs):\n","    total_acc_train = 0\n","    total_loss_train = 0\n","    prev_acc = -1\n","    for train_input, train_label in tqdm(train_dataloader):\n","\n","      # (4) move the train input to the device\n","      train_label = train_label.to(device)\n","\n","      # (5) move the train label to the device\n","      train_input = train_input.to(device)\n","\n","\n","      # (6) do the forward pass\n","      output = model(train_input)\n","      # print(\"output.shape\",output.shape)\n","      # print(\"train_label.shape\",train_label.shape)\n","      # (7) loss calculation (you need to think in this part how to calculate the loss correctly)\n","      batch_loss = criterion(output.view(-1, output.shape[-1]), train_label.view(-1))\n","\n","      # (8) append the batch loss to the total_loss_train\n","      total_loss_train += batch_loss\n","\n","      # (9) calculate the batch accuracy (just add the number of correct predictions)\n","      acc = (output.argmax(2) == train_label).sum().item()\n","\n","      total_acc_train += acc\n","\n","      # (10) zero your gradients\n","      optimizer.zero_grad()\n","\n","\n","      # (11) do the backward pass\n","      batch_loss.backward()\n","\n","\n","      # (12) update the weights with your optimizer\n","      optimizer.step()\n","\n","      # update the learning rate\n","      scheduler.step()\n","\n","    # epoch loss\n","    epoch_loss = total_loss_train / len(train_dataset)\n","\n","    # (13) calculate the accuracy\n","    epoch_acc = total_acc_train / (len(train_dataset) * train_dataset[0][0].shape[0])\n","    if prev_acc >= epoch_acc:\n","      print(\"Finish training because too many epochs\")\n","      return\n","    else:\n","      prev_acc = epoch_acc\n","\n","    print(\n","        f'Epochs: {epoch_num + 1} | Train Loss: {epoch_loss} \\\n","        | Train Accuracy: {epoch_acc}\\n')\n","    model_name = f\"model_baseline\"\n","\n","    torch.save(model.state_dict(), f'./BaseLineModels/{model_name}_EPOCH{epoch_num + 1}')\n","\n","    # clear the cache each epoch\n","\n","  ##############################################################################################################"]},{"cell_type":"code","execution_count":14,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":8912,"status":"ok","timestamp":1704149103508,"user":{"displayName":"peter test","userId":"00789745063192476581"},"user_tz":-120},"id":"3BI7_ANkLf7G","outputId":"3018199e-7cb9-43dc-82e0-b4646173e6b3"},"outputs":[{"output_type":"stream","name":"stdout","text":["The max length of the sentences is 1815\n","The max length of the labels is 1815\n","The max length of the sentences is 1936\n","The max length of the labels is 1936\n"]}],"source":["train_dataset = NERDataset(t_sentences, t_labels, vocab['<PAD>'])\n","# val_dataset = NERDataset(v_sentences, v_labels, vocab['<PAD>'])\n","test_dataset = NERDataset(test_sentences, test_labels, vocab['<PAD>'])"]},{"cell_type":"code","execution_count":15,"metadata":{"id":"b2Lj2SneWZUj","executionInfo":{"status":"ok","timestamp":1704149162229,"user_tz":-120,"elapsed":271,"user":{"displayName":"peter test","userId":"00789745063192476581"}}},"outputs":[],"source":["def load_model(model,model_name):\n","  model.load_state_dict(torch.load(f'./SavedModels/{model_name}'))\n","  return model\n","\n","def load_baseline_model(model,model_name,epoch_num):\n","  model.load_state_dict(torch.load(f'./BaseLineModels/{model_name}_EPOCH{epoch_num}'))\n","  return model"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"LMXjDv51LU6k","outputId":"ca79b833-99d1-47e9-94a6-aa783ace4506"},"outputs":[{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Resuming from epoch  5\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["100%|██████████| 1818/1818 [39:38<00:00,  1.31s/it]\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Epochs: 6 | Train Loss: 0.001386831165291369         | Train Accuracy: 0.9951628988617687\n","\n"]},{"output_type":"stream","name":"stderr","text":[" 99%|█████████▉| 1803/1818 [39:20<00:19,  1.31s/it]"]}],"source":["model_name = f\"model_baseline\"\n","start_epoc = 5\n","if start_epoc != 0:\n","  model = load_baseline_model(model, model_name, start_epoc)\n","\n","\n","train(model, train_dataset,start_epoch = start_epoc, epochs = 10)\n","torch.save(model.state_dict(), f'./SavedModels/{model_name}')\n","# model = load_model(model, model_name)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pVfRdrLJNW4C"},"outputs":[],"source":["del train_dataset\n","# del val_dataset\n","gc.collect()"]},{"cell_type":"markdown","metadata":{"id":"TWJNO6mUXPRI"},"source":["# Evaluation"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PSlji02dD53P"},"outputs":[],"source":["diacritic_results = []\n","gold_results = []\n","test_input_list = []\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Gz5mxUAJM1xS"},"outputs":[],"source":["def evaluate(model, test_dataset, batch_size=64):\n","  # (1) create the test data loader\n","  test_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size)\n","\n","  # GPU Configuration\n","  use_cuda = torch.cuda.is_available()\n","  device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n","  if use_cuda:\n","    model = model.cuda()\n","\n","  total_acc_test = 0\n","\n","\n","  # (2) disable gradients\n","  with torch.no_grad():\n","\n","    for test_input, test_label in tqdm(test_dataloader):\n","      # (3) move the test input to the device\n","      test_label = test_label.to(device)\n","\n","      # (4) move the test label to the device\n","      test_input = test_input.to(device)\n","\n","      # (5) do the forward pass\n","      output = model(test_input)\n","      prediction = output.argmax(2)\n","\n","\n","      diacritic_results.extend(np.array(prediction.cpu().data).flatten())\n","      gold_results.extend(np.array(test_label.cpu().data).flatten())\n","      test_input_list.extend(np.array(test_input.cpu().data).flatten())\n","\n","      # for i in range(test_input.shape[0]):\n","      #   for j in range(test_input.shape[1]):\n","      #     # result['id'].append(count)\n","      #     # count += 1\n","      #     result.append(prediction[i][j].cpu().data.numpy()[0])\n","\n","\n","      # accuracy calculation (just add the correct predicted items to total_acc_test)\n","      # acc = (prediction == test_label).sum().item()\n","      # total_acc_test += acc\n","\n","      # if total_acc_test ==0:\n","      #   print(\"test_input\",test_input.shape)\n","      #   print(\"test_label\",test_label.shape)\n","      #   print(\"output\",output.shape)\n","      #   print(\"output.argmax(2)\",output.argmax(2).shape)\n","      # del output\n","      # del prediction\n","\n","    # (6) calculate the over all accuracy\n","    # total_acc_test /= (len(test_dataset) * test_dataset[0][0].shape[0])\n","  ##################################################################################################\n","\n","\n","  # print(f'\\nTest Accuracy: {total_acc_test}')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"94AoN8qUlVom"},"outputs":[],"source":["# Give the model a name\n","# model_name = f\"model_{MODEL}_{NUM_LAYERS}Layers_chars_only\"\n","model_name = f\"model_baseline\""]},{"cell_type":"markdown","metadata":{"id":"4J32rNo0SRxb"},"source":["### To Load an Existing Model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"K8k1SuZGQTYv"},"outputs":[],"source":["# device = torch.device('cuda')\n","# model.load_state_dict(torch.load(f'./SavedModels/{model_name}'))\n","# model = model.to(device)"]},{"cell_type":"markdown","metadata":{"id":"DMsZQ4ITWAEl"},"source":["### Evaluatio and Save"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6FD8JNcHWmMY"},"outputs":[],"source":["evaluate(model, test_dataset)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vrP2pxAUb13a"},"outputs":[],"source":["# inputs = [LIST_OF_ARABIC_LETTERS[test_input_list[i]] for i in range(10)]\n","# print(inputs)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ypRxqVV_g3sJ"},"outputs":[],"source":["gc.collect()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bHY019YvKejN"},"outputs":[],"source":["der = 0\n","total_size = 0\n","for i in range(len(diacritic_results)):\n","  if test_input_list[i] != vocab['<PAD>']: # Do not include padding in DER calculations\n","    if diacritic_results[i] != gold_results[i] : # Miss Classification\n","      der += 1\n","    total_size += 1\n","der /= total_size\n","der *= 100\n","print(\"DER = \",der,\"%\")\n","print(\"Accuracy = \",100 - der,\"%\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FlUBXsRYPc-1"},"outputs":[],"source":["# Save the Diacritic Classifier Model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LLtlWNm6YVG2"},"outputs":[],"source":["# these list are sorted as mentioned by the TA\n","LIST_OF_DIACRITICS = [\n","    \"FATHA\",\n","    \"FATHATAN\",\n","    \"DAMMA\",\n","    \"DAMMATAN\",\n","    \"KASRA\",\n","    \"KASRATAN\",\n","    \"SUKUN\",\n","    \"SHADDA\",\n","    \"SHADDA_FATHA\",\n","    \"SHADDA_FATHATAN\",\n","    \"SHADDA_DAMMA\",\n","    \"SHADDA_DAMMATAN\",\n","    \"SHADDA_KASRA\",\n","    \"SHADDA_KASRATAN\",\n","    \"_\"\n","]\n","LIST_OF_ARABIC_LETTERS = list(vocab.keys())\n","print(LIST_OF_ARABIC_LETTERS)\n","s = ['آ' ,'ض','ف','ص','أ','ت','ق','ث','ا','ه','غ','ة','ج','ك','م','ن','ي','ب','د','س','و','ل','ؤ','ش','إ','ط','ئ','ظ','ز','ى','ء','ر','ع','ذ','ح','خ','UNK','<pad>']\n","# for i in range(len(s)):\n","#   if s[i] not in LIST_OF_ARABIC_LETTERS:\n","#     print(\"####sd\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"O9yUW2nlgMTk"},"outputs":[],"source":["print(tag_map)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GbOfNxV4b-eC"},"outputs":[],"source":["# Prepare the data that will be written in the CSV file\n","index = 5000\n","filtered_diacritic_results = [] # diacrtic results without paddings\n","filtered_gold_results = [] # diacrtic gold results without paddings\n","filtered_inputs = [] # inputs without paddings\n","state = [] # This marks the incorrect results in the CSV file\n","\n","for i in range(len(diacritic_results)):\n","  if test_input_list[i] != vocab['<PAD>']:\n","    filtered_diacritic_results.append(diacritic_results[i])\n","    # filtered_gold_results.append(gold_results[i])\n","    filtered_inputs.append(test_input_list[i])\n","    # if diacritic_results[i] != gold_results[i]:\n","    #   state.append(\"Incorrect\")\n","    # else:\n","    #   state.append(\"\")\n","\n","index = len(filtered_diacritic_results)\n","print(len(filtered_diacritic_results))\n","# print(len(filtered_gold_results))\n","print(len(filtered_inputs))\n","\n","inputs = [LIST_OF_ARABIC_LETTERS[filtered_inputs[i]] for i in range(index)]\n","model_prediction = [LIST_OF_DIACRITICS[filtered_diacritic_results[i]] for i in range(index)]\n","# gold_out = [LIST_OF_DIACRITICS[filtered_gold_results[i]] for i in range(index)]\n","# state = state[0:index]\n","print(inputs[0:100])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DNhBQoIbEYpp"},"outputs":[],"source":["# Output the accuracy to CSV file\n","df = pd.DataFrame(\n","    {\n","     'ID': range(len(filtered_diacritic_results[0:index])),\n","     'label': filtered_diacritic_results[0:index],\n","     # The following columns are provided for illustrative purposes only and are not necessary (will be commented when submitting the CSV file).\n","     'input':  inputs,\n","    #  'diactric':  model_prediction,\n","    #  'gold out': gold_out,\n","    #  'state' : state\n","     })\n","\n","df.to_csv(f'./Results/result_compare_{model_name}.csv', index=False)\n","# df.to_csv(f'./Results/result_{model_name}.csv', index=False)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VeJ2H99Hgxz9"},"outputs":[],"source":[]}],"metadata":{"accelerator":"GPU","colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.5"}},"nbformat":4,"nbformat_minor":0}