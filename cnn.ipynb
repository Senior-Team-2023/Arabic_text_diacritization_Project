{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from gensim.models import FastText\n",
    "from gensim.test.utils import common_texts\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "# import the necessary packages\n",
    "# from torch.nn import Sequential\n",
    "# from torch.nn import Conv1d\n",
    "# from torch.nn import MaxPool1d\n",
    "# from torch.nn import LogSoftmax\n",
    "from torch import flatten\n",
    "\n",
    "# Combines arrays in a vertically stacked sequence (used for data manipulation)\n",
    "from numpy import vstack\n",
    "\n",
    "# Reads a CSV file into a DataFrame (used for loading datasets)\n",
    "from pandas import read_csv\n",
    "\n",
    "# Encodes categorical labels into numerical format (used for label preprocessing)\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Calculates the accuracy of a classification model (used for model evaluation)\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Defines a custom dataset class for PyTorch (used for handling data)\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "# Creates a DataLoader for efficient batch processing in PyTorch (used for data loading)\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Splits a dataset into training and validation sets (used for data splitting)\n",
    "from torch.utils.data import random_split\n",
    "\n",
    "# Represents a multi-dimensional matrix in PyTorch (used for tensor manipulation)\n",
    "from torch import Tensor\n",
    "import torch.nn as nn\n",
    "import utils\n",
    "# # Implements a linear layer in a neural network (used for defining neural network architecture)\n",
    "# from torch.nn import Linear\n",
    "\n",
    "# # Applies rectified linear unit (ReLU) activation function (used for introducing non-linearity)\n",
    "# from torch.nn import ReLU\n",
    "\n",
    "# # Applies sigmoid activation function (used for binary classification output)\n",
    "# from torch.nn import Sigmoid\n",
    "\n",
    "# # Base class for all neural network modules in PyTorch (used for creating custom models)\n",
    "# from torch.nn import Module\n",
    "\n",
    "# # Stochastic Gradient Descent optimizer (used for model optimization during training)\n",
    "from torch.optim import Adam\n",
    "\n",
    "\n",
    "# # embedding layer\n",
    "# from torch.nn import Embedding\n",
    "# from torch.nn import Dropout\n",
    "# from torch.nn import ModuleList\n",
    "import config as conf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configurations : \n",
      "number_test_of_words : 10000\n",
      "number_validation_of_words : 1000\n",
      "classifier : lstm\n",
      "embedding : fasttext\n",
      "is_training : True\n",
      "word_embeddings : False\n",
      "character_embeddings : False\n",
      "embedding_vector_size : 100\n",
      "character_embedding_vector_size : 200\n",
      "batch_size : 64\n",
      "num_epochs : 7\n"
     ]
    }
   ],
   "source": [
    "config = conf.ConfigLoader().load_config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Peter\\anaconda3\\Lib\\site-packages\\torch\\cuda\\__init__.py:107: UserWarning: CUDA initialization: The NVIDIA driver on your system is too old (found version 10010). Please update your GPU driver by downloading and installing a new version from the URL: http://www.nvidia.com/Download/index.aspx Alternatively, go to: https://pytorch.org to install a PyTorch version that has been compiled with your version of the CUDA driver. (Triggered internally at ..\\c10\\cuda\\CUDAFunctions.cpp:109.)\n",
      "  return torch._C._cuda_getDeviceCount() > 0\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Hello': 1, 'from': 2, 'the': 3, 'other': 4, 'world': 5}\n",
      "torch.Size([5, 300])\n",
      "torch.Size([1, 299])\n",
      "torch.Size([1, 149])\n",
      "torch.Size([1, 149])\n"
     ]
    }
   ],
   "source": [
    "# Define the vocabulary size and embedding dimension\n",
    "vocab_size = 50\n",
    "embedding_dim = 300\n",
    "\n",
    "# Create an embedding layer\n",
    "embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "\n",
    "# Define a sequence of strings\n",
    "strings = \"Hello from the other world\".split()\n",
    "\n",
    "# Define a dictionary to map strings to indices\n",
    "word_to_index = {w:i+1 for i,w in enumerate(strings)}\n",
    "print(word_to_index)\n",
    "# Convert the sequence of strings to a sequence of indices\n",
    "indices = torch.LongTensor([word_to_index[word] for word in strings])\n",
    "# Define a convolutional layer\n",
    "conv = nn.Conv1d(in_channels=5, out_channels=1, kernel_size=2,stride=1)\n",
    "\n",
    "# Pass the tensor through the convolutional layer\n",
    "embed_out = embedding(indices)\n",
    "print(embed_out.size())\n",
    "\n",
    "conv_out = conv(embed_out)\n",
    "# conv_out = [32, 16, 26, 26]\n",
    "print(conv_out.size())\n",
    "# pooling layer\n",
    "pool = nn.MaxPool1d(kernel_size=2)\n",
    "# Pass the convolved output through the pooling layer\n",
    "pool_out = pool(conv_out)\n",
    "print(pool_out.size())\n",
    "\n",
    "# Flatten the output of the convolutional layer\n",
    "flatten = nn.Flatten()\n",
    "flat_out = flatten(pool_out)\n",
    "print(flat_out.size())\n",
    "# 17 * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Diacritics\n",
    "KASRA = \"\\u0650\"\n",
    "DAMMA = \"\\u064F\"\n",
    "FATHA = \"\\u064E\"\n",
    "KASRATAN = \"\\u064D\"\n",
    "DAMMATAN = \"\\u064C\"\n",
    "FATHATAN = \"\\u064B\"\n",
    "SUKUN = \"\\u0652\"\n",
    "SHADDA = \"\\u0651\"\n",
    "DAMMA_SHADDA =  DAMMA + SHADDA\n",
    "SHADDA_DAMMA =  SHADDA + DAMMA\n",
    "FATHA_SHADDA =  FATHA + SHADDA\n",
    "SHADDA_FATHA =  SHADDA + FATHA\n",
    "KASRA_SHADDA =  KASRA + SHADDA\n",
    "SHADDA_KASRA =  SHADDA + KASRA\n",
    "DAMMATAN_SHADDA =  DAMMATAN + SHADDA\n",
    "SHADDA_DAMMATAN =  SHADDA + DAMMATAN\n",
    "FATHATAN_SHADDA =  FATHATAN + SHADDA\n",
    "SHADDA_FATHATAN =  SHADDA + FATHATAN\n",
    "KASRATAN_SHADDA =  KASRATAN + SHADDA\n",
    "SHADDA_KASRATAN =  SHADDA + KASRATAN\n",
    "EMPTY = \"\"\n",
    "DIACRITICS = [KASRA, DAMMA, FATHA, KASRATAN, DAMMATAN, FATHATAN, SUKUN, SHADDA, DAMMA_SHADDA, SHADDA_DAMMA, FATHA_SHADDA, SHADDA_FATHA, KASRA_SHADDA, SHADDA_KASRA, DAMMATAN_SHADDA, SHADDA_DAMMATAN, FATHATAN_SHADDA, SHADDA_FATHATAN, KASRATAN_SHADDA, SHADDA_KASRATAN, EMPTY]\n",
    "ARABIC_ALPHABIT = \"اأآإئءبتةثجحخدذرزسشصضطظعغفقكلمنهوؤيى\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter data takes a list of strings and removes unwanted patterns\n",
    "def filter_data(data: str) -> str:\n",
    "    # data = re.sub(r\"\\( \\d+ (/ \\d+)? \\)\", \"\", data)\n",
    "    # remove all numbers\n",
    "    data = re.sub(r\"\\d+\", \"\", data)\n",
    "    # regex to remove all special characters\n",
    "    data = re.sub(r\"[][//,;\\?؟()$:\\-{}_*؛،:«»`–\\\"~!]\", \"\", data)\n",
    "    # remove all english letters\n",
    "    data = re.sub(r\"[a-zA-Z]\", \"\", data)\n",
    "    # Substituting multiple spaces with single space\n",
    "    data = re.sub(r\"([^\\S\\n])+\", \" \", data, flags=re.I)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(path: str) -> str:\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        # train_set = f.read().splitlines()\n",
    "        return f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data_to_words(data: str) -> list:\n",
    "    words = re.split(r\"\\s+\", data)\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function is responsible for mapping diacritics to their corresponding strings\n",
    "def diacritic_to_str(diacritic):\n",
    "    if diacritic == SHADDA:\n",
    "        diacritic = \"SHADDA\"\n",
    "    elif diacritic == KASRA:\n",
    "        diacritic = \"KASRA\"\n",
    "    elif diacritic == DAMMA:\n",
    "        diacritic = \"DAMMA\"\n",
    "    elif diacritic == FATHA:\n",
    "        diacritic = \"FATHA\"\n",
    "    elif diacritic == KASRATAN:\n",
    "        diacritic = \"KASRATAN\"\n",
    "    elif diacritic == DAMMATAN:\n",
    "        diacritic = \"DAMMATAN\"\n",
    "    elif diacritic == FATHATAN:\n",
    "        diacritic = \"FATHATAN\"\n",
    "    elif diacritic == SUKUN:\n",
    "        diacritic = \"SUKUN\"\n",
    "    elif diacritic == DAMMA_SHADDA or diacritic == SHADDA_DAMMA :\n",
    "        diacritic = \"SHADDA_DAMMA\"\n",
    "    elif diacritic == FATHA_SHADDA or diacritic == SHADDA_FATHA:\n",
    "        diacritic = \"SHADDA_FATHA\"\n",
    "    elif diacritic == KASRA_SHADDA or diacritic == SHADDA_KASRA:\n",
    "        diacritic = \"SHADDA_KASRA\"\n",
    "    elif diacritic == DAMMATAN_SHADDA or diacritic == SHADDA_DAMMATAN:\n",
    "        diacritic = \"SHADDA_DAMMATAN\"\n",
    "    elif diacritic == FATHATAN_SHADDA or diacritic == SHADDA_FATHATAN:\n",
    "        diacritic = \"SHADDA_FATHATAN\"\n",
    "    elif diacritic == KASRATAN_SHADDA or diacritic == SHADDA_KASRATAN:\n",
    "        diacritic = \"SHADDA_KASRATAN\"\n",
    "    else:\n",
    "        diacritic = \" \"\n",
    "    return diacritic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TxtDataset(Dataset):\n",
    "    def __init__(self, path):\n",
    "        super().__init__()\n",
    "        self.x = read_data(path)\n",
    "        self.x = filter_data(self.x).split('\\n')[1] #only one line\n",
    "        self.x = split_data_to_words(self.x)\n",
    "        self.x = [word for word in self.x if word != \"\"]\n",
    "        # get the diacritic of the last character of each word\n",
    "        original_labels = []\n",
    "        for i in range(len(self.x)):\n",
    "            \n",
    "            original_labels.append(diacritic_to_str(self.x[i][-1]))\n",
    "            self.x[i] = self.x[i][:-1]\n",
    "            if self.x[i] == \"\":\n",
    "                self.x.pop(i)\n",
    "                original_labels.pop(i)\n",
    "        \n",
    "        original_labels.reverse()\n",
    "        self.y = LabelEncoder().fit_transform(original_labels)\n",
    "\n",
    "        self.encoding_mapping = dict(zip(self.y, original_labels))\n",
    "\n",
    "        # ensure the target is float\n",
    "        self.y = self.y.astype('float32')\n",
    "        self.y = self.y.reshape((len(self.y), 1))\n",
    "\n",
    "        print(self.x)\n",
    "        print(original_labels)\n",
    "        print(self.encoding_mapping)\n",
    "        print(len(self.x))\n",
    "        print(len(self.y))\n",
    "\n",
    "    # number of rows in the dataset\n",
    "    # The __len__ function returns the number of samples in our dataset.\n",
    "    def __len__(self):\n",
    "        return len(self.x)\n",
    "\n",
    "    # get a row at an index\n",
    "    # The __getitem__ function loads and returns a sample from the dataset at the given index idx\n",
    "    def __getitem__(self, idx):\n",
    "        return self.x[idx], self.y[idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Module Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(\n",
    "        self,number_of_words ,vocab_size, embedding_dim, num_filters, filter_sizes, output_dim, dropout\n",
    "    ):\n",
    "        super().__init__()\n",
    "        # embedding layer\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        # convolutional layers\n",
    "        self.convs = nn.ModuleList(\n",
    "            [\n",
    "                nn.Conv1d(\n",
    "                    in_channels=number_of_words, out_channels=num_filters, kernel_size=fs\n",
    "                )\n",
    "                for fs in filter_sizes\n",
    "            ]\n",
    "        )\n",
    "        # max pooling layers\n",
    "        self.maxpools = nn.ModuleList(\n",
    "            [nn.MaxPool1d(kernel_size=fs) for fs in filter_sizes]\n",
    "        )\n",
    "\n",
    "        # fully-connected layer\n",
    "        self.flatten_layers = nn.ModuleList(\n",
    "            [nn.Flatten() for _ in range(len(filter_sizes))]\n",
    "        )\n",
    "        # linear layer which calculates the output y = x * A.T + b\n",
    "        self.hidden1 = nn.Linear(232, number_of_words,device=device)\n",
    "        nn.init.kaiming_uniform_(self.hidden1.weight, nonlinearity='relu')\n",
    "        self.act1 = nn.ReLU()\n",
    "        # dropout layer\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.hidden2 = nn.Linear(output_dim, output_dim,device=device)\n",
    "        nn.init.xavier_uniform_(self.hidden2.weight)\n",
    "\n",
    "    def forward(self, list_of_strings: list):\n",
    "        # Define a dictionary to map strings to indices\n",
    "        word_to_index = {w:i+1 for i,w in enumerate(list_of_strings)}\n",
    "        # print(word_to_index)\n",
    "        # Convert the sequence of strings to a sequence of indices\n",
    "        indices = torch.LongTensor([word_to_index[word] for word in list_of_strings])\n",
    "        # pass text through embedding layer\n",
    "        embedded = self.embedding(indices)\n",
    "        print(\"embedded.size()\",embedded.size())\n",
    "        # embedded is len(list_of_strings) * embedding_dim\n",
    "        # initialize list for capturing output of each convolutional layer\n",
    "        conved = []\n",
    "        # pass embedded through convolutional layers and apply ReLU activation function\n",
    "        for i in range(len(self.convs)):\n",
    "            conved.append(\n",
    "                nn.ReLU()(self.convs[i](embedded))\n",
    "            )\n",
    "            print(f\"conved[{i}].size()\",conved[i].size())\n",
    "        # initialize list for capturing output of each max pooling layer\n",
    "        pooled = []\n",
    "        # pass each output of convolutional layer through max pooling layer\n",
    "        for i in range(len(self.maxpools)):\n",
    "            pooled.append(\n",
    "                self.maxpools[i](conved[i])\n",
    "            )\n",
    "            print(f\"pooled[{i}].size()\",pooled[i].size())\n",
    "        # pooled[i] = [batch size, num_filters, 1]\n",
    "        # initialize list for capturing output of each flatten layer\n",
    "        flattened = []\n",
    "        # pass each output of max pooling layer through flatten layer\n",
    "        for i in range(len(self.flatten_layers)):\n",
    "            flattened.append(\n",
    "                self.flatten_layers[i](pooled[i])\n",
    "            )\n",
    "            print(f\"flattened[{i}].size()\",flattened[i].size())\n",
    "        # flattened[i] = [batch size, num_filters]\n",
    "        # concatenate output of each flatten layer\n",
    "        cat = self.dropout(torch.cat(flattened, dim=1))\n",
    "        print(\"cat.size()\",cat.size())\n",
    "        # cat = torch.cat(flattened, dim=1)\n",
    "        # cat = [batch size, num_filters * len(filter_sizes)]\n",
    "        # pass cat through fully-connected layer\n",
    "        hedden1 = self.hidden1(cat)\n",
    "        print(\"hedden1.size()\",hedden1.size())\n",
    "        act1 = self.act1(hedden1)\n",
    "        print(\"act1.size()\",act1.size())\n",
    "        # hidden = [batch size, output dim]\n",
    "        # pass hidden through dropout layer\n",
    "        dropped1 = self.dropout(act1)\n",
    "        print(\"dropped1.size()\",dropped1.size())\n",
    "        # dropped1 = [batch size, output dim]\n",
    "        # # pass dropped through ReLU activation function\n",
    "        # act1 = self.act1(dropped1)\n",
    "        # act1 = [batch size, output dim]\n",
    "        # pass act1 through fully-connected layer\n",
    "        hidden2 = self.hidden2(dropped1)\n",
    "        print(\"hidden2.size()\",hidden2.size())\n",
    "        # hidden2 = [batch size, output dim]\n",
    "        # pass hidden2 through dropout layer\n",
    "        dropped2 = self.dropout(hidden2)\n",
    "        print(\"dropped2.size()\",dropped2.size())\n",
    "        # dropped2 = [batch size, output dim]\n",
    "        return dropped2\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train the model\n",
    "def train_model(train_dl, model):\n",
    "    # define the optimization\n",
    "    criterion = nn.CrossEntropyLoss() #  Cross-Entropy\n",
    "    # Stochastic Gradient Descent Optimizer\n",
    "    # model.parameters(): model weights\n",
    "    optimizer = Adam(model.parameters(), lr=0.01)\n",
    "    # enumerate epochs\n",
    "    for epoch in range(100):\n",
    "        # enumerate mini batches\n",
    "        for i, (inputs, targets) in enumerate(train_dl):\n",
    "            print(\"inputs\",inputs)\n",
    "            print(\"targets\",targets)\n",
    "            # clear the gradients stored in the memory as the defualt behavior of pytorch is to sum the gradients\n",
    "            # but we want to use gradients of each iteration separatly so we initialize the gradients of zeros\n",
    "            optimizer.zero_grad()\n",
    "            # compute the model output\n",
    "            yhat = model(inputs)\n",
    "            print(\"yhat\",yhat)\n",
    "\n",
    "            print(\"yhat.size()\",yhat.size())\n",
    "            print(\"targets.size()\",targets.size())\n",
    "            # calculate loss\n",
    "            loss = criterion(yhat, targets)\n",
    "            # credit assignment\n",
    "            loss.backward()\n",
    "            # update model weights\n",
    "            optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# evaluate the model\n",
    "def evaluate_model(test_dl, model):\n",
    "    predictions, actuals = list(), list()\n",
    "    for i, (inputs, targets) in enumerate(test_dl):\n",
    "        # evaluate the model on the test set\n",
    "        yhat = model(inputs)\n",
    "        # retrieve numpy array\n",
    "        # detach(): bykon fe graph fe pytorch byrbot ben al tensors fa detach btfsl al tensor da 3n ali ablo\n",
    "        yhat = yhat.detach().numpy()\n",
    "        actual = targets.numpy()\n",
    "        actual = actual.reshape((len(actual), 1))\n",
    "        # round to class values\n",
    "        yhat = yhat.round()\n",
    "        # store\n",
    "        predictions.append(yhat)\n",
    "        actuals.append(actual)\n",
    "    predictions, actuals = vstack(predictions), vstack(actuals)\n",
    "    # calculate accuracy\n",
    "    acc = accuracy_score(actuals, predictions)\n",
    "    return acc\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a class prediction for one row of data\n",
    "def predict(row, model: nn.Module):\n",
    "    # convert row to data\n",
    "    model.eval() # This is equivalent with self.train(False)\n",
    "    # model.eval : bt2ol lel model en de predict phase fa ma3tml4 operations mo3yna 34an twfr computations like drop out layers in NN m4 bst5dmha fel testing\n",
    "    # bt2fl kol al layers ali m4 m7tagha fel tetsing also storing the gradients in cache is not needed so we avoid wasting memory\n",
    "    row = Tensor([row])\n",
    "    # make prediction\n",
    "    yhat = model(row)\n",
    "    # retrieve numpy array\n",
    "    yhat = yhat.detach().numpy()\n",
    "\n",
    "    return yhat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ابْن', 'عَرَفَة', 'قَوْلُه', 'بِلَفْظ', 'يَقْتَضِي', 'كَإِنْكَار', 'غَيْر', 'حَدِيث', 'بِالْإِسْلَام', 'وُجُوب', 'مَ', 'عُلِم', 'وُجُوبُه', 'مِن', 'الدِّين', 'ضَرُورَة', 'كَإِلْقَاء', 'مُصْحَف', 'بِقَذَر', 'وَشَدّ', 'زُنَّار', 'ابْن', 'عَرَفَة', 'قَوْل', 'ابْن', 'شَاس', 'أَو', 'بِفِعْل', 'يَتَضَمَّنُه', 'هُو', 'كَلُبْس', 'الزُّنَّار', 'وَإِلْقَاء', 'الْمُصْحَف', 'فِ', 'صَرِيح', 'النَّجَاسَة', 'وَالسُّجُود', 'لِلصَّنَم', 'وَنَحْو', 'ذَلِك', 'وَسِحْر', 'مُحَمَّد', 'قَوْل', 'مَالِك', 'وَأَصْحَابِه', 'أَنّ', 'السَّاحِر', 'كَافِر', 'بِاَللَّه', 'تَعَالَ', 'قَال', 'مَالِك', 'هُو', 'كَالزِّنْدِيق', 'إذَ', 'عَمِل', 'السِّحْر', 'بِنَفْسِه', 'قُتِل', 'وَلَم', 'يُسْتَتَب']\n",
      "['SUKUN', 'SUKUN', 'FATHA', 'KASRA', 'FATHA', 'FATHA', ' ', 'KASRA', 'FATHA', 'DAMMATAN', 'FATHA', ' ', 'KASRA', 'DAMMATAN', 'FATHA', 'FATHA', 'KASRA', 'KASRATAN', 'DAMMA', 'DAMMATAN', 'KASRATAN', 'FATHA', 'KASRA', 'KASRA', 'KASRA', 'KASRA', 'KASRA', ' ', 'KASRA', 'KASRA', 'KASRA', 'KASRA', 'FATHA', 'DAMMA', 'KASRATAN', 'SUKUN', 'KASRATAN', 'KASRA', 'DAMMA', 'FATHA', 'DAMMA', 'KASRATAN', 'KASRA', 'KASRATAN', 'KASRATAN', 'KASRA', 'FATHATAN', 'KASRA', 'SUKUN', 'DAMMA', 'FATHA', ' ', 'FATHA', 'KASRA', 'KASRATAN', 'KASRA', 'KASRA', ' ', 'KASRATAN', 'DAMMA', 'FATHA', 'DAMMA']\n",
      "{7: 'SUKUN', 3: 'FATHA', 5: 'KASRA', 0: ' ', 2: 'DAMMATAN', 6: 'KASRATAN', 1: 'DAMMA', 4: 'FATHATAN'}\n",
      "62\n",
      "62\n"
     ]
    }
   ],
   "source": [
    "dataset = TxtDataset('./Dataset/train.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = random_split(dataset, [dataset.__len__(),0])\n",
    "train_dl = DataLoader(train, batch_size=32, shuffle=True)\n",
    "test_dl = DataLoader(test, batch_size=None, shuffle=False)\n",
    "encoding_mapping = dataset.encoding_mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<torch.utils.data.dataloader.DataLoader object at 0x000001E04ABBC210>\n",
      "62 0\n"
     ]
    }
   ],
   "source": [
    "print(train_dl)\n",
    "print(len(train_dl.dataset), len(test_dl.dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(dataset.x) * 2\n",
    "embedding_dim = 300\n",
    "model = CNN(\n",
    "    32,\n",
    "    vocab_size,\n",
    "    embedding_dim,\n",
    "    num_filters=1,\n",
    "    filter_sizes=[3, 4, 5],\n",
    "    output_dim=len(encoding_mapping),\n",
    "    dropout=0.5,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs ('يَتَضَمَّنُه', 'يَقْتَضِي', 'النَّجَاسَة', 'ابْن', 'مَالِك', 'أَو', 'عَمِل', 'قَوْلُه', 'قَال', 'مِن', 'مَ', 'إذَ', 'صَرِيح', 'بِلَفْظ', 'قَوْل', 'ذَلِك', 'بِفِعْل', 'شَاس', 'كَإِنْكَار', 'كَإِلْقَاء', 'تَعَالَ', 'بِالْإِسْلَام', 'وُجُوبُه', 'بِاَللَّه', 'السَّاحِر', 'الدِّين', 'وَإِلْقَاء', 'كَافِر', 'يُسْتَتَب', 'هُو', 'وُجُوب', 'الْمُصْحَف')\n",
      "targets tensor([[5.],\n",
      "        [3.],\n",
      "        [6.],\n",
      "        [7.],\n",
      "        [6.],\n",
      "        [5.],\n",
      "        [5.],\n",
      "        [3.],\n",
      "        [0.],\n",
      "        [2.],\n",
      "        [3.],\n",
      "        [5.],\n",
      "        [7.],\n",
      "        [5.],\n",
      "        [6.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [5.],\n",
      "        [3.],\n",
      "        [5.],\n",
      "        [3.],\n",
      "        [3.],\n",
      "        [5.],\n",
      "        [1.],\n",
      "        [5.],\n",
      "        [3.],\n",
      "        [3.],\n",
      "        [7.],\n",
      "        [1.],\n",
      "        [5.],\n",
      "        [2.],\n",
      "        [1.]])\n",
      "embedded.size() torch.Size([32, 300])\n",
      "conved[0].size() torch.Size([1, 298])\n",
      "conved[1].size() torch.Size([1, 297])\n",
      "conved[2].size() torch.Size([1, 296])\n",
      "pooled[0].size() torch.Size([1, 99])\n",
      "pooled[1].size() torch.Size([1, 74])\n",
      "pooled[2].size() torch.Size([1, 59])\n",
      "flattened[0].size() torch.Size([1, 99])\n",
      "flattened[1].size() torch.Size([1, 74])\n",
      "flattened[2].size() torch.Size([1, 59])\n",
      "cat.size() torch.Size([1, 232])\n",
      "hedden1.size() torch.Size([1, 32])\n",
      "act1.size() torch.Size([1, 32])\n",
      "dropped1.size() torch.Size([1, 32])\n",
      "hidden2.size() torch.Size([1, 32])\n",
      "dropped2.size() torch.Size([1, 32])\n",
      "yhat tensor([[-2.0249,  2.1978, -0.0000, -0.0000,  0.0000, -3.4113,  3.6448,  2.0494,\n",
      "         -6.0598,  0.0000,  0.0000, -0.0000,  0.0000,  4.2264,  0.0000,  0.0000,\n",
      "         -0.7334,  0.0000, -0.0000,  0.0000, -2.6676, -0.0000,  0.0000, -0.0000,\n",
      "         -0.0000,  0.0000,  0.0000,  0.1690, -0.0000, -2.0016, -0.0000, -0.0000]],\n",
      "       grad_fn=<MulBackward0>)\n",
      "yhat.size() torch.Size([1, 32])\n",
      "targets.size() torch.Size([32, 1])\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Expected input batch_size (1) to match target batch_size (32).",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[180], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m model\u001b[38;5;241m.\u001b[39mtrain(\u001b[38;5;28;01mTrue\u001b[39;00m) \u001b[38;5;66;03m# set it to False for inference \u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_dl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# evaluate the model\u001b[39;00m\n\u001b[0;32m      4\u001b[0m acc \u001b[38;5;241m=\u001b[39m evaluate_model(test_dl, model)\n",
      "Cell \u001b[1;32mIn[177], line 24\u001b[0m, in \u001b[0;36mtrain_model\u001b[1;34m(train_dl, model)\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtargets.size()\u001b[39m\u001b[38;5;124m\"\u001b[39m,targets\u001b[38;5;241m.\u001b[39msize())\n\u001b[0;32m     23\u001b[0m \u001b[38;5;66;03m# calculate loss\u001b[39;00m\n\u001b[1;32m---> 24\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[43mcriterion\u001b[49m\u001b[43m(\u001b[49m\u001b[43myhat\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtargets\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     25\u001b[0m \u001b[38;5;66;03m# credit assignment\u001b[39;00m\n\u001b[0;32m     26\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[1;32mc:\\Users\\Peter\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\Peter\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:1174\u001b[0m, in \u001b[0;36mCrossEntropyLoss.forward\u001b[1;34m(self, input, target)\u001b[0m\n\u001b[0;32m   1173\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor, target: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m-> 1174\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcross_entropy\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1175\u001b[0m \u001b[43m                           \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreduction\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreduction\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1176\u001b[0m \u001b[43m                           \u001b[49m\u001b[43mlabel_smoothing\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlabel_smoothing\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Peter\\anaconda3\\Lib\\site-packages\\torch\\nn\\functional.py:3029\u001b[0m, in \u001b[0;36mcross_entropy\u001b[1;34m(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001b[0m\n\u001b[0;32m   3027\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m size_average \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m reduce \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   3028\u001b[0m     reduction \u001b[38;5;241m=\u001b[39m _Reduction\u001b[38;5;241m.\u001b[39mlegacy_get_string(size_average, reduce)\n\u001b[1;32m-> 3029\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_nn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcross_entropy_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_Reduction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_enum\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreduction\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel_smoothing\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mValueError\u001b[0m: Expected input batch_size (1) to match target batch_size (32)."
     ]
    }
   ],
   "source": [
    "model.train(True) # set it to False for inference \n",
    "train_model(train_dl, model)\n",
    "# evaluate the model\n",
    "acc = evaluate_model(test_dl, model)\n",
    "print('Accuracy: %.3f' % acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
