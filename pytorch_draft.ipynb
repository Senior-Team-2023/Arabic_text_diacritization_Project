{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read data from train.txt and filter it from unwanted patterns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from Preprocessing import utils, character_encoding\n",
    "from Models import rnn_pytorch\n",
    "# import config as conf\n",
    "\n",
    "# config = conf.ConfigLoader().load_config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "VECTOR_SIZE = 10\n",
    "\n",
    "NUM_TRAIN_LINES = 10\n",
    "NUM_TEST_LINES = 3\n",
    "PADDING_SIZE = 150\n",
    "\n",
    "MODEL = 'lstm'\n",
    "TRAIN_MODEL = True # True if you want to train the model else load an existing model\n",
    "NUM_EPOCHS = 200\n",
    "HIDDEN_SIZE = 1000\n",
    "LEARNING_RATE = 0.001\n",
    "BATCH_SIZE = 1\n",
    "MODEL_NAME = f'model_{MODEL}_{NUM_TRAIN_LINES}L_{NUM_EPOCHS}epoch_{HIDDEN_SIZE}Hidden.pth'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing\n",
    "\n",
    "Clean data and save it (uncomment the following lines if you need to re-clean the data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def save_data(path: str, data: str):\n",
    "#     with open(path, \"w\", encoding=\"utf-8\") as f:\n",
    "#         f.write(data)\n",
    "\n",
    "# dataset = utils.read_data(\"./dataset/val.txt\")\n",
    "# filtered_dataset = utils.filter_data(dataset)\n",
    "# save_data(\"./dataset/val_filtered.txt\", filtered_dataset)\n",
    "\n",
    "# dataset = utils.read_data(\"./dataset/train.txt\")\n",
    "# filtered_dataset = utils.filter_data(dataset)\n",
    "# save_data(\"./dataset/train_filtered.txt\", filtered_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Extraction\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split training data to sentences and remove diacritics from each sentence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, test = False, eval = False, testdata = None):\n",
    "        if test:\n",
    "            dataset = testdata\n",
    "            # self.filtered_dataset = utils.filter_data(dataset)\n",
    "            self.data = utils.split_data_to_sentences(dataset)[0:NUM_TEST_LINES]\n",
    "        elif eval:\n",
    "            dataset = utils.read_data(\"./dataset/val_filtered.txt\")\n",
    "            # self.filtered_dataset = utils.filter_data(dataset)\n",
    "            self.data = utils.split_data_to_sentences(dataset)[0:NUM_TRAIN_LINES]\n",
    "        else:\n",
    "            dataset = utils.read_data(\"./dataset/train_filtered.txt\")\n",
    "            # self.filtered_dataset = utils.filter_data(dataset)\n",
    "            self.data = utils.split_data_to_sentences(dataset)[0:NUM_TRAIN_LINES]\n",
    "        self.max_length = PADDING_SIZE\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        sentence = self.data[index]\n",
    "        # separate data (sentence) and label (diacritic of each character)\n",
    "        sentence, diactritic = character_encoding.remove_diacritics(sentence, True)\n",
    "        # get sentence vector\n",
    "        sentence = character_encoding.getSentenceVector(sentence)\n",
    "        # get diacritic vector\n",
    "        diactritic = character_encoding.getDiacriticVector(diactritic)\n",
    "        # add padding to sentence vector or clip it\n",
    "        sentence,original_length = character_encoding.padding(sentence, len(character_encoding.ARABIC_ALPHABIT) +1,max_length=self.max_length)\n",
    "        diactritic,_ = character_encoding.padding(diactritic, len(character_encoding.DIACRITICS),max_length=self.max_length)\n",
    "        # convert to tensor\n",
    "        sentence = torch.tensor(sentence, dtype=(torch.float32))\n",
    "        diactritic = torch.tensor(diactritic, dtype=(torch.float32))\n",
    "        return sentence, diactritic, original_length \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building The Model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### connect to GPU if available\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device =  cuda:0\n",
      "Cuda :  True\n",
      "Number of Cuda devices : 1\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Device = \" ,device)\n",
    "print(\"Cuda : \",torch.cuda.is_available())\n",
    "print(\"Number of Cuda devices :\", torch.cuda.device_count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = len(character_encoding.ARABIC_ALPHABIT) + 1\n",
    "hidden_size = HIDDEN_SIZE\n",
    "output_size = len(character_encoding.DIACRITICS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LSTMClassifier(\n",
       "  (lstm): LSTM(38, 1000, batch_first=True)\n",
       "  (out): Sequential(\n",
       "    (0): Linear(in_features=1000, out_features=64, bias=True)\n",
       "    (1): Tanh()\n",
       "    (2): Linear(in_features=64, out_features=15, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create an instance of the RNN classifier\n",
    "if MODEL == 'rnn':\n",
    "    model = rnn_pytorch.RNNClassifier(input_size, hidden_size, output_size)\n",
    "\n",
    "# Creare an instance of the LSTM classifier\n",
    "elif MODEL == 'lstm':\n",
    "    model = rnn_pytorch.LSTMClassifier(input_size, hidden_size, output_size)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = BATCH_SIZE\n",
    "dataset = CustomDataset()\n",
    "# Create a dataloader to handle batching and shuffling\n",
    "train_dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100, Loss: 50.88290023803711\n",
      "Epoch 2/100, Loss: 42.7320671081543\n",
      "Epoch 3/100, Loss: 49.835208892822266\n",
      "Epoch 4/100, Loss: 49.17815017700195\n",
      "Epoch 5/100, Loss: 47.977882385253906\n",
      "Epoch 6/100, Loss: 46.13722610473633\n",
      "Epoch 7/100, Loss: 28.772645950317383\n",
      "Epoch 8/100, Loss: 27.94643783569336\n",
      "Epoch 9/100, Loss: 18.302244186401367\n",
      "Epoch 10/100, Loss: 27.17530632019043\n",
      "Epoch 11/100, Loss: 45.189945220947266\n",
      "Epoch 12/100, Loss: 8.86462688446045\n",
      "Epoch 13/100, Loss: 36.65714645385742\n",
      "Epoch 14/100, Loss: 8.553558349609375\n",
      "Epoch 15/100, Loss: 44.411048889160156\n",
      "Epoch 16/100, Loss: 36.02238082885742\n",
      "Epoch 17/100, Loss: 3.0328292846679688\n",
      "Epoch 18/100, Loss: 43.29643630981445\n",
      "Epoch 19/100, Loss: 43.0249137878418\n",
      "Epoch 20/100, Loss: 2.920105218887329\n",
      "Epoch 21/100, Loss: 35.76701736450195\n",
      "Epoch 22/100, Loss: 35.126495361328125\n",
      "Epoch 23/100, Loss: 15.661724090576172\n",
      "Epoch 24/100, Loss: 25.01476287841797\n",
      "Epoch 25/100, Loss: 15.462905883789062\n",
      "Epoch 26/100, Loss: 23.664382934570312\n",
      "Epoch 27/100, Loss: 15.013933181762695\n",
      "Epoch 28/100, Loss: 23.53472137451172\n",
      "Epoch 29/100, Loss: 33.66767501831055\n",
      "Epoch 30/100, Loss: 2.108614444732666\n",
      "Epoch 31/100, Loss: 39.79020309448242\n",
      "Epoch 32/100, Loss: 40.75668716430664\n",
      "Epoch 33/100, Loss: 22.73579978942871\n",
      "Epoch 34/100, Loss: 23.90559959411621\n",
      "Epoch 35/100, Loss: 38.667049407958984\n",
      "Epoch 36/100, Loss: 37.85390090942383\n",
      "Epoch 37/100, Loss: 22.288074493408203\n",
      "Epoch 38/100, Loss: 39.182918548583984\n",
      "Epoch 39/100, Loss: 38.31977844238281\n",
      "Epoch 40/100, Loss: 1.6784793138504028\n",
      "Epoch 41/100, Loss: 32.13638687133789\n",
      "Epoch 42/100, Loss: 7.427282810211182\n",
      "Epoch 43/100, Loss: 13.42521858215332\n",
      "Epoch 44/100, Loss: 37.8237419128418\n",
      "Epoch 45/100, Loss: 21.35997772216797\n",
      "Epoch 46/100, Loss: 7.265789985656738\n",
      "Epoch 47/100, Loss: 21.927536010742188\n",
      "Epoch 48/100, Loss: 36.26171875\n",
      "Epoch 49/100, Loss: 6.884518623352051\n",
      "Epoch 50/100, Loss: 36.03388214111328\n",
      "Epoch 51/100, Loss: 6.7915449142456055\n",
      "Epoch 52/100, Loss: 1.578830599784851\n",
      "Epoch 53/100, Loss: 5.7627081871032715\n",
      "Epoch 54/100, Loss: 21.071880340576172\n",
      "Epoch 55/100, Loss: 29.549781799316406\n",
      "Epoch 56/100, Loss: 20.690916061401367\n",
      "Epoch 57/100, Loss: 5.6494574546813965\n",
      "Epoch 58/100, Loss: 5.6163458824157715\n",
      "Epoch 59/100, Loss: 20.161975860595703\n",
      "Epoch 60/100, Loss: 34.473201751708984\n",
      "Epoch 61/100, Loss: 34.86796951293945\n",
      "Epoch 62/100, Loss: 7.0976948738098145\n",
      "Epoch 63/100, Loss: 12.803324699401855\n",
      "Epoch 64/100, Loss: 1.7491517066955566\n",
      "Epoch 65/100, Loss: 34.814361572265625\n",
      "Epoch 66/100, Loss: 35.3157958984375\n",
      "Epoch 67/100, Loss: 34.91172409057617\n",
      "Epoch 68/100, Loss: 1.5282975435256958\n",
      "Epoch 69/100, Loss: 20.053573608398438\n",
      "Epoch 70/100, Loss: 20.194988250732422\n",
      "Epoch 71/100, Loss: 1.5006999969482422\n",
      "Epoch 72/100, Loss: 1.4957469701766968\n",
      "Epoch 73/100, Loss: 5.33051061630249\n",
      "Epoch 74/100, Loss: 27.980270385742188\n",
      "Epoch 75/100, Loss: 28.046573638916016\n",
      "Epoch 76/100, Loss: 19.91684341430664\n",
      "Epoch 77/100, Loss: 27.973234176635742\n",
      "Epoch 78/100, Loss: 1.470838189125061\n",
      "Epoch 79/100, Loss: 33.479148864746094\n",
      "Epoch 80/100, Loss: 19.823068618774414\n",
      "Epoch 81/100, Loss: 33.44163513183594\n",
      "Epoch 82/100, Loss: 6.396221160888672\n",
      "Epoch 83/100, Loss: 19.80069923400879\n",
      "Epoch 84/100, Loss: 32.86271667480469\n",
      "Epoch 85/100, Loss: 27.87783432006836\n",
      "Epoch 86/100, Loss: 5.250355243682861\n",
      "Epoch 87/100, Loss: 19.64546012878418\n",
      "Epoch 88/100, Loss: 11.54501724243164\n",
      "Epoch 89/100, Loss: 11.545844078063965\n",
      "Epoch 90/100, Loss: 19.63359832763672\n",
      "Epoch 91/100, Loss: 19.64164161682129\n",
      "Epoch 92/100, Loss: 32.73436737060547\n",
      "Epoch 93/100, Loss: 34.40185546875\n",
      "Epoch 94/100, Loss: 6.427821159362793\n",
      "Epoch 95/100, Loss: 11.520713806152344\n",
      "Epoch 96/100, Loss: 6.348604679107666\n",
      "Epoch 97/100, Loss: 34.353572845458984\n",
      "Epoch 98/100, Loss: 19.766651153564453\n",
      "Epoch 99/100, Loss: 34.33817672729492\n",
      "Epoch 100/100, Loss: 1.4530078172683716\n"
     ]
    }
   ],
   "source": [
    "num_epochs = NUM_EPOCHS\n",
    "\n",
    "\n",
    "# Training loop\n",
    "if TRAIN_MODEL:\n",
    "    for epoch in range(num_epochs):\n",
    "        for i, (sentences, labels, _) in enumerate(train_dataloader): \n",
    "            # Reshape input and labels to (batch_size, seq_length, input_size)\n",
    "            sentences = sentences.view(batch_size, -1,input_size).to(device)\n",
    "            labels = labels.view(batch_size, -1, output_size).to(device)\n",
    "            # RNN\n",
    "            if MODEL == 'rnn':\n",
    "                hidden_state = model.init_hidden(batch_size=batch_size).to(device) # RNN has one hidden state\n",
    "                optimizer.zero_grad()\n",
    "                outputs = model(sentences, hidden_state)\n",
    "            \n",
    "            # LSTM\n",
    "            elif MODEL == 'lstm':\n",
    "                hidden_state, cell_state = model.init_hidden(batch_size=batch_size)  # LSTM has two hidden states\n",
    "                hidden_state = hidden_state.to(device)\n",
    "                cell_state = cell_state.to(device)\n",
    "                optimizer.zero_grad()\n",
    "                outputs = model(sentences, (hidden_state, cell_state))\n",
    "\n",
    "            \n",
    "            \n",
    "            # calculate the loss\n",
    "            loss = criterion(outputs, labels)\n",
    "            # backward pass\n",
    "            loss.backward()\n",
    "            # update the weights\n",
    "            optimizer.step()\n",
    "            \n",
    "\n",
    "        print(f'Epoch {epoch+1}/{num_epochs}, Loss: {loss.item()}')\n",
    "    torch.save(model.state_dict(), f'./SavedModels/{MODEL_NAME}')\n",
    "else:\n",
    "    model.load_state_dict(torch.load( f'./SavedModels/{MODEL_NAME}'))\n",
    "    print(\"Model loaded successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Evaluation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preparing Validation data to be passed into the `model.evaluate()`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Evaluation Loss: 41.80366373062134\n"
     ]
    }
   ],
   "source": [
    "# Assuming you have a test dataset prepared in the same format as your training dataset\n",
    "eval_dataset = CustomDataset(eval=True)  # You'll need to modify your CustomDataset class to accept this parameter and load the eval data\n",
    "eval_dataloader = DataLoader(eval_dataset, batch_size = batch_size)\n",
    "\n",
    "# Switch the model to evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Initialize the test loss\n",
    "test_loss = 0\n",
    "\n",
    "# We don't need to compute gradients during evaluation, so we wrap this in torch.no_grad()\n",
    "with torch.no_grad():\n",
    "    for sentences, labels, _ in eval_dataloader:\n",
    "        sentences = sentences.view(batch_size, -1, input_size).to(device)\n",
    "        labels = labels.view(batch_size, -1, output_size).to(device)\n",
    "        # RNN\n",
    "        if MODEL == 'rnn':\n",
    "            hidden_state = model.init_hidden(batch_size=batch_size).to(device) # RNN has one hidden state\n",
    "            outputs = model(sentences, hidden_state)\n",
    "        \n",
    "        # LSTM\n",
    "        elif MODEL == 'lstm':\n",
    "            hidden_state, cell_state = model.init_hidden(batch_size=batch_size)  # LSTM has two hidden states\n",
    "            hidden_state = hidden_state.to(device)\n",
    "            cell_state = cell_state.to(device)\n",
    "            outputs = model(sentences, (hidden_state, cell_state))\n",
    "\n",
    "        # Compute the loss\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Accumulate the test loss\n",
    "        test_loss += loss.item()\n",
    "\n",
    "# Compute the average test loss\n",
    "avg_test_loss = test_loss / len(eval_dataloader)\n",
    "\n",
    "print(f'Average Evaluation Loss: {avg_test_loss}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "testing on a given sentence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set = utils.read_data(f\"./Dataset/train_filtered.txt\")\n",
    "# test_set = utils.read_data(f\"./Dataset/val_filtered.txt\")\n",
    "# filtered_training_set = utils.filter_data(training_set)\n",
    "# test_sentences = utils.split_data_to_sentences(filtered_training_set)[0:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentences diacritics prediction :  (3, 150)\n",
      "Sentences Without diacritics    :  (3, 150, 38)\n",
      "Original diacritics             :  (3, 150, 15)\n"
     ]
    }
   ],
   "source": [
    "# Assuming you have a test dataset prepared in the same format as your training dataset\n",
    "test_dataset = CustomDataset(test=True, testdata = test_set) \n",
    "test_dataloader = DataLoader(test_dataset, batch_size = batch_size)\n",
    "\n",
    "# Switch the model to evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# We don't need to compute gradients during evaluation, so we wrap this in torch.no_grad()\n",
    "sentences_diacritics_prediction = []\n",
    "sentences_without_diacritics = []\n",
    "original_diacritics = []\n",
    "original_sentences_len = []\n",
    "with torch.no_grad():\n",
    "    for sentences, labels, sentence_length in test_dataloader:\n",
    "        original_sentences_len.append(sentence_length)\n",
    "        sentences_without_diacritics.extend(sentences)\n",
    "        original_diacritics.extend(labels)\n",
    "        sentences = sentences.view(batch_size, -1, input_size).to(device)\n",
    "        labels = labels.view(batch_size, -1, output_size).to(device)\n",
    "        # RNN\n",
    "        if MODEL == 'rnn':\n",
    "            hidden_state = model.init_hidden(batch_size=batch_size).to(device) # RNN has one hidden state\n",
    "            outputs = model(sentences, hidden_state)\n",
    "        \n",
    "        \n",
    "        # LSTM\n",
    "        elif MODEL == 'lstm':\n",
    "            hidden_state, cell_state = model.init_hidden(batch_size=batch_size)  # LSTM has two hidden states\n",
    "            hidden_state = hidden_state.to(device)\n",
    "            cell_state = cell_state.to(device)\n",
    "            outputs = model(sentences, (hidden_state, cell_state))\n",
    "\n",
    "        sentences_diacritics_prediction.extend(outputs.argmax(dim=2).cpu()) \n",
    "\n",
    "sentences_diacritics_prediction = np.array(sentences_diacritics_prediction)\n",
    "sentences_without_diacritics = np.array(sentences_without_diacritics)\n",
    "original_diacritics = np.array(original_diacritics)\n",
    "\n",
    "print(\"Sentences diacritics prediction : \",sentences_diacritics_prediction.shape)\n",
    "print(\"Sentences Without diacritics    : \",sentences_without_diacritics.shape)\n",
    "print(\"Original diacritics             : \",original_diacritics.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "def AverageDER(sentences_diacritics_prediction,sentences_without_diacritics,original_sentences_len):\n",
    "    diacritic_error_rate = 0\n",
    "    number_of_mis_classified = 0\n",
    "    number_of_chars_to_classify = 0\n",
    "    for i, p in enumerate(sentences_diacritics_prediction):\n",
    "        pred = character_encoding.index_to_char(p)\n",
    "        s = character_encoding.oneHot_to_sentence(sentences_without_diacritics[i][0:original_sentences_len[i]]) # sentence without diacritics\n",
    "        d = character_encoding.oneHot_to_diacritic(original_diacritics[i][0:original_sentences_len[i]])         # original diacritics of the sentence\n",
    "        original_text = character_encoding.restore_diacritics(s, d)\n",
    "        restored_text = character_encoding.restore_diacritics(s, pred)\n",
    "        diac, miss = character_encoding.diacritics_error_rate(d, pred)\n",
    "        # print(\"Original Sentence : \", original_text)\n",
    "        # print(\"Restored Sentence : \", restored_text)\n",
    "        # print(f\"DER sentence [{i}] = {diac} %\")\n",
    "        diacritic_error_rate += diac\n",
    "        number_of_mis_classified += miss\n",
    "        number_of_chars_to_classify += int(original_sentences_len[i][0]) if len(pred) > original_sentences_len[i] else len(pred)\n",
    "\n",
    "    # diacritic_error_rate /= len(sentences_diacritics_prediction)\n",
    "    diacritic_error_rate = number_of_mis_classified / number_of_chars_to_classify * 100\n",
    "    print(\"Diacritic Error Rate = \", diacritic_error_rate, \"%\")\n",
    "    print(\"Diacritic Correct Rate = \", 100 - diacritic_error_rate, \"%\")\n",
    "    print(\"Number of Misclassified = \", number_of_mis_classified, \"out of\", number_of_chars_to_classify)\n",
    "    return diacritic_error_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Diacritic Error Rate =  1.7751479289940828 %\n",
      "Diacritic Correct Rate =  98.22485207100591 %\n",
      "Number of Misclassified =  6 out of 338\n"
     ]
    }
   ],
   "source": [
    "avg_der = AverageDER(sentences_diacritics_prediction,sentences_without_diacritics,original_sentences_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Switch the model to evaluation mode\n",
    "# model.eval()\n",
    "# # Assume 'input_sentence' is your input sentence\n",
    "# input_sentence = test_sentences[0]\n",
    "# print(\"Input sentence : \",input_sentence)\n",
    "\n",
    "# # Process the input_sentence in the same way as you did for your training data\n",
    "# sentence_without_diacritics, original_diacritics = character_encoding.remove_diacritics(input_sentence, True)\n",
    "# sentence = character_encoding.getSentenceVector(sentence_without_diacritics)\n",
    "# sentence,_ = character_encoding.padding(sentence, len(character_encoding.ARABIC_ALPHABIT) + 2, max_length=PADDING_SIZE)\n",
    "# diacritic = character_encoding.getDiacriticVector(original_diacritics)\n",
    "# diacritic,_ = character_encoding.padding(diacritic, len(character_encoding.DIACRITICS), max_length=PADDING_SIZE)\n",
    "# sentence = torch.tensor(sentence, dtype=(torch.float32)).unsqueeze(0).to(device)  # Add an extra dimension for batch and move to device\n",
    "\n",
    "# # We don't need to compute gradients during evaluation, so we wrap this in torch.no_grad()\n",
    "# with torch.no_grad():\n",
    "#     hidden = model.init_hidden(batch_size=1).to(device)  # Batch size is 1 for inference\n",
    "#     # Forward pass\n",
    "#     output = model(sentence, hidden)\n",
    "# print(sentence.shape)\n",
    "\n",
    "# # The output is the model's prediction, you might want to post-process this output to convert it back into a readable format\n",
    "# prediction = output.argmax(dim=2)  # This gives you the index of the highest value in the output tensor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predicted_diacritics = character_encoding.index_to_char(prediction[0])\n",
    "# der, miss = diacritics_error_rate(original_diacritics, predicted_diacritics)\n",
    "# print(\"Diacritics error rate : \", der, \"%\")\n",
    "# print(\"Correct diacritics rate : \", 100 - der, \"%\")\n",
    "# print(\"Number of miss : \", miss, \"out of \", len(original_diacritics))\n",
    "# print(\"Original Sentence : \", input_sentence)\n",
    "# restored_sentence = character_encoding.restore_diacritics(sentence_without_diacritics,predicted_diacritics)\n",
    "# print(\"Restored Sentence : \", restored_sentence)\n",
    "# print(\"Original diacritics : \", character_encoding.map_text_to_diacritic(original_diacritics))\n",
    "# print(\"Predicted diacritics : \",character_encoding.map_text_to_diacritic( predicted_diacritics))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
