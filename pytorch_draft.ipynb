{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read data from train.txt and filter it from unwanted patterns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from Preprocessing import utils, character_encoding\n",
    "from Models import rnn_pytorch\n",
    "import config as conf\n",
    "\n",
    "# config = conf.ConfigLoader().load_config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "VECTOR_SIZE = 10\n",
    "NUM_TRAIN_LINES = 100\n",
    "PADDING_SIZE = 150\n",
    "HIDDEN_SIZE = 300\n",
    "BATCH_SIZE = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Extraction\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split training data to sentences and remove diacritics from each sentence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, test = False, eval = False, testdata = None):\n",
    "        if test:\n",
    "            dataset = testdata\n",
    "        elif eval:\n",
    "            dataset = utils.read_data(\"./dataset/val.txt\")\n",
    "        else:\n",
    "            dataset = utils.read_data(\"./dataset/train.txt\")\n",
    "        self.filtered_dataset = utils.filter_data(dataset)\n",
    "        self.data = utils.split_data_to_sentences(self.filtered_dataset)[0:NUM_TRAIN_LINES]\n",
    "        self.max_length = PADDING_SIZE\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        sentence = self.data[index]\n",
    "        # separate data (sentence) and label (diacritic of each character)\n",
    "        sentence, diactritic = character_encoding.remove_diacritics(sentence, True)\n",
    "        # get sentence vector\n",
    "        sentence = character_encoding.getSentenceVector(sentence)\n",
    "        # get diacritic vector\n",
    "        diactritic = character_encoding.getDiacriticVector(diactritic)\n",
    "        # add padding to sentence vector or clip it\n",
    "        sentence = character_encoding.padding(sentence, len(character_encoding.ARABIC_ALPHABIT) +2,max_length=self.max_length)\n",
    "        diactritic = character_encoding.padding(diactritic, len(character_encoding.DIACRITICS),max_length=self.max_length)\n",
    "        # convert to tensor\n",
    "        sentence = torch.tensor(sentence, dtype=(torch.float32))\n",
    "        diactritic = torch.tensor(diactritic, dtype=(torch.float32))\n",
    "        return sentence, diactritic\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building The Model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### connect to GPU if available\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device =  cuda:0\n",
      "Cuda :  True\n",
      "Number of Cuda devices : 1\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Device = \" ,device)\n",
    "print(\"Cuda : \",torch.cuda.is_available())\n",
    "print(\"Number of Cuda devices :\", torch.cuda.device_count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = len(character_encoding.ARABIC_ALPHABIT) + 2\n",
    "hidden_size = HIDDEN_SIZE\n",
    "output_size = len(character_encoding.DIACRITICS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LSTMClassifier(\n",
       "  (lstm): LSTM(38, 300, batch_first=True)\n",
       "  (out): Sequential(\n",
       "    (0): Linear(in_features=300, out_features=64, bias=True)\n",
       "    (1): Tanh()\n",
       "    (2): Linear(in_features=64, out_features=15, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create an instance of the RNN classifier\n",
    "# model = rnn_pytorch.RNNClassifier(input_size, hidden_size, output_size)\n",
    "\n",
    "# Creare an instance of the LSTM classifier\n",
    "model = rnn_pytorch.LSTMClassifier(input_size, hidden_size, output_size)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = BATCH_SIZE\n",
    "dataset = CustomDataset()\n",
    "# Create a dataloader to handle batching and shuffling\n",
    "train_dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100, Loss: 44.87592697143555\n",
      "Epoch 2/100, Loss: 42.99579620361328\n",
      "Epoch 3/100, Loss: 41.67938995361328\n",
      "Epoch 4/100, Loss: 8.14149284362793\n",
      "Epoch 5/100, Loss: 41.47943878173828\n",
      "Epoch 6/100, Loss: 8.693621635437012\n",
      "Epoch 7/100, Loss: 7.073116779327393\n",
      "Epoch 8/100, Loss: 6.125248432159424\n",
      "Epoch 9/100, Loss: 41.2519645690918\n",
      "Epoch 10/100, Loss: 25.252731323242188\n",
      "Epoch 11/100, Loss: 2.217221260070801\n",
      "Epoch 12/100, Loss: 8.881248474121094\n",
      "Epoch 13/100, Loss: 40.20249557495117\n",
      "Epoch 14/100, Loss: 9.963299751281738\n",
      "Epoch 15/100, Loss: 39.79909133911133\n",
      "Epoch 16/100, Loss: 39.410491943359375\n",
      "Epoch 17/100, Loss: 39.420345306396484\n",
      "Epoch 18/100, Loss: 13.74252986907959\n",
      "Epoch 19/100, Loss: 38.41596603393555\n",
      "Epoch 20/100, Loss: 38.73453140258789\n",
      "Epoch 21/100, Loss: 3.5453364849090576\n",
      "Epoch 22/100, Loss: 9.334505081176758\n",
      "Epoch 23/100, Loss: 37.98280334472656\n",
      "Epoch 24/100, Loss: 37.21031188964844\n",
      "Epoch 25/100, Loss: 24.920244216918945\n",
      "Epoch 26/100, Loss: 15.8903169631958\n",
      "Epoch 27/100, Loss: 7.488582611083984\n",
      "Epoch 28/100, Loss: 36.960601806640625\n",
      "Epoch 29/100, Loss: 36.62562942504883\n",
      "Epoch 30/100, Loss: 36.89541244506836\n",
      "Epoch 31/100, Loss: 35.209449768066406\n",
      "Epoch 32/100, Loss: 34.87484359741211\n",
      "Epoch 33/100, Loss: 20.49681854248047\n",
      "Epoch 34/100, Loss: 29.207687377929688\n",
      "Epoch 35/100, Loss: 35.20197296142578\n",
      "Epoch 36/100, Loss: 35.03670120239258\n",
      "Epoch 37/100, Loss: 34.85793685913086\n",
      "Epoch 38/100, Loss: 35.41283416748047\n",
      "Epoch 39/100, Loss: 30.616880416870117\n",
      "Epoch 40/100, Loss: 33.97972106933594\n",
      "Epoch 41/100, Loss: 30.32362174987793\n",
      "Epoch 42/100, Loss: 34.06086730957031\n",
      "Epoch 43/100, Loss: 28.472213745117188\n",
      "Epoch 44/100, Loss: 34.47309112548828\n",
      "Epoch 45/100, Loss: 32.97934341430664\n",
      "Epoch 46/100, Loss: 34.68532943725586\n",
      "Epoch 47/100, Loss: 34.42405319213867\n",
      "Epoch 48/100, Loss: 33.6807746887207\n",
      "Epoch 49/100, Loss: 14.262093544006348\n",
      "Epoch 50/100, Loss: 1.5081114768981934\n",
      "Epoch 51/100, Loss: 34.34986877441406\n",
      "Epoch 52/100, Loss: 13.602724075317383\n",
      "Epoch 53/100, Loss: 21.107057571411133\n",
      "Epoch 54/100, Loss: 34.548057556152344\n",
      "Epoch 55/100, Loss: 6.754958629608154\n",
      "Epoch 56/100, Loss: 34.09788513183594\n",
      "Epoch 57/100, Loss: 34.894100189208984\n",
      "Epoch 58/100, Loss: 6.466248512268066\n",
      "Epoch 59/100, Loss: 34.39403533935547\n",
      "Epoch 60/100, Loss: 34.35575485229492\n",
      "Epoch 61/100, Loss: 4.122403621673584\n",
      "Epoch 62/100, Loss: 28.208229064941406\n",
      "Epoch 63/100, Loss: 33.5282096862793\n",
      "Epoch 64/100, Loss: 33.881431579589844\n",
      "Epoch 65/100, Loss: 34.935264587402344\n",
      "Epoch 66/100, Loss: 34.405120849609375\n",
      "Epoch 67/100, Loss: 33.60524368286133\n",
      "Epoch 68/100, Loss: 33.769195556640625\n",
      "Epoch 69/100, Loss: 7.862987041473389\n",
      "Epoch 70/100, Loss: 1.449079990386963\n",
      "Epoch 71/100, Loss: 4.9331159591674805\n",
      "Epoch 72/100, Loss: 33.90726089477539\n",
      "Epoch 73/100, Loss: 1.4589475393295288\n",
      "Epoch 74/100, Loss: 33.99819564819336\n",
      "Epoch 75/100, Loss: 26.86960792541504\n",
      "Epoch 76/100, Loss: 33.63026809692383\n",
      "Epoch 77/100, Loss: 33.924930572509766\n",
      "Epoch 78/100, Loss: 5.87746524810791\n",
      "Epoch 79/100, Loss: 19.74960708618164\n",
      "Epoch 80/100, Loss: 32.71049880981445\n",
      "Epoch 81/100, Loss: 10.579305648803711\n",
      "Epoch 82/100, Loss: 33.21967697143555\n",
      "Epoch 83/100, Loss: 7.459266662597656\n",
      "Epoch 84/100, Loss: 11.331025123596191\n",
      "Epoch 85/100, Loss: 32.49153137207031\n",
      "Epoch 86/100, Loss: 15.003186225891113\n",
      "Epoch 87/100, Loss: 7.237797737121582\n",
      "Epoch 88/100, Loss: 6.524464130401611\n",
      "Epoch 89/100, Loss: 32.6569709777832\n",
      "Epoch 90/100, Loss: 13.45315933227539\n",
      "Epoch 91/100, Loss: 10.578023910522461\n",
      "Epoch 92/100, Loss: 32.475189208984375\n",
      "Epoch 93/100, Loss: 34.27277374267578\n",
      "Epoch 94/100, Loss: 7.428595542907715\n",
      "Epoch 95/100, Loss: 32.3201904296875\n",
      "Epoch 96/100, Loss: 32.471351623535156\n",
      "Epoch 97/100, Loss: 7.83322286605835\n",
      "Epoch 98/100, Loss: 5.744542598724365\n",
      "Epoch 99/100, Loss: 11.190855026245117\n",
      "Epoch 100/100, Loss: 34.85684585571289\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 100\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (sentences, labels) in enumerate(train_dataloader): \n",
    "        # Reshape input and labels to (batch_size, seq_length, input_size)\n",
    "        input = sentences.view(batch_size, -1,input_size).to(device)\n",
    "        labels = labels.view(batch_size, -1, output_size).to(device)\n",
    "        # RNN\n",
    "        # hidden_state = model.init_hidden(batch_size=batch_size).to(device) # RNN has one hidden state\n",
    "        # optimizer.zero_grad()\n",
    "        # outputs = model(input, hidden_state)\n",
    "        \n",
    "        # LSTM\n",
    "        hidden_state, cell_state = model.init_hidden(batch_size=batch_size)  # LSTM has two hidden states\n",
    "        hidden_state = hidden_state.to(device)\n",
    "        cell_state = cell_state.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(input, (hidden_state, cell_state))\n",
    "\n",
    "        \n",
    "        \n",
    "        # calculate the loss\n",
    "        loss = criterion(outputs, labels)\n",
    "        # backward pass\n",
    "        loss.backward()\n",
    "        # update the weights\n",
    "        optimizer.step()\n",
    "        \n",
    "\n",
    "    print(f'Epoch {epoch+1}/{num_epochs}, Loss: {loss.item()}')\n",
    "# torch.save(model.state_dict(), 'model.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Evaluation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preparing Validation data to be passed into the `model.evaluate()`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "testing on a given sentence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Evaluation Loss: 34.41014338582754\n"
     ]
    }
   ],
   "source": [
    "# Assuming you have a test dataset prepared in the same format as your training dataset\n",
    "eval_dataset = CustomDataset(eval=True)  # You'll need to modify your CustomDataset class to accept this parameter and load the eval data\n",
    "eval_dataloader = DataLoader(eval_dataset, batch_size = batch_size)\n",
    "\n",
    "# Switch the model to evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Initialize the test loss\n",
    "test_loss = 0\n",
    "\n",
    "# We don't need to compute gradients during evaluation, so we wrap this in torch.no_grad()\n",
    "with torch.no_grad():\n",
    "    for sentences, labels in eval_dataloader:\n",
    "        sentences = sentences.view(batch_size, -1, input_size).to(device)\n",
    "        labels = labels.view(batch_size, -1, output_size).to(device)\n",
    "        # RNN\n",
    "        # hidden_state = model.init_hidden(batch_size=batch_size).to(device) # RNN has one hidden state\n",
    "        # outputs = model(input, hidden_state)\n",
    "        \n",
    "        # LSTM\n",
    "        hidden_state, cell_state = model.init_hidden(batch_size=batch_size)  # LSTM has two hidden states\n",
    "        hidden_state = hidden_state.to(device)\n",
    "        cell_state = cell_state.to(device)\n",
    "        outputs = model(input, (hidden_state, cell_state))\n",
    "\n",
    "        # Compute the loss\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Accumulate the test loss\n",
    "        test_loss += loss.item()\n",
    "\n",
    "# Compute the average test loss\n",
    "avg_test_loss = test_loss / len(eval_dataloader)\n",
    "\n",
    "print(f'Average Evaluation Loss: {avg_test_loss}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def index_to_char(indeces_list):\n",
    "    char_list = []\n",
    "    for index in indeces_list:\n",
    "        char_list.append(character_encoding.DIACRITICS[index])\n",
    "    return char_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set = utils.read_data(f\"./Dataset/train.txt\")\n",
    "filtered_training_set = utils.filter_data(training_set)\n",
    "test_sentences = utils.split_data_to_sentences(filtered_training_set)[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentences diacritics prediction :  (100, 150)\n",
      "Sentences Without diacritics    :  (100, 150, 38)\n",
      "Original diacritics             :  (100, 150, 15)\n"
     ]
    }
   ],
   "source": [
    "# Assuming you have a test dataset prepared in the same format as your training dataset\n",
    "test_dataset = CustomDataset(test=True, testdata = training_set) \n",
    "test_dataloader = DataLoader(test_dataset, batch_size = batch_size)\n",
    "\n",
    "# Switch the model to evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# We don't need to compute gradients during evaluation, so we wrap this in torch.no_grad()\n",
    "sentences_diacritics_prediction = []\n",
    "sentences_without_diacritics = []\n",
    "original_diacritics = []\n",
    "with torch.no_grad():\n",
    "    for sentences, labels in test_dataloader:\n",
    "        sentences_without_diacritics.extend(sentences)\n",
    "        original_diacritics.extend(labels)\n",
    "        sentences = sentences.view(batch_size, -1, input_size).to(device)\n",
    "        labels = labels.view(batch_size, -1, output_size).to(device)\n",
    "        # RNN\n",
    "        # hidden_state = model.init_hidden(batch_size=batch_size).to(device) # RNN has one hidden state\n",
    "        # outputs = model(input, hidden_state)\n",
    "        \n",
    "        # LSTM\n",
    "        hidden_state, cell_state = model.init_hidden(batch_size=batch_size)  # LSTM has two hidden states\n",
    "        hidden_state = hidden_state.to(device)\n",
    "        cell_state = cell_state.to(device)\n",
    "        outputs = model(input, (hidden_state, cell_state))\n",
    "\n",
    "        sentences_diacritics_prediction.extend(outputs.argmax(dim=2).cpu()) \n",
    "\n",
    "sentences_diacritics_prediction = np.array(sentences_diacritics_prediction)\n",
    "sentences_without_diacritics = np.array(sentences_without_diacritics)\n",
    "original_diacritics = np.array(original_diacritics)\n",
    "\n",
    "print(\"Sentences diacritics prediction : \",sentences_diacritics_prediction.shape)\n",
    "print(\"Sentences Without diacritics    : \",sentences_without_diacritics.shape)\n",
    "print(\"Original diacritics             : \",original_diacritics.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Switch the model to evaluation mode\n",
    "# model.eval()\n",
    "# # Assume 'input_sentence' is your input sentence\n",
    "# input_sentence = test_sentences[5]\n",
    "\n",
    "# # Process the input_sentence in the same way as you did for your training data\n",
    "# sentence_without_diacritics, original_diacritics = character_encoding.remove_diacritics(input_sentence, True)\n",
    "# sentence = character_encoding.getSentenceVector(sentence_without_diacritics)\n",
    "# sentence = character_encoding.padding(sentence, len(character_encoding.ARABIC_ALPHABIT) + 2, max_length=PADDING_SIZE)\n",
    "# diacritic = character_encoding.getDiacriticVector(original_diacritics)\n",
    "# diacritic = character_encoding.padding(diacritic, len(character_encoding.DIACRITICS), max_length=PADDING_SIZE)\n",
    "# sentence = torch.tensor(sentence, dtype=(torch.float32)).unsqueeze(0).to(device)  # Add an extra dimension for batch and move to device\n",
    "\n",
    "# # We don't need to compute gradients during evaluation, so we wrap this in torch.no_grad()\n",
    "# with torch.no_grad():\n",
    "#     hidden = model.init_hidden(batch_size=1)  # Batch size is 1 for inference\n",
    "#     # Forward pass\n",
    "#     output = model(sentence, hidden)\n",
    "# print(sentence.shape)\n",
    "\n",
    "# # The output is the model's prediction, you might want to post-process this output to convert it back into a readable format\n",
    "# prediction = output.argmax(dim=2)  # This gives you the index of the highest value in the output tensor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def oneHot_to_sentence(list_of_oneHot):\n",
    "    sentence = \"\"\n",
    "    l = character_encoding.ARABIC_ALPHABIT + \" \" + \"\\n\"\n",
    "    for oneHot in list_of_oneHot:\n",
    "        sentence += l[oneHot.argmax()]\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def oneHot_to_diacritic(list_of_oneHot):\n",
    "    sentence = []\n",
    "    l = character_encoding.DIACRITICS\n",
    "    for oneHot in list_of_oneHot:\n",
    "        sentence.append(l[oneHot.argmax()])\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def diacritics_error_rate(original_diacritics, predicted_diacritics):\n",
    "    error = 0\n",
    "    for i in range(len(original_diacritics)):\n",
    "        if original_diacritics[i] != predicted_diacritics[i]:\n",
    "            error += 1\n",
    "    return error / len(original_diacritics) * 100, error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the prediction tensor to the corresponding diacritics\n",
    "predicted_diacritics = []\n",
    "diacritic_error_rate = 0\n",
    "number_of_mis_classified = 0\n",
    "number_of_char_to_classify = 0\n",
    "for i, p in enumerate(sentences_diacritics_prediction):\n",
    "    pred = index_to_char(p)\n",
    "    predicted_diacritics.append(pred)\n",
    "    s = oneHot_to_sentence(sentences_without_diacritics[i])\n",
    "    restored_text = character_encoding.restore_diacritics(s[0:len(pred)], pred)\n",
    "    d = oneHot_to_diacritic(original_diacritics[i])\n",
    "    diac, miss = diacritics_error_rate(d[0:len(predicted_diacritics)], pred)\n",
    "    diacritic_error_rate += diac\n",
    "    number_of_mis_classified += miss\n",
    "    number_of_char_to_classify += len(predicted_diacritics)\n",
    "    # print(\"Restored\", restored_text)\n",
    "\n",
    "diacritic_error_rate /= len(sentences_diacritics_prediction)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Diacritic Error Rate =  24.854964124893126 %\n",
      "Diacritic Correct Rate =  75.14503587510687 %\n",
      "Number of Misclassified =  1368 out of 5050\n"
     ]
    }
   ],
   "source": [
    "print(\"Diacritic Error Rate = \", diacritic_error_rate, \"%\")\n",
    "print(\"Diacritic Correct Rate = \", 100 - diacritic_error_rate, \"%\")\n",
    "print(\"Number of Misclassified = \", number_of_mis_classified, \"out of\", number_of_char_to_classify)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
