{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read data from train.txt and filter it from unwanted patterns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configurations : \n",
      "number_test_of_words : 10000\n",
      "number_validation_of_words : 1000\n",
      "classifier : lstm\n",
      "embedding : fasttext\n",
      "is_training : True\n",
      "word_embeddings : False\n",
      "character_embeddings : False\n",
      "embedding_vector_size : 100\n",
      "character_embedding_vector_size : 200\n",
      "batch_size : 64\n",
      "num_epochs : 7\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from keras.utils import to_categorical\n",
    "from Embeddings import Word2Vec, FastText\n",
    "from Preprocessing import utils, character_encoding\n",
    "from Models import rnn, lstm, bilstm, rnn_pytorch\n",
    "import config as conf\n",
    "\n",
    "config = conf.ConfigLoader().load_config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VECTOR_SIZE = config['embedding_vector_size ']\n",
    "VECTOR_SIZE = 10\n",
    "NUM_TRAIN_LINES = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clean data from special characcters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training_set قَوْلُهُ : ( أَوْ قَطَعَ الْأَوَّلُ يَدَهُ إلَخْ ) قَالَ الزَّرْكَشِيُّ( 14 / 123 )\n",
      "ابْنُ عَرَفَةَ : قَوْلُهُ : بِلَفْظٍ يَقْتَضِيه كَإِنْكَارِ غَيْرِ حَدِيثٍ بِالْإِسْلَامِ وُجُوبَ مَا عُلِمَ وُجُوبُهُ مِنْ الدِّينِ ضَرُورَةً ( كَإِلْقَاءِ مُصْحَفٍ بِقَذَرٍ وَشَدِّ زُنَّارٍ ) ابْنُ عَرَفَةَ : قَوْلُ ابْنِ شَاسٍ : أَوْ بِفِعْلٍ يَتَضَمَّنُهُ هُوَ كَلُبْسِ الزُّنَّارِ وَإِلْقَاءِ الْمُصْحَفِ فِي صَرِيحِ النَّجَاسَةِ وَالسُّجُودِ لِلصَّنَمِ وَنَحْوِ ذَلِكَ ( وَسِحْرٍ ) مُحَمَّدٌ : قَوْلُ مَالِكٍ و\n",
      "\n",
      "\n",
      "filtered_training_set قَوْلُهُ أَوْ قَطَعَ الْأَوَّلُ يَدَهُ إلَخْ قَالَ الزَّرْكَشِيُّ \n",
      "ابْنُ عَرَفَةَ قَوْلُهُ بِلَفْظٍ يَقْتَضِيه كَإِنْكَارِ غَيْرِ حَدِيثٍ بِالْإِسْلَامِ وُجُوبَ مَا عُلِمَ وُجُوبُهُ مِنْ الدِّينِ ضَرُورَةً كَإِلْقَاءِ مُصْحَفٍ بِقَذَرٍ وَشَدِّ زُنَّارٍ ابْنُ عَرَفَةَ قَوْلُ ابْنِ شَاسٍ أَوْ بِفِعْلٍ يَتَضَمَّنُهُ هُوَ كَلُبْسِ الزُّنَّارِ وَإِلْقَاءِ الْمُصْحَفِ فِي صَرِيحِ النَّجَاسَةِ وَالسُّجُودِ لِلصَّنَمِ وَنَحْوِ ذَلِكَ وَسِحْرٍ مُحَمَّدٌ قَوْلُ مَالِكٍ وَأَصْحَابِهِ أَنَّ السَّاحِرَ كَافِ\n"
     ]
    }
   ],
   "source": [
    "training_set = utils.read_data(f\"./Dataset/train.txt\")\n",
    "print(\"training_set\", training_set[0:500])\n",
    "print('\\n')\n",
    "filtered_training_set = utils.filter_data(training_set)\n",
    "print(\"filtered_training_set\", filtered_training_set[0:500])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation_set ( 27 ) قَوْلُهُ : وَلَا تُكْرَهُ ضِيَافَتُهُ .\n",
      "( الْفَرْقُ الثَّالِثُ وَالثَّلَاثُونَ بَيْنَ قَاعِدَةِ تَقَدُّمِ الْحُكْمِ عَلَى سَبَبِهِ دُونَ شَرْطِهِ أَوْ شَرْطِهِ دُونَ سَبَبِهِ وَبَيْنَ قَاعِدَةِ تَقَدُّمِهِ عَلَى السَّبَبِ وَالشَّرْطِ جَمِيعًا ) وَتَحْرِيرُهُ أَنَّ الْحُكْمَ إنْ كَانَ لَهُ سَبَبٌ بِغَيْرِ شَرْطٍ فَتَقَدَّمَ عَلَيْهِ لَا يُعْتَبَرُ أَوْ كَانَ لَهُ سَبَبَانِ أَوْ أَسْبَابٌ فَتَقَدَّمَ عَلَى جَمِيعِهَا لَمْ يُعْتَبَرْ أَوْ عَلَى بَعْضِهَا دُونَ بَعْضٍ اُعْتُبِرَ بِنَاءً عَلَى\n",
      "\n",
      "\n",
      "filtered_validation_set  قَوْلُهُ وَلَا تُكْرَهُ ضِيَافَتُهُ \n",
      " الْفَرْقُ الثَّالِثُ وَالثَّلَاثُونَ بَيْنَ قَاعِدَةِ تَقَدُّمِ الْحُكْمِ عَلَى سَبَبِهِ دُونَ شَرْطِهِ أَوْ شَرْطِهِ دُونَ سَبَبِهِ وَبَيْنَ قَاعِدَةِ تَقَدُّمِهِ عَلَى السَّبَبِ وَالشَّرْطِ جَمِيعًا وَتَحْرِيرُهُ أَنَّ الْحُكْمَ إنْ كَانَ لَهُ سَبَبٌ بِغَيْرِ شَرْطٍ فَتَقَدَّمَ عَلَيْهِ لَا يُعْتَبَرُ أَوْ كَانَ لَهُ سَبَبَانِ أَوْ أَسْبَابٌ فَتَقَدَّمَ عَلَى جَمِيعِهَا لَمْ يُعْتَبَرْ أَوْ عَلَى بَعْضِهَا دُونَ بَعْضٍ اُعْتُبِرَ بِنَاءً عَلَى سَبَبِ الْخ\n"
     ]
    }
   ],
   "source": [
    "validation_set = utils.read_data(f\"./Dataset/val.txt\")\n",
    "print(\"validation_set\", validation_set[0:500])\n",
    "print('\\n')\n",
    "filtered_validation_set = utils.filter_data(validation_set)\n",
    "print(\"filtered_validation_set\", filtered_validation_set[0:500])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Splite Training data and Validation data into words then separate diacritics from each word\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Extraction\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split training data to sentences and remove diacritics from each sentence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, data, labels):\n",
    "        # self.dataset = np.loadtxt(\"./dataset/train.txt\")\n",
    "        self.data = torch.from_numpy(data.astype(np.float32))\n",
    "        self.labels = torch.from_numpy(labels.astype(np.float32))\n",
    "        \n",
    "        self.n_samples = data.shape[0]\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # print(\"self.data[index]\", self.data[index])\n",
    "        # print(\"self.labels[index]\", self.labels[index])\n",
    "        # \n",
    "        return self.data[index], self.labels[index]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.n_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "قَوْلُهُ أَوْ قَطَعَ الْأَوَّلُ يَدَهُ إلَخْ قَالَ الزَّرْكَشِيُّ \n"
     ]
    }
   ],
   "source": [
    "\n",
    "sentences = utils.split_data_to_sentences(filtered_training_set)[0:NUM_TRAIN_LINES]\n",
    "max_sentence_length = utils.get_max_len(sequences=[sentences])\n",
    "sentences_without_diacritics, sentences_diacritics = character_encoding.RemoveDiacriticFromSentence(sentences)\n",
    "print(sentences[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "testing new approach\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_model = FastText.FastTextEmbedding(sentences_without_diacritics, vector_size = VECTOR_SIZE)\n",
    "embedding_model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sentences_without_diacritics( 38 ) قوله أو قطع الأول يده إلخ قال الزركشي \n",
      "sentences_diacritics( 38 ) ['FATHA', 'SUKUN', 'DAMMA', 'DAMMA', ' ', 'FATHA', 'SUKUN', ' ', 'FATHA', 'FATHA', 'FATHA', ' ', ' ', 'SUKUN', 'FATHA', 'SHADDA_FATHA', 'DAMMA', ' ', 'FATHA', 'FATHA', 'DAMMA', ' ', ' ', 'FATHA', 'SUKUN', ' ', 'FATHA', ' ', 'FATHA', ' ', ' ', ' ', 'SHADDA_FATHA', 'SUKUN', 'FATHA', 'KASRA', 'SHADDA_DAMMA', ' ']\n",
      "original_Text قَوْلُهُ أَوْ قَطَعَ الْأَوَّلُ يَدَهُ إلَخْ قَالَ الزَّرْكَشِيُّ \n",
      "restored_text قَوْلُهُ أَوْ قَطَعَ الْأَوَّلُ يَدَهُ إلَخْ قَالَ الزَّرْكَشِيُّ \n"
     ]
    }
   ],
   "source": [
    "index = 0\n",
    "sentence_to_test = sentences_without_diacritics[index]\n",
    "diacritic_list_to_test = sentences_diacritics[index]\n",
    "\n",
    "print(\"sentences_without_diacritics(\",len(sentence_to_test),\")\", sentence_to_test)\n",
    "print(\"sentences_diacritics(\",len(diacritic_list_to_test),\")\", character_encoding.map_text_to_diacritic(diacritic_list_to_test))\n",
    "\n",
    "# words = utils.split_data_to_words(sentence_to_test)\n",
    "restored_text = character_encoding.restore_diacritics(sentence_to_test, diacritic_list_to_test)\n",
    "print(\"original_Text\", sentences[index])\n",
    "print(\"restored_text\", restored_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_spaces(text, diacritic_list):\n",
    "    text = list(text)\n",
    "    copy_diacritic_list = diacritic_list.copy()\n",
    "    for i,c in enumerate(text):\n",
    "        if c.isspace():\n",
    "            text.pop(i)\n",
    "            copy_diacritic_list.pop(i)\n",
    "    return text, copy_diacritic_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def split_data_to_words(data: str) -> list:\n",
    "    words = re.split(r\"[^\\S\\n]+\", data)\n",
    "    # # remove empty words\n",
    "    # words = [word for word in words if word]\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # input is one hot incodeing including spaces and with word embedding\n",
    "# concatinated_vector_train = []\n",
    "# for i,sentence in enumerate(sentences_without_diacritics):\n",
    "#     words = split_data_to_words(sentence)\n",
    "#     for j, word in enumerate(words):\n",
    "#         try:\n",
    "#             word_vec = embedding_model.vector(word)\n",
    "#         except:\n",
    "#             print(\"word not found : \\\"\", word , \"\\\"\")\n",
    "        \n",
    "#         for k,c in enumerate(word):\n",
    "#             one_hot = character_encoding.CharToOneHOt(c)\n",
    "#             v = np.concatenate((word_vec, one_hot), axis=None)\n",
    "#             concatinated_vector_train.append(v)\n",
    "\n",
    "#         # add space between words except for the last word\n",
    "#         if j != len(words) - 1:    \n",
    "#             one_hot = character_encoding.CharToOneHOt(' ')\n",
    "#             v = np.concatenate((word_vec, one_hot), axis = None)\n",
    "#             concatinated_vector_train.append(v) \n",
    "\n",
    "\n",
    "# c = 0\n",
    "# for diacritics in sentences_diacritics:\n",
    "#     for diacritic in diacritics:\n",
    "#         c += 1\n",
    "# # print(\"vector example :\", v)\n",
    "# print(\"len concatinated :\", len(concatinated_vector_train))\n",
    "# print(\"len diacritics   :\", c)\n",
    "# print(\"Diff = \", len(concatinated_vector_train) - c)\n",
    "# print(\"Original sentence :\", sentences[0])\n",
    "# print(\"First sentence    :\", sentences_without_diacritics[0])\n",
    "# print(\"First diacritics  :\", character_encoding.map_text_to_diacritic(sentences_diacritics[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len sentence  323\n",
      "len diacritics  2\n",
      "sentence  ابن عرفة قوله بلفظ يقتضيه كإنكار غير حديث بالإسلام وجوب ما علم وجوبه من الدين ضرورة كإلقاء مصحف بقذر وشد زنار ابن عرفة قول ابن شاس أو بفعل يتضمنه هو كلبس الزنار وإلقاء المصحف في صريح النجاسة والسجود للصنم ونحو ذلك وسحر محمد قول مالك وأصحابه أن الساحر كافر بالله تعالى قال مالك هو كالزنديق إذا عمل السحر بنفسه قتل ولم يستتب \n",
      "diacritics  ['', 'ْ', 'ُ', '', 'َ', 'َ', 'َ', 'َ', '', 'َ', 'ْ', 'ُ', 'ُ', '', 'ِ', 'َ', 'ْ', 'ٍ', '', 'َ', 'ْ', 'َ', 'ِ', '', '', '', 'َ', 'ِ', 'ْ', 'َ', '', 'ِ', '', 'َ', 'ْ', 'ِ', '', 'َ', 'ِ', '', 'ٍ', '', 'ِ', '', 'ْ', 'ِ', 'ْ', 'َ', '', 'ِ', '', 'ُ', 'ُ', '', 'َ', '', 'َ', '', '', 'ُ', 'ِ', 'َ', '', 'ُ', 'ُ', '', 'ُ', 'ُ', '', 'ِ', 'ْ', '', '', '', 'ِّ', '', 'ِ', '', 'َ', 'ُ', '', 'َ', 'ً', '', 'َ', 'ِ', 'ْ', 'َ', '', 'ِ', '', 'ُ', 'ْ', 'َ', 'ٍ', '', 'ِ', 'َ', 'َ', 'ٍ', '', 'َ', 'َ', 'ِّ', '', 'ُ', 'َّ', '', 'ٍ', '', '', 'ْ', 'ُ', '', 'َ', 'َ', 'َ', 'َ', '', 'َ', 'ْ', 'ُ', '', '', 'ْ', 'ِ', '', 'َ', '', 'ٍ', '', 'َ', 'ْ', '', 'ِ', 'ِ', 'ْ', 'ٍ', '', 'َ', 'َ', 'َ', 'َّ', 'ُ', 'ُ', '', 'ُ', 'َ', '', 'َ', 'ُ', 'ْ', 'ِ', '', '', '', 'ُّ', 'َّ', '', 'ِ', '', 'َ', 'ِ', 'ْ', 'َ', '', 'ِ', '', '', 'ْ', 'ُ', 'ْ', 'َ', 'ِ', '', 'ِ', '', '', 'َ', 'ِ', '', 'ِ', '', '', '', 'َّ', 'َ', '', 'َ', 'ِ', '', 'َ', '', '', 'ُّ', 'ُ', '', 'ِ', '', 'ِ', '', 'َّ', 'َ', 'ِ', '', 'َ', 'َ', 'ْ', 'ِ', '', 'َ', 'ِ', 'َ', '', 'َ', 'ِ', 'ْ', 'ٍ', '', 'ُ', 'َ', 'َّ', 'ٌ', '', 'َ', 'ْ', 'ُ', '', 'َ', '', 'ِ', 'ٍ', '', 'َ', 'َ', 'ْ', 'َ', '', 'ِ', 'ِ', '', 'َ', 'َّ', '', '', '', 'َّ', '', 'ِ', 'َ', '', 'َ', '', 'ِ', 'ٌ', '', 'ِ', 'َ', '', 'َّ', 'ِ', '', 'َ', 'َ', '', 'َ', '', '', 'َ', '', 'َ', '', 'َ', '', 'ِ', 'ٌ', '', 'ُ', 'َ', '', 'َ', '', '', 'ِّ', 'ْ', 'ِ', '', 'ِ', '', '', 'َ', '', '', 'َ', 'ِ', 'َ', '', '', '', 'ِّ', 'ْ', 'َ', '', 'ِ', 'َ', 'ْ', 'ِ', 'ِ', '', 'ُ', 'ِ', 'َ', '', 'َ', 'َ', 'ْ', '', 'ُ', 'ْ', 'َ', 'َ', 'ْ', '']\n",
      "Original sentence : قَوْلُهُ أَوْ قَطَعَ الْأَوَّلُ يَدَهُ إلَخْ قَالَ الزَّرْكَشِيُّ \n",
      "First sentence    : قوله أو قطع الأول يده إلخ قال الزركشي \n",
      "First diacritics  : ['FATHA', 'SUKUN', 'DAMMA', 'DAMMA', ' ', 'FATHA', 'SUKUN', ' ', 'FATHA', 'FATHA', 'FATHA', ' ', ' ', 'SUKUN', 'FATHA', 'SHADDA_FATHA', 'DAMMA', ' ', 'FATHA', 'FATHA', 'DAMMA', ' ', ' ', 'FATHA', 'SUKUN', ' ', 'FATHA', ' ', 'FATHA', ' ', ' ', ' ', 'SHADDA_FATHA', 'SUKUN', 'FATHA', 'KASRA', 'SHADDA_DAMMA', ' ']\n"
     ]
    }
   ],
   "source": [
    "# input is one hot incodeing including spaces\n",
    "concatinated_vector_train = []\n",
    "for i,sentence in enumerate(sentences_without_diacritics):\n",
    "    sentence_vec = []\n",
    "    for j,c in enumerate(sentence):\n",
    "        one_hot = character_encoding.CharToOneHOt(c)\n",
    "        sentence_vec.append(one_hot)\n",
    "    concatinated_vector_train.append(sentence_vec)\n",
    "\n",
    "print(\"len sentence \", len(sentences_without_diacritics[-1]))\n",
    "print(\"len diacritics \", len(sentences_diacritics))\n",
    "print(\"sentence \", sentences_without_diacritics[-1])\n",
    "print(\"diacritics \", sentences_diacritics[-1])\n",
    "print(\"Original sentence :\", sentences[0])\n",
    "print(\"First sentence    :\", sentences_without_diacritics[0])\n",
    "print(\"First diacritics  :\", character_encoding.map_text_to_diacritic(sentences_diacritics[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building The Model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preparing Training data to be passed into the `model.train()`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train :  [[[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0], [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0], [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0], [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0], [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]], [[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0], [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0], [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0], [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0], [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0], [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0], [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0], [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0], [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0], [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0], [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0], [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0], [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0], [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0], [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0], [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0], [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0], [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0], [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0], [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0], [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0], [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0], [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0], [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0], [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0], [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0], [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]]]\n",
      "y_train :  [array([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "      dtype=float32), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.],\n",
      "      dtype=float32), array([1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "      dtype=float32), array([1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "      dtype=float32), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
      "      dtype=float32), array([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "      dtype=float32), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.],\n",
      "      dtype=float32), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
      "      dtype=float32), array([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "      dtype=float32), array([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "      dtype=float32), array([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "      dtype=float32), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
      "      dtype=float32), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
      "      dtype=float32), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.],\n",
      "      dtype=float32), array([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "      dtype=float32), array([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
      "      dtype=float32), array([1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "      dtype=float32), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
      "      dtype=float32), array([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "      dtype=float32), array([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "      dtype=float32), array([1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "      dtype=float32), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
      "      dtype=float32), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
      "      dtype=float32), array([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "      dtype=float32), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.],\n",
      "      dtype=float32), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
      "      dtype=float32), array([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "      dtype=float32), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
      "      dtype=float32), array([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "      dtype=float32), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
      "      dtype=float32), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
      "      dtype=float32), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
      "      dtype=float32), array([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
      "      dtype=float32), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.],\n",
      "      dtype=float32), array([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "      dtype=float32), array([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "      dtype=float32), array([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "      dtype=float32), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
      "      dtype=float32), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
      "      dtype=float32), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.],\n",
      "      dtype=float32), array([1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "      dtype=float32), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
      "      dtype=float32), array([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "      dtype=float32), array([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "      dtype=float32), array([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "      dtype=float32), array([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "      dtype=float32), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
      "      dtype=float32), array([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "      dtype=float32), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.],\n",
      "      dtype=float32), array([1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "      dtype=float32), array([1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "      dtype=float32), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
      "      dtype=float32), array([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "      dtype=float32), array([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "      dtype=float32), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.],\n",
      "      dtype=float32), array([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "      dtype=float32), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
      "      dtype=float32), array([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "      dtype=float32), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.],\n",
      "      dtype=float32), array([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "      dtype=float32), array([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "      dtype=float32), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
      "      dtype=float32), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
      "      dtype=float32), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
      "      dtype=float32), array([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "      dtype=float32), array([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "      dtype=float32), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.],\n",
      "      dtype=float32), array([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "      dtype=float32), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
      "      dtype=float32), array([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "      dtype=float32), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
      "      dtype=float32), array([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "      dtype=float32), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.],\n",
      "      dtype=float32), array([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "      dtype=float32), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
      "      dtype=float32), array([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "      dtype=float32), array([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "      dtype=float32), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
      "      dtype=float32), array([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "      dtype=float32), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
      "      dtype=float32), array([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "      dtype=float32), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
      "      dtype=float32), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.],\n",
      "      dtype=float32), array([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "      dtype=float32), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.],\n",
      "      dtype=float32), array([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "      dtype=float32), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
      "      dtype=float32), array([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "      dtype=float32), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
      "      dtype=float32), array([1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "      dtype=float32), array([1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "      dtype=float32), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
      "      dtype=float32), array([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "      dtype=float32), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
      "      dtype=float32), array([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "      dtype=float32), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
      "      dtype=float32), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
      "      dtype=float32), array([1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "      dtype=float32), array([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "      dtype=float32), array([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "      dtype=float32), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
      "      dtype=float32), array([1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "      dtype=float32), array([1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "      dtype=float32), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
      "      dtype=float32), array([1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "      dtype=float32), array([1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "      dtype=float32), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
      "      dtype=float32), array([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "      dtype=float32), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.],\n",
      "      dtype=float32), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
      "      dtype=float32), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
      "      dtype=float32), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
      "      dtype=float32), array([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
      "      dtype=float32), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
      "      dtype=float32), array([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "      dtype=float32), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
      "      dtype=float32), array([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "      dtype=float32), array([1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "      dtype=float32), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
      "      dtype=float32), array([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "      dtype=float32), array([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "      dtype=float32), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
      "      dtype=float32), array([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "      dtype=float32), array([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "      dtype=float32), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.],\n",
      "      dtype=float32), array([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "      dtype=float32), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
      "      dtype=float32), array([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "      dtype=float32), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
      "      dtype=float32), array([1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "      dtype=float32), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.],\n",
      "      dtype=float32), array([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "      dtype=float32), array([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "      dtype=float32), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
      "      dtype=float32), array([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "      dtype=float32), array([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "      dtype=float32), array([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "      dtype=float32), array([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "      dtype=float32), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
      "      dtype=float32), array([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "      dtype=float32), array([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "      dtype=float32), array([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
      "      dtype=float32), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
      "      dtype=float32), array([1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "      dtype=float32), array([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
      "      dtype=float32), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
      "      dtype=float32), array([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "      dtype=float32), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
      "      dtype=float32), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
      "      dtype=float32), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.],\n",
      "      dtype=float32), array([1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "      dtype=float32), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
      "      dtype=float32), array([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "      dtype=float32), array([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "      dtype=float32), array([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "      dtype=float32), array([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "      dtype=float32), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
      "      dtype=float32), array([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "      dtype=float32), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.],\n",
      "      dtype=float32), array([1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "      dtype=float32), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
      "      dtype=float32), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
      "      dtype=float32), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.],\n",
      "      dtype=float32), array([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "      dtype=float32), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
      "      dtype=float32), array([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "      dtype=float32), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
      "      dtype=float32), array([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "      dtype=float32), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
      "      dtype=float32), array([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "      dtype=float32), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.],\n",
      "      dtype=float32), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
      "      dtype=float32), array([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "      dtype=float32), array([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "      dtype=float32), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.],\n",
      "      dtype=float32), array([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "      dtype=float32), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
      "      dtype=float32), array([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "      dtype=float32), array([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "      dtype=float32), array([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "      dtype=float32), array([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
      "      dtype=float32), array([1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "      dtype=float32), array([1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "      dtype=float32), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
      "      dtype=float32), array([1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "      dtype=float32), array([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "      dtype=float32), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
      "      dtype=float32), array([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "      dtype=float32), array([1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "      dtype=float32), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.],\n",
      "      dtype=float32), array([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "      dtype=float32), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
      "      dtype=float32), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
      "      dtype=float32), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
      "      dtype=float32), array([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "      dtype=float32), array([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
      "      dtype=float32), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
      "      dtype=float32), array([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "      dtype=float32), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
      "      dtype=float32), array([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "      dtype=float32), array([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "      dtype=float32), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.],\n",
      "      dtype=float32), array([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "      dtype=float32), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
      "      dtype=float32), array([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "      dtype=float32), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
      "      dtype=float32), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
      "      dtype=float32), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.],\n",
      "      dtype=float32), array([1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "      dtype=float32), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.],\n",
      "      dtype=float32), array([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "      dtype=float32), array([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "      dtype=float32), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
      "      dtype=float32), array([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "      dtype=float32), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
      "      dtype=float32), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
      "      dtype=float32), array([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "      dtype=float32), array([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "      dtype=float32), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
      "      dtype=float32), array([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "      dtype=float32), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
      "      dtype=float32), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
      "      dtype=float32), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
      "      dtype=float32), array([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
      "      dtype=float32), array([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "      dtype=float32), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
      "      dtype=float32), array([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "      dtype=float32), array([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "      dtype=float32), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
      "      dtype=float32), array([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "      dtype=float32), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
      "      dtype=float32), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
      "      dtype=float32), array([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "      dtype=float32), array([1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "      dtype=float32), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
      "      dtype=float32), array([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "      dtype=float32), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
      "      dtype=float32), array([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "      dtype=float32), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
      "      dtype=float32), array([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
      "      dtype=float32), array([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "      dtype=float32), array([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "      dtype=float32), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
      "      dtype=float32), array([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "      dtype=float32), array([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "      dtype=float32), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.],\n",
      "      dtype=float32), array([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "      dtype=float32), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
      "      dtype=float32), array([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "      dtype=float32), array([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "      dtype=float32), array([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "      dtype=float32), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
      "      dtype=float32), array([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "      dtype=float32), array([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "      dtype=float32), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.],\n",
      "      dtype=float32), array([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "      dtype=float32), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
      "      dtype=float32), array([1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "      dtype=float32), array([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "      dtype=float32), array([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
      "      dtype=float32), array([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "      dtype=float32), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
      "      dtype=float32), array([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "      dtype=float32), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.],\n",
      "      dtype=float32), array([1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "      dtype=float32), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
      "      dtype=float32), array([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "      dtype=float32), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
      "      dtype=float32), array([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "      dtype=float32), array([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "      dtype=float32), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
      "      dtype=float32), array([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "      dtype=float32), array([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "      dtype=float32), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.],\n",
      "      dtype=float32), array([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "      dtype=float32), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
      "      dtype=float32), array([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "      dtype=float32), array([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "      dtype=float32), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
      "      dtype=float32), array([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "      dtype=float32), array([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
      "      dtype=float32), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
      "      dtype=float32), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
      "      dtype=float32), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
      "      dtype=float32), array([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
      "      dtype=float32), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
      "      dtype=float32), array([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "      dtype=float32), array([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "      dtype=float32), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
      "      dtype=float32), array([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "      dtype=float32), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
      "      dtype=float32), array([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "      dtype=float32), array([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "      dtype=float32), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
      "      dtype=float32), array([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "      dtype=float32), array([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "      dtype=float32), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
      "      dtype=float32), array([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
      "      dtype=float32), array([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "      dtype=float32), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
      "      dtype=float32), array([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "      dtype=float32), array([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "      dtype=float32), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
      "      dtype=float32), array([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "      dtype=float32), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
      "      dtype=float32), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
      "      dtype=float32), array([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "      dtype=float32), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
      "      dtype=float32), array([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "      dtype=float32), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
      "      dtype=float32), array([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "      dtype=float32), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
      "      dtype=float32), array([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "      dtype=float32), array([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "      dtype=float32), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
      "      dtype=float32), array([1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "      dtype=float32), array([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "      dtype=float32), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
      "      dtype=float32), array([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "      dtype=float32), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
      "      dtype=float32), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
      "      dtype=float32), array([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
      "      dtype=float32), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.],\n",
      "      dtype=float32), array([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "      dtype=float32), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
      "      dtype=float32), array([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "      dtype=float32), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
      "      dtype=float32), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
      "      dtype=float32), array([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "      dtype=float32), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
      "      dtype=float32), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
      "      dtype=float32), array([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "      dtype=float32), array([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "      dtype=float32), array([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "      dtype=float32), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
      "      dtype=float32), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
      "      dtype=float32), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
      "      dtype=float32), array([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
      "      dtype=float32), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.],\n",
      "      dtype=float32), array([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "      dtype=float32), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
      "      dtype=float32), array([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "      dtype=float32), array([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "      dtype=float32), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.],\n",
      "      dtype=float32), array([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "      dtype=float32), array([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "      dtype=float32), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
      "      dtype=float32), array([1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "      dtype=float32), array([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "      dtype=float32), array([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "      dtype=float32), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
      "      dtype=float32), array([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "      dtype=float32), array([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "      dtype=float32), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.],\n",
      "      dtype=float32), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
      "      dtype=float32), array([1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "      dtype=float32), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.],\n",
      "      dtype=float32), array([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "      dtype=float32), array([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "      dtype=float32), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.],\n",
      "      dtype=float32), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
      "      dtype=float32)]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (2,) + inhomogeneous part.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[45], line 15\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my_train : \u001b[39m\u001b[38;5;124m\"\u001b[39m, y_train)\n\u001b[0;32m     14\u001b[0m y_train \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([y_train])\n\u001b[1;32m---> 15\u001b[0m X_train \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mX_train size : \u001b[39m\u001b[38;5;124m\"\u001b[39m, X_train\u001b[38;5;241m.\u001b[39mshape)\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my_train size : \u001b[39m\u001b[38;5;124m\"\u001b[39m, y_train\u001b[38;5;241m.\u001b[39mshape)\n",
      "\u001b[1;31mValueError\u001b[0m: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (2,) + inhomogeneous part."
     ]
    }
   ],
   "source": [
    "# Convert the training data to the required format\n",
    "output_size = len(character_encoding.DIACRITICS)\n",
    "X_train = concatinated_vector_train # Original\n",
    "# X_train = padded_sequences\n",
    "\n",
    "y_train = []\n",
    "for diacritics in sentences_diacritics:\n",
    "    for diacritic in diacritics:\n",
    "        index = character_encoding.DIACRITICS.index(diacritic)\n",
    "        y_train.append(to_categorical(index, num_classes = output_size))\n",
    "\n",
    "print(\"X_train : \", X_train)\n",
    "print(\"y_train : \", y_train)\n",
    "y_train = np.array([y_train])\n",
    "X_train = np.array(X_train)\n",
    "print(\"X_train size : \", X_train.shape)\n",
    "print(\"y_train size : \", y_train.shape)\n",
    "\n",
    "training_data = (concatinated_vector_train, sentences_diacritics)\n",
    "print(\"sentences size : \", len(training_data[0]))\n",
    "print(\"labels size : \", len(training_data[1]))\n",
    "# assert X_train.shape[0] == y_train.shape[0] # Original\n",
    "assert X_train.shape[1] == y_train.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "features tensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 1., 0.]]])\n",
      "labels tensor([[[0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.],\n",
      "         [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
      "         [0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
      "         [0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.],\n",
      "         [0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
      "         [0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
      "         [0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
      "         [0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
      "         [0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.],\n",
      "         [0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]]])\n"
     ]
    }
   ],
   "source": [
    "batch_size = 1\n",
    "dataset = CustomDataset(X_train, y_train)\n",
    "dataloader = DataLoader(dataset=dataset, batch_size=batch_size, shuffle=True)\n",
    "dataiter = iter(dataloader)\n",
    "data = next(dataiter)\n",
    "features, labels = data\n",
    "print(\"features\", features)\n",
    "print(\"labels\", labels)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Choose Classification model from `config.json file`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_size :  38\n",
      "trainnig on :  cpu\n",
      "Epoch 1/1000, Loss: 602.5191040039062\n",
      "Epoch 2/1000, Loss: 593.7539672851562\n",
      "Epoch 3/1000, Loss: 584.3287353515625\n",
      "Epoch 4/1000, Loss: 571.6887817382812\n",
      "Epoch 5/1000, Loss: 550.9510498046875\n",
      "Epoch 6/1000, Loss: 526.5035400390625\n",
      "Epoch 7/1000, Loss: 517.244873046875\n",
      "Epoch 8/1000, Loss: 511.04901123046875\n",
      "Epoch 9/1000, Loss: 507.5639343261719\n",
      "Epoch 10/1000, Loss: 506.6523742675781\n",
      "Epoch 11/1000, Loss: 505.21771240234375\n",
      "Epoch 12/1000, Loss: 502.7598876953125\n",
      "Epoch 13/1000, Loss: 501.0660095214844\n",
      "Epoch 14/1000, Loss: 501.0423278808594\n",
      "Epoch 15/1000, Loss: 501.5986328125\n",
      "Epoch 16/1000, Loss: 501.47454833984375\n",
      "Epoch 17/1000, Loss: 500.6409912109375\n",
      "Epoch 18/1000, Loss: 499.7369689941406\n",
      "Epoch 19/1000, Loss: 499.20574951171875\n",
      "Epoch 20/1000, Loss: 499.1269226074219\n",
      "Epoch 21/1000, Loss: 499.36077880859375\n",
      "Epoch 22/1000, Loss: 499.6348876953125\n",
      "Epoch 23/1000, Loss: 499.6956481933594\n",
      "Epoch 24/1000, Loss: 499.46917724609375\n",
      "Epoch 25/1000, Loss: 499.1019287109375\n",
      "Epoch 26/1000, Loss: 498.8142395019531\n",
      "Epoch 27/1000, Loss: 498.69891357421875\n",
      "Epoch 28/1000, Loss: 498.6903381347656\n",
      "Epoch 29/1000, Loss: 498.7207336425781\n",
      "Epoch 30/1000, Loss: 498.77838134765625\n",
      "Epoch 31/1000, Loss: 498.82183837890625\n",
      "Epoch 32/1000, Loss: 498.7843017578125\n",
      "Epoch 33/1000, Loss: 498.6676940917969\n",
      "Epoch 34/1000, Loss: 498.5515441894531\n",
      "Epoch 35/1000, Loss: 498.50079345703125\n",
      "Epoch 36/1000, Loss: 498.5091857910156\n",
      "Epoch 37/1000, Loss: 498.5281677246094\n",
      "Epoch 38/1000, Loss: 498.51959228515625\n",
      "Epoch 39/1000, Loss: 498.4875793457031\n",
      "Epoch 40/1000, Loss: 498.4599304199219\n",
      "Epoch 41/1000, Loss: 498.44683837890625\n",
      "Epoch 42/1000, Loss: 498.4390869140625\n",
      "Epoch 43/1000, Loss: 498.428466796875\n",
      "Epoch 44/1000, Loss: 498.41363525390625\n",
      "Epoch 45/1000, Loss: 498.39752197265625\n",
      "Epoch 46/1000, Loss: 498.38623046875\n",
      "Epoch 47/1000, Loss: 498.38043212890625\n",
      "Epoch 48/1000, Loss: 498.37371826171875\n",
      "Epoch 49/1000, Loss: 498.3643798828125\n",
      "Epoch 50/1000, Loss: 498.3577880859375\n",
      "Epoch 51/1000, Loss: 498.3561096191406\n",
      "Epoch 52/1000, Loss: 498.357177734375\n",
      "Epoch 53/1000, Loss: 498.35662841796875\n",
      "Epoch 54/1000, Loss: 498.3482666015625\n",
      "Epoch 55/1000, Loss: 498.33294677734375\n",
      "Epoch 56/1000, Loss: 498.3233642578125\n",
      "Epoch 57/1000, Loss: 498.32598876953125\n",
      "Epoch 58/1000, Loss: 498.3321228027344\n",
      "Epoch 59/1000, Loss: 498.331787109375\n",
      "Epoch 60/1000, Loss: 498.325439453125\n",
      "Epoch 61/1000, Loss: 498.3194274902344\n",
      "Epoch 62/1000, Loss: 498.31707763671875\n",
      "Epoch 63/1000, Loss: 498.3167724609375\n",
      "Epoch 64/1000, Loss: 498.31640625\n",
      "Epoch 65/1000, Loss: 498.31451416015625\n",
      "Epoch 66/1000, Loss: 498.3111877441406\n",
      "Epoch 67/1000, Loss: 498.3089904785156\n",
      "Epoch 68/1000, Loss: 498.30950927734375\n",
      "Epoch 69/1000, Loss: 498.3105163574219\n",
      "Epoch 70/1000, Loss: 498.30938720703125\n",
      "Epoch 71/1000, Loss: 498.30657958984375\n",
      "Epoch 72/1000, Loss: 498.3045959472656\n",
      "Epoch 73/1000, Loss: 498.30389404296875\n",
      "Epoch 74/1000, Loss: 498.30364990234375\n",
      "Epoch 75/1000, Loss: 498.30328369140625\n",
      "Epoch 76/1000, Loss: 498.3028564453125\n",
      "Epoch 77/1000, Loss: 498.30255126953125\n",
      "Epoch 78/1000, Loss: 498.30206298828125\n",
      "Epoch 79/1000, Loss: 498.3009948730469\n",
      "Epoch 80/1000, Loss: 498.2996826171875\n",
      "Epoch 81/1000, Loss: 498.299072265625\n",
      "Epoch 82/1000, Loss: 498.2991638183594\n",
      "Epoch 83/1000, Loss: 498.2991027832031\n",
      "Epoch 84/1000, Loss: 498.298583984375\n",
      "Epoch 85/1000, Loss: 498.298095703125\n",
      "Epoch 86/1000, Loss: 498.29779052734375\n",
      "Epoch 87/1000, Loss: 498.29730224609375\n",
      "Epoch 88/1000, Loss: 498.29656982421875\n",
      "Epoch 89/1000, Loss: 498.2962646484375\n",
      "Epoch 90/1000, Loss: 498.2962646484375\n",
      "Epoch 91/1000, Loss: 498.2961120605469\n",
      "Epoch 92/1000, Loss: 498.2955627441406\n",
      "Epoch 93/1000, Loss: 498.29522705078125\n",
      "Epoch 94/1000, Loss: 498.2950134277344\n",
      "Epoch 95/1000, Loss: 498.29486083984375\n",
      "Epoch 96/1000, Loss: 498.29437255859375\n",
      "Epoch 97/1000, Loss: 498.29400634765625\n",
      "Epoch 98/1000, Loss: 498.2938232421875\n",
      "Epoch 99/1000, Loss: 498.29376220703125\n",
      "Epoch 100/1000, Loss: 498.2935791015625\n",
      "Epoch 101/1000, Loss: 498.293212890625\n",
      "Epoch 102/1000, Loss: 498.2928771972656\n",
      "Epoch 103/1000, Loss: 498.292724609375\n",
      "Epoch 104/1000, Loss: 498.29248046875\n",
      "Epoch 105/1000, Loss: 498.29229736328125\n",
      "Epoch 106/1000, Loss: 498.2920837402344\n",
      "Epoch 107/1000, Loss: 498.2918395996094\n",
      "Epoch 108/1000, Loss: 498.29168701171875\n",
      "Epoch 109/1000, Loss: 498.2914123535156\n",
      "Epoch 110/1000, Loss: 498.2912292480469\n",
      "Epoch 111/1000, Loss: 498.291015625\n",
      "Epoch 112/1000, Loss: 498.2908935546875\n",
      "Epoch 113/1000, Loss: 498.2906494140625\n",
      "Epoch 114/1000, Loss: 498.2904968261719\n",
      "Epoch 115/1000, Loss: 498.290283203125\n",
      "Epoch 116/1000, Loss: 498.2901306152344\n",
      "Epoch 117/1000, Loss: 498.28997802734375\n",
      "Epoch 118/1000, Loss: 498.2897644042969\n",
      "Epoch 119/1000, Loss: 498.2895812988281\n",
      "Epoch 120/1000, Loss: 498.2894287109375\n",
      "Epoch 121/1000, Loss: 498.2892761230469\n",
      "Epoch 122/1000, Loss: 498.2890319824219\n",
      "Epoch 123/1000, Loss: 498.2889099121094\n",
      "Epoch 124/1000, Loss: 498.2887268066406\n",
      "Epoch 125/1000, Loss: 498.2885437011719\n",
      "Epoch 126/1000, Loss: 498.28839111328125\n",
      "Epoch 127/1000, Loss: 498.28826904296875\n",
      "Epoch 128/1000, Loss: 498.2880859375\n",
      "Epoch 129/1000, Loss: 498.2879333496094\n",
      "Epoch 130/1000, Loss: 498.2878112792969\n",
      "Epoch 131/1000, Loss: 498.28765869140625\n",
      "Epoch 132/1000, Loss: 498.28753662109375\n",
      "Epoch 133/1000, Loss: 498.28729248046875\n",
      "Epoch 134/1000, Loss: 498.2872009277344\n",
      "Epoch 135/1000, Loss: 498.28704833984375\n",
      "Epoch 136/1000, Loss: 498.2868957519531\n",
      "Epoch 137/1000, Loss: 498.2867431640625\n",
      "Epoch 138/1000, Loss: 498.2865905761719\n",
      "Epoch 139/1000, Loss: 498.2864685058594\n",
      "Epoch 140/1000, Loss: 498.28631591796875\n",
      "Epoch 141/1000, Loss: 498.2861633300781\n",
      "Epoch 142/1000, Loss: 498.28607177734375\n",
      "Epoch 143/1000, Loss: 498.2859191894531\n",
      "Epoch 144/1000, Loss: 498.2857666015625\n",
      "Epoch 145/1000, Loss: 498.2856140136719\n",
      "Epoch 146/1000, Loss: 498.2855224609375\n",
      "Epoch 147/1000, Loss: 498.2853698730469\n",
      "Epoch 148/1000, Loss: 498.2852478027344\n",
      "Epoch 149/1000, Loss: 498.28515625\n",
      "Epoch 150/1000, Loss: 498.2850036621094\n",
      "Epoch 151/1000, Loss: 498.2848815917969\n",
      "Epoch 152/1000, Loss: 498.28472900390625\n",
      "Epoch 153/1000, Loss: 498.2846374511719\n",
      "Epoch 154/1000, Loss: 498.28448486328125\n",
      "Epoch 155/1000, Loss: 498.28436279296875\n",
      "Epoch 156/1000, Loss: 498.2842712402344\n",
      "Epoch 157/1000, Loss: 498.2841796875\n",
      "Epoch 158/1000, Loss: 498.2840270996094\n",
      "Epoch 159/1000, Loss: 498.2839050292969\n",
      "Epoch 160/1000, Loss: 498.2838134765625\n",
      "Epoch 161/1000, Loss: 498.2836608886719\n",
      "Epoch 162/1000, Loss: 498.2835693359375\n",
      "Epoch 163/1000, Loss: 498.2834777832031\n",
      "Epoch 164/1000, Loss: 498.2833251953125\n",
      "Epoch 165/1000, Loss: 498.28326416015625\n",
      "Epoch 166/1000, Loss: 498.2831726074219\n",
      "Epoch 167/1000, Loss: 498.28302001953125\n",
      "Epoch 168/1000, Loss: 498.28289794921875\n",
      "Epoch 169/1000, Loss: 498.28277587890625\n",
      "Epoch 170/1000, Loss: 498.28271484375\n",
      "Epoch 171/1000, Loss: 498.2825927734375\n",
      "Epoch 172/1000, Loss: 498.28253173828125\n",
      "Epoch 173/1000, Loss: 498.2823486328125\n",
      "Epoch 174/1000, Loss: 498.2823181152344\n",
      "Epoch 175/1000, Loss: 498.2821960449219\n",
      "Epoch 176/1000, Loss: 498.2820739746094\n",
      "Epoch 177/1000, Loss: 498.2820129394531\n",
      "Epoch 178/1000, Loss: 498.2818908691406\n",
      "Epoch 179/1000, Loss: 498.2818603515625\n",
      "Epoch 180/1000, Loss: 498.28167724609375\n",
      "Epoch 181/1000, Loss: 498.2816467285156\n",
      "Epoch 182/1000, Loss: 498.2815246582031\n",
      "Epoch 183/1000, Loss: 498.2814025878906\n",
      "Epoch 184/1000, Loss: 498.2813415527344\n",
      "Epoch 185/1000, Loss: 498.28125\n",
      "Epoch 186/1000, Loss: 498.2811279296875\n",
      "Epoch 187/1000, Loss: 498.28106689453125\n",
      "Epoch 188/1000, Loss: 498.28094482421875\n",
      "Epoch 189/1000, Loss: 498.28082275390625\n",
      "Epoch 190/1000, Loss: 498.2807922363281\n",
      "Epoch 191/1000, Loss: 498.28070068359375\n",
      "Epoch 192/1000, Loss: 498.2806091308594\n",
      "Epoch 193/1000, Loss: 498.2805480957031\n",
      "Epoch 194/1000, Loss: 498.28045654296875\n",
      "Epoch 195/1000, Loss: 498.28033447265625\n",
      "Epoch 196/1000, Loss: 498.2802734375\n",
      "Epoch 197/1000, Loss: 498.2801818847656\n",
      "Epoch 198/1000, Loss: 498.2801208496094\n",
      "Epoch 199/1000, Loss: 498.280029296875\n",
      "Epoch 200/1000, Loss: 498.27996826171875\n",
      "Epoch 201/1000, Loss: 498.2798767089844\n",
      "Epoch 202/1000, Loss: 498.2798156738281\n",
      "Epoch 203/1000, Loss: 498.27972412109375\n",
      "Epoch 204/1000, Loss: 498.2796325683594\n",
      "Epoch 205/1000, Loss: 498.27960205078125\n",
      "Epoch 206/1000, Loss: 498.2795104980469\n",
      "Epoch 207/1000, Loss: 498.2794494628906\n",
      "Epoch 208/1000, Loss: 498.2793273925781\n",
      "Epoch 209/1000, Loss: 498.279296875\n",
      "Epoch 210/1000, Loss: 498.2791442871094\n",
      "Epoch 211/1000, Loss: 498.27911376953125\n",
      "Epoch 212/1000, Loss: 498.279052734375\n",
      "Epoch 213/1000, Loss: 498.2789306640625\n",
      "Epoch 214/1000, Loss: 498.27886962890625\n",
      "Epoch 215/1000, Loss: 498.27880859375\n",
      "Epoch 216/1000, Loss: 498.27874755859375\n",
      "Epoch 217/1000, Loss: 498.2787170410156\n",
      "Epoch 218/1000, Loss: 498.2785949707031\n",
      "Epoch 219/1000, Loss: 498.278564453125\n",
      "Epoch 220/1000, Loss: 498.2784729003906\n",
      "Epoch 221/1000, Loss: 498.2784118652344\n",
      "Epoch 222/1000, Loss: 498.2783508300781\n",
      "Epoch 223/1000, Loss: 498.27825927734375\n",
      "Epoch 224/1000, Loss: 498.27825927734375\n",
      "Epoch 225/1000, Loss: 498.27813720703125\n",
      "Epoch 226/1000, Loss: 498.27801513671875\n",
      "Epoch 227/1000, Loss: 498.2780456542969\n",
      "Epoch 228/1000, Loss: 498.2779235839844\n",
      "Epoch 229/1000, Loss: 498.27789306640625\n",
      "Epoch 230/1000, Loss: 498.27783203125\n",
      "Epoch 231/1000, Loss: 498.27783203125\n",
      "Epoch 232/1000, Loss: 498.2777099609375\n",
      "Epoch 233/1000, Loss: 498.2776184082031\n",
      "Epoch 234/1000, Loss: 498.27752685546875\n",
      "Epoch 235/1000, Loss: 498.27752685546875\n",
      "Epoch 236/1000, Loss: 498.27740478515625\n",
      "Epoch 237/1000, Loss: 498.27740478515625\n",
      "Epoch 238/1000, Loss: 498.27728271484375\n",
      "Epoch 239/1000, Loss: 498.27728271484375\n",
      "Epoch 240/1000, Loss: 498.2771911621094\n",
      "Epoch 241/1000, Loss: 498.2771301269531\n",
      "Epoch 242/1000, Loss: 498.277099609375\n",
      "Epoch 243/1000, Loss: 498.2770690917969\n",
      "Epoch 244/1000, Loss: 498.2769775390625\n",
      "Epoch 245/1000, Loss: 498.2769470214844\n",
      "Epoch 246/1000, Loss: 498.2768859863281\n",
      "Epoch 247/1000, Loss: 498.27685546875\n",
      "Epoch 248/1000, Loss: 498.2767639160156\n",
      "Epoch 249/1000, Loss: 498.2767333984375\n",
      "Epoch 250/1000, Loss: 498.27667236328125\n",
      "Epoch 251/1000, Loss: 498.27667236328125\n",
      "Epoch 252/1000, Loss: 498.2765197753906\n",
      "Epoch 253/1000, Loss: 498.2765197753906\n",
      "Epoch 254/1000, Loss: 498.27642822265625\n",
      "Epoch 255/1000, Loss: 498.2763977050781\n",
      "Epoch 256/1000, Loss: 498.2763671875\n",
      "Epoch 257/1000, Loss: 498.2762756347656\n",
      "Epoch 258/1000, Loss: 498.2762451171875\n",
      "Epoch 259/1000, Loss: 498.2761535644531\n",
      "Epoch 260/1000, Loss: 498.276123046875\n",
      "Epoch 261/1000, Loss: 498.2760925292969\n",
      "Epoch 262/1000, Loss: 498.2760314941406\n",
      "Epoch 263/1000, Loss: 498.2760009765625\n",
      "Epoch 264/1000, Loss: 498.2759094238281\n",
      "Epoch 265/1000, Loss: 498.2758483886719\n",
      "Epoch 266/1000, Loss: 498.2758483886719\n",
      "Epoch 267/1000, Loss: 498.27581787109375\n",
      "Epoch 268/1000, Loss: 498.2757263183594\n",
      "Epoch 269/1000, Loss: 498.27569580078125\n",
      "Epoch 270/1000, Loss: 498.2756652832031\n",
      "Epoch 271/1000, Loss: 498.275634765625\n",
      "Epoch 272/1000, Loss: 498.2756042480469\n",
      "Epoch 273/1000, Loss: 498.2755432128906\n",
      "Epoch 274/1000, Loss: 498.27545166015625\n",
      "Epoch 275/1000, Loss: 498.2754211425781\n",
      "Epoch 276/1000, Loss: 498.275390625\n",
      "Epoch 277/1000, Loss: 498.27532958984375\n",
      "Epoch 278/1000, Loss: 498.2752685546875\n",
      "Epoch 279/1000, Loss: 498.2752685546875\n",
      "Epoch 280/1000, Loss: 498.27520751953125\n",
      "Epoch 281/1000, Loss: 498.275146484375\n",
      "Epoch 282/1000, Loss: 498.2751159667969\n",
      "Epoch 283/1000, Loss: 498.2750549316406\n",
      "Epoch 284/1000, Loss: 498.2750244140625\n",
      "Epoch 285/1000, Loss: 498.2749938964844\n",
      "Epoch 286/1000, Loss: 498.27496337890625\n",
      "Epoch 287/1000, Loss: 498.27496337890625\n",
      "Epoch 288/1000, Loss: 498.27490234375\n",
      "Epoch 289/1000, Loss: 498.27484130859375\n",
      "Epoch 290/1000, Loss: 498.2747802734375\n",
      "Epoch 291/1000, Loss: 498.2747497558594\n",
      "Epoch 292/1000, Loss: 498.2746887207031\n",
      "Epoch 293/1000, Loss: 498.274658203125\n",
      "Epoch 294/1000, Loss: 498.274658203125\n",
      "Epoch 295/1000, Loss: 498.27459716796875\n",
      "Epoch 296/1000, Loss: 498.2745666503906\n",
      "Epoch 297/1000, Loss: 498.2745056152344\n",
      "Epoch 298/1000, Loss: 498.2745361328125\n",
      "Epoch 299/1000, Loss: 498.2744140625\n",
      "Epoch 300/1000, Loss: 498.27435302734375\n",
      "Epoch 301/1000, Loss: 498.27435302734375\n",
      "Epoch 302/1000, Loss: 498.2743225097656\n",
      "Epoch 303/1000, Loss: 498.2742614746094\n",
      "Epoch 304/1000, Loss: 498.27423095703125\n",
      "Epoch 305/1000, Loss: 498.2742614746094\n",
      "Epoch 306/1000, Loss: 498.274169921875\n",
      "Epoch 307/1000, Loss: 498.2742004394531\n",
      "Epoch 308/1000, Loss: 498.2741394042969\n",
      "Epoch 309/1000, Loss: 498.2740783691406\n",
      "Epoch 310/1000, Loss: 498.2740173339844\n",
      "Epoch 311/1000, Loss: 498.2739562988281\n",
      "Epoch 312/1000, Loss: 498.27398681640625\n",
      "Epoch 313/1000, Loss: 498.27392578125\n",
      "Epoch 314/1000, Loss: 498.2738952636719\n",
      "Epoch 315/1000, Loss: 498.2738342285156\n",
      "Epoch 316/1000, Loss: 498.27386474609375\n",
      "Epoch 317/1000, Loss: 498.2738037109375\n",
      "Epoch 318/1000, Loss: 498.2737731933594\n",
      "Epoch 319/1000, Loss: 498.2737121582031\n",
      "Epoch 320/1000, Loss: 498.27362060546875\n",
      "Epoch 321/1000, Loss: 498.273681640625\n",
      "Epoch 322/1000, Loss: 498.27362060546875\n",
      "Epoch 323/1000, Loss: 498.2735900878906\n",
      "Epoch 324/1000, Loss: 498.2735595703125\n",
      "Epoch 325/1000, Loss: 498.27349853515625\n",
      "Epoch 326/1000, Loss: 498.2735290527344\n",
      "Epoch 327/1000, Loss: 498.2734069824219\n",
      "Epoch 328/1000, Loss: 498.2734069824219\n",
      "Epoch 329/1000, Loss: 498.2734069824219\n",
      "Epoch 330/1000, Loss: 498.27337646484375\n",
      "Epoch 331/1000, Loss: 498.2733154296875\n",
      "Epoch 332/1000, Loss: 498.2732849121094\n",
      "Epoch 333/1000, Loss: 498.2732849121094\n",
      "Epoch 334/1000, Loss: 498.273193359375\n",
      "Epoch 335/1000, Loss: 498.2732238769531\n",
      "Epoch 336/1000, Loss: 498.27313232421875\n",
      "Epoch 337/1000, Loss: 498.2731628417969\n",
      "Epoch 338/1000, Loss: 498.2730712890625\n",
      "Epoch 339/1000, Loss: 498.2730712890625\n",
      "Epoch 340/1000, Loss: 498.27301025390625\n",
      "Epoch 341/1000, Loss: 498.2730712890625\n",
      "Epoch 342/1000, Loss: 498.27301025390625\n",
      "Epoch 343/1000, Loss: 498.27294921875\n",
      "Epoch 344/1000, Loss: 498.2729187011719\n",
      "Epoch 345/1000, Loss: 498.2729187011719\n",
      "Epoch 346/1000, Loss: 498.27288818359375\n",
      "Epoch 347/1000, Loss: 498.2728576660156\n",
      "Epoch 348/1000, Loss: 498.2728576660156\n",
      "Epoch 349/1000, Loss: 498.2727966308594\n",
      "Epoch 350/1000, Loss: 498.2727966308594\n",
      "Epoch 351/1000, Loss: 498.2727355957031\n",
      "Epoch 352/1000, Loss: 498.272705078125\n",
      "Epoch 353/1000, Loss: 498.2726745605469\n",
      "Epoch 354/1000, Loss: 498.272705078125\n",
      "Epoch 355/1000, Loss: 498.2726135253906\n",
      "Epoch 356/1000, Loss: 498.2725830078125\n",
      "Epoch 357/1000, Loss: 498.2725830078125\n",
      "Epoch 358/1000, Loss: 498.2725524902344\n",
      "Epoch 359/1000, Loss: 498.27252197265625\n",
      "Epoch 360/1000, Loss: 498.27252197265625\n",
      "Epoch 361/1000, Loss: 498.2724609375\n",
      "Epoch 362/1000, Loss: 498.2724304199219\n",
      "Epoch 363/1000, Loss: 498.2724609375\n",
      "Epoch 364/1000, Loss: 498.2723693847656\n",
      "Epoch 365/1000, Loss: 498.27239990234375\n",
      "Epoch 366/1000, Loss: 498.2723693847656\n",
      "Epoch 367/1000, Loss: 498.2723388671875\n",
      "Epoch 368/1000, Loss: 498.2723388671875\n",
      "Epoch 369/1000, Loss: 498.2723083496094\n",
      "Epoch 370/1000, Loss: 498.2723083496094\n",
      "Epoch 371/1000, Loss: 498.272216796875\n",
      "Epoch 372/1000, Loss: 498.272216796875\n",
      "Epoch 373/1000, Loss: 498.272216796875\n",
      "Epoch 374/1000, Loss: 498.2721862792969\n",
      "Epoch 375/1000, Loss: 498.27215576171875\n",
      "Epoch 376/1000, Loss: 498.2720947265625\n",
      "Epoch 377/1000, Loss: 498.2720642089844\n",
      "Epoch 378/1000, Loss: 498.2720947265625\n",
      "Epoch 379/1000, Loss: 498.27203369140625\n",
      "Epoch 380/1000, Loss: 498.2720031738281\n",
      "Epoch 381/1000, Loss: 498.27197265625\n",
      "Epoch 382/1000, Loss: 498.27197265625\n",
      "Epoch 383/1000, Loss: 498.2720031738281\n",
      "Epoch 384/1000, Loss: 498.2719421386719\n",
      "Epoch 385/1000, Loss: 498.27191162109375\n",
      "Epoch 386/1000, Loss: 498.2718811035156\n",
      "Epoch 387/1000, Loss: 498.2718505859375\n",
      "Epoch 388/1000, Loss: 498.2718505859375\n",
      "Epoch 389/1000, Loss: 498.2718505859375\n",
      "Epoch 390/1000, Loss: 498.2717590332031\n",
      "Epoch 391/1000, Loss: 498.27178955078125\n",
      "Epoch 392/1000, Loss: 498.271728515625\n",
      "Epoch 393/1000, Loss: 498.2717590332031\n",
      "Epoch 394/1000, Loss: 498.2716979980469\n",
      "Epoch 395/1000, Loss: 498.2716979980469\n",
      "Epoch 396/1000, Loss: 498.2716979980469\n",
      "Epoch 397/1000, Loss: 498.27166748046875\n",
      "Epoch 398/1000, Loss: 498.2716064453125\n",
      "Epoch 399/1000, Loss: 498.2715759277344\n",
      "Epoch 400/1000, Loss: 498.2716064453125\n",
      "Epoch 401/1000, Loss: 498.2715759277344\n",
      "Epoch 402/1000, Loss: 498.2715148925781\n",
      "Epoch 403/1000, Loss: 498.2715148925781\n",
      "Epoch 404/1000, Loss: 498.27154541015625\n",
      "Epoch 405/1000, Loss: 498.271484375\n",
      "Epoch 406/1000, Loss: 498.2714538574219\n",
      "Epoch 407/1000, Loss: 498.2714538574219\n",
      "Epoch 408/1000, Loss: 498.2714538574219\n",
      "Epoch 409/1000, Loss: 498.27142333984375\n",
      "Epoch 410/1000, Loss: 498.27142333984375\n",
      "Epoch 411/1000, Loss: 498.2713317871094\n",
      "Epoch 412/1000, Loss: 498.2713623046875\n",
      "Epoch 413/1000, Loss: 498.2713623046875\n",
      "Epoch 414/1000, Loss: 498.27130126953125\n",
      "Epoch 415/1000, Loss: 498.2712707519531\n",
      "Epoch 416/1000, Loss: 498.271240234375\n",
      "Epoch 417/1000, Loss: 498.271240234375\n",
      "Epoch 418/1000, Loss: 498.2712097167969\n",
      "Epoch 419/1000, Loss: 498.2712707519531\n",
      "Epoch 420/1000, Loss: 498.271240234375\n",
      "Epoch 421/1000, Loss: 498.27117919921875\n",
      "Epoch 422/1000, Loss: 498.2712097167969\n",
      "Epoch 423/1000, Loss: 498.2711181640625\n",
      "Epoch 424/1000, Loss: 498.2711181640625\n",
      "Epoch 425/1000, Loss: 498.2710876464844\n",
      "Epoch 426/1000, Loss: 498.27105712890625\n",
      "Epoch 427/1000, Loss: 498.27105712890625\n",
      "Epoch 428/1000, Loss: 498.27099609375\n",
      "Epoch 429/1000, Loss: 498.27105712890625\n",
      "Epoch 430/1000, Loss: 498.27105712890625\n",
      "Epoch 431/1000, Loss: 498.27099609375\n",
      "Epoch 432/1000, Loss: 498.27099609375\n",
      "Epoch 433/1000, Loss: 498.2709045410156\n",
      "Epoch 434/1000, Loss: 498.2709045410156\n",
      "Epoch 435/1000, Loss: 498.2708740234375\n",
      "Epoch 436/1000, Loss: 498.27093505859375\n",
      "Epoch 437/1000, Loss: 498.27093505859375\n",
      "Epoch 438/1000, Loss: 498.2708740234375\n",
      "Epoch 439/1000, Loss: 498.2708435058594\n",
      "Epoch 440/1000, Loss: 498.2708740234375\n",
      "Epoch 441/1000, Loss: 498.2708435058594\n",
      "Epoch 442/1000, Loss: 498.2707824707031\n",
      "Epoch 443/1000, Loss: 498.270751953125\n",
      "Epoch 444/1000, Loss: 498.270751953125\n",
      "Epoch 445/1000, Loss: 498.2707824707031\n",
      "Epoch 446/1000, Loss: 498.270751953125\n",
      "Epoch 447/1000, Loss: 498.27069091796875\n",
      "Epoch 448/1000, Loss: 498.27069091796875\n",
      "Epoch 449/1000, Loss: 498.27069091796875\n",
      "Epoch 450/1000, Loss: 498.2707214355469\n",
      "Epoch 451/1000, Loss: 498.27069091796875\n",
      "Epoch 452/1000, Loss: 498.2706604003906\n",
      "Epoch 453/1000, Loss: 498.2706298828125\n",
      "Epoch 454/1000, Loss: 498.2706298828125\n",
      "Epoch 455/1000, Loss: 498.2706298828125\n",
      "Epoch 456/1000, Loss: 498.27056884765625\n",
      "Epoch 457/1000, Loss: 498.2705383300781\n",
      "Epoch 458/1000, Loss: 498.2705383300781\n",
      "Epoch 459/1000, Loss: 498.2705383300781\n",
      "Epoch 460/1000, Loss: 498.2705383300781\n",
      "Epoch 461/1000, Loss: 498.2705078125\n",
      "Epoch 462/1000, Loss: 498.2704772949219\n",
      "Epoch 463/1000, Loss: 498.2704162597656\n",
      "Epoch 464/1000, Loss: 498.27044677734375\n",
      "Epoch 465/1000, Loss: 498.2703857421875\n",
      "Epoch 466/1000, Loss: 498.2704162597656\n",
      "Epoch 467/1000, Loss: 498.2703857421875\n",
      "Epoch 468/1000, Loss: 498.2703857421875\n",
      "Epoch 469/1000, Loss: 498.2703857421875\n",
      "Epoch 470/1000, Loss: 498.27032470703125\n",
      "Epoch 471/1000, Loss: 498.2703857421875\n",
      "Epoch 472/1000, Loss: 498.27032470703125\n",
      "Epoch 473/1000, Loss: 498.2702941894531\n",
      "Epoch 474/1000, Loss: 498.27032470703125\n",
      "Epoch 475/1000, Loss: 498.27032470703125\n",
      "Epoch 476/1000, Loss: 498.2702941894531\n",
      "Epoch 477/1000, Loss: 498.27020263671875\n",
      "Epoch 478/1000, Loss: 498.270263671875\n",
      "Epoch 479/1000, Loss: 498.2702331542969\n",
      "Epoch 480/1000, Loss: 498.2701721191406\n",
      "Epoch 481/1000, Loss: 498.2702331542969\n",
      "Epoch 482/1000, Loss: 498.27020263671875\n",
      "Epoch 483/1000, Loss: 498.27020263671875\n",
      "Epoch 484/1000, Loss: 498.2701721191406\n",
      "Epoch 485/1000, Loss: 498.2701416015625\n",
      "Epoch 486/1000, Loss: 498.2701416015625\n",
      "Epoch 487/1000, Loss: 498.27008056640625\n",
      "Epoch 488/1000, Loss: 498.27008056640625\n",
      "Epoch 489/1000, Loss: 498.2700500488281\n",
      "Epoch 490/1000, Loss: 498.2700500488281\n",
      "Epoch 491/1000, Loss: 498.2700500488281\n",
      "Epoch 492/1000, Loss: 498.2700500488281\n",
      "Epoch 493/1000, Loss: 498.2701110839844\n",
      "Epoch 494/1000, Loss: 498.26995849609375\n",
      "Epoch 495/1000, Loss: 498.2699890136719\n",
      "Epoch 496/1000, Loss: 498.2699890136719\n",
      "Epoch 497/1000, Loss: 498.26995849609375\n",
      "Epoch 498/1000, Loss: 498.2699890136719\n",
      "Epoch 499/1000, Loss: 498.2699279785156\n",
      "Epoch 500/1000, Loss: 498.26995849609375\n",
      "Epoch 501/1000, Loss: 498.26995849609375\n",
      "Epoch 502/1000, Loss: 498.2698974609375\n",
      "Epoch 503/1000, Loss: 498.26995849609375\n",
      "Epoch 504/1000, Loss: 498.26995849609375\n",
      "Epoch 505/1000, Loss: 498.2698974609375\n",
      "Epoch 506/1000, Loss: 498.2698974609375\n",
      "Epoch 507/1000, Loss: 498.26983642578125\n",
      "Epoch 508/1000, Loss: 498.26983642578125\n",
      "Epoch 509/1000, Loss: 498.2698059082031\n",
      "Epoch 510/1000, Loss: 498.26983642578125\n",
      "Epoch 511/1000, Loss: 498.26983642578125\n",
      "Epoch 512/1000, Loss: 498.2698059082031\n",
      "Epoch 513/1000, Loss: 498.269775390625\n",
      "Epoch 514/1000, Loss: 498.2698059082031\n",
      "Epoch 515/1000, Loss: 498.269775390625\n",
      "Epoch 516/1000, Loss: 498.269775390625\n",
      "Epoch 517/1000, Loss: 498.2697448730469\n",
      "Epoch 518/1000, Loss: 498.2697448730469\n",
      "Epoch 519/1000, Loss: 498.26971435546875\n",
      "Epoch 520/1000, Loss: 498.26971435546875\n",
      "Epoch 521/1000, Loss: 498.2696533203125\n",
      "Epoch 522/1000, Loss: 498.2696838378906\n",
      "Epoch 523/1000, Loss: 498.2696533203125\n",
      "Epoch 524/1000, Loss: 498.2696533203125\n",
      "Epoch 525/1000, Loss: 498.2696533203125\n",
      "Epoch 526/1000, Loss: 498.26959228515625\n",
      "Epoch 527/1000, Loss: 498.2696533203125\n",
      "Epoch 528/1000, Loss: 498.2696533203125\n",
      "Epoch 529/1000, Loss: 498.26959228515625\n",
      "Epoch 530/1000, Loss: 498.2696228027344\n",
      "Epoch 531/1000, Loss: 498.2695617675781\n",
      "Epoch 532/1000, Loss: 498.26959228515625\n",
      "Epoch 533/1000, Loss: 498.26953125\n",
      "Epoch 534/1000, Loss: 498.26959228515625\n",
      "Epoch 535/1000, Loss: 498.26959228515625\n",
      "Epoch 536/1000, Loss: 498.2695007324219\n",
      "Epoch 537/1000, Loss: 498.2695617675781\n",
      "Epoch 538/1000, Loss: 498.26953125\n",
      "Epoch 539/1000, Loss: 498.26953125\n",
      "Epoch 540/1000, Loss: 498.26947021484375\n",
      "Epoch 541/1000, Loss: 498.2695007324219\n",
      "Epoch 542/1000, Loss: 498.2695007324219\n",
      "Epoch 543/1000, Loss: 498.2694396972656\n",
      "Epoch 544/1000, Loss: 498.2694396972656\n",
      "Epoch 545/1000, Loss: 498.26947021484375\n",
      "Epoch 546/1000, Loss: 498.2694091796875\n",
      "Epoch 547/1000, Loss: 498.2694396972656\n",
      "Epoch 548/1000, Loss: 498.2693786621094\n",
      "Epoch 549/1000, Loss: 498.2693786621094\n",
      "Epoch 550/1000, Loss: 498.2693786621094\n",
      "Epoch 551/1000, Loss: 498.2693786621094\n",
      "Epoch 552/1000, Loss: 498.2693786621094\n",
      "Epoch 553/1000, Loss: 498.2693176269531\n",
      "Epoch 554/1000, Loss: 498.2693786621094\n",
      "Epoch 555/1000, Loss: 498.2693176269531\n",
      "Epoch 556/1000, Loss: 498.2693176269531\n",
      "Epoch 557/1000, Loss: 498.2693176269531\n",
      "Epoch 558/1000, Loss: 498.26934814453125\n",
      "Epoch 559/1000, Loss: 498.269287109375\n",
      "Epoch 560/1000, Loss: 498.2693176269531\n",
      "Epoch 561/1000, Loss: 498.2692565917969\n",
      "Epoch 562/1000, Loss: 498.2693176269531\n",
      "Epoch 563/1000, Loss: 498.269287109375\n",
      "Epoch 564/1000, Loss: 498.26922607421875\n",
      "Epoch 565/1000, Loss: 498.2692565917969\n",
      "Epoch 566/1000, Loss: 498.26922607421875\n",
      "Epoch 567/1000, Loss: 498.26922607421875\n",
      "Epoch 568/1000, Loss: 498.26922607421875\n",
      "Epoch 569/1000, Loss: 498.2691955566406\n",
      "Epoch 570/1000, Loss: 498.26922607421875\n",
      "Epoch 571/1000, Loss: 498.2691955566406\n",
      "Epoch 572/1000, Loss: 498.2691650390625\n",
      "Epoch 573/1000, Loss: 498.2691955566406\n",
      "Epoch 574/1000, Loss: 498.2691955566406\n",
      "Epoch 575/1000, Loss: 498.2691650390625\n",
      "Epoch 576/1000, Loss: 498.26910400390625\n",
      "Epoch 577/1000, Loss: 498.2691650390625\n",
      "Epoch 578/1000, Loss: 498.2691345214844\n",
      "Epoch 579/1000, Loss: 498.2691345214844\n",
      "Epoch 580/1000, Loss: 498.26910400390625\n",
      "Epoch 581/1000, Loss: 498.26910400390625\n",
      "Epoch 582/1000, Loss: 498.26904296875\n",
      "Epoch 583/1000, Loss: 498.2690734863281\n",
      "Epoch 584/1000, Loss: 498.26904296875\n",
      "Epoch 585/1000, Loss: 498.2690734863281\n",
      "Epoch 586/1000, Loss: 498.26904296875\n",
      "Epoch 587/1000, Loss: 498.26904296875\n",
      "Epoch 588/1000, Loss: 498.26898193359375\n",
      "Epoch 589/1000, Loss: 498.26904296875\n",
      "Epoch 590/1000, Loss: 498.26898193359375\n",
      "Epoch 591/1000, Loss: 498.2690124511719\n",
      "Epoch 592/1000, Loss: 498.2689208984375\n",
      "Epoch 593/1000, Loss: 498.2689514160156\n",
      "Epoch 594/1000, Loss: 498.26898193359375\n",
      "Epoch 595/1000, Loss: 498.26898193359375\n",
      "Epoch 596/1000, Loss: 498.26898193359375\n",
      "Epoch 597/1000, Loss: 498.2689208984375\n",
      "Epoch 598/1000, Loss: 498.26898193359375\n",
      "Epoch 599/1000, Loss: 498.26898193359375\n",
      "Epoch 600/1000, Loss: 498.2689208984375\n",
      "Epoch 601/1000, Loss: 498.2688903808594\n",
      "Epoch 602/1000, Loss: 498.2689208984375\n",
      "Epoch 603/1000, Loss: 498.2689208984375\n",
      "Epoch 604/1000, Loss: 498.2688903808594\n",
      "Epoch 605/1000, Loss: 498.2689208984375\n",
      "Epoch 606/1000, Loss: 498.26885986328125\n",
      "Epoch 607/1000, Loss: 498.2689208984375\n",
      "Epoch 608/1000, Loss: 498.26885986328125\n",
      "Epoch 609/1000, Loss: 498.268798828125\n",
      "Epoch 610/1000, Loss: 498.2688293457031\n",
      "Epoch 611/1000, Loss: 498.268798828125\n",
      "Epoch 612/1000, Loss: 498.2688293457031\n",
      "Epoch 613/1000, Loss: 498.268798828125\n",
      "Epoch 614/1000, Loss: 498.268798828125\n",
      "Epoch 615/1000, Loss: 498.268798828125\n",
      "Epoch 616/1000, Loss: 498.2687683105469\n",
      "Epoch 617/1000, Loss: 498.2688293457031\n",
      "Epoch 618/1000, Loss: 498.2687683105469\n",
      "Epoch 619/1000, Loss: 498.268798828125\n",
      "Epoch 620/1000, Loss: 498.2687683105469\n",
      "Epoch 621/1000, Loss: 498.26873779296875\n",
      "Epoch 622/1000, Loss: 498.26873779296875\n",
      "Epoch 623/1000, Loss: 498.268798828125\n",
      "Epoch 624/1000, Loss: 498.2687683105469\n",
      "Epoch 625/1000, Loss: 498.2687072753906\n",
      "Epoch 626/1000, Loss: 498.26873779296875\n",
      "Epoch 627/1000, Loss: 498.26873779296875\n",
      "Epoch 628/1000, Loss: 498.2687072753906\n",
      "Epoch 629/1000, Loss: 498.26873779296875\n",
      "Epoch 630/1000, Loss: 498.26873779296875\n",
      "Epoch 631/1000, Loss: 498.2687072753906\n",
      "Epoch 632/1000, Loss: 498.2686767578125\n",
      "Epoch 633/1000, Loss: 498.26873779296875\n",
      "Epoch 634/1000, Loss: 498.2686462402344\n",
      "Epoch 635/1000, Loss: 498.26861572265625\n",
      "Epoch 636/1000, Loss: 498.2686462402344\n",
      "Epoch 637/1000, Loss: 498.2686462402344\n",
      "Epoch 638/1000, Loss: 498.2686462402344\n",
      "Epoch 639/1000, Loss: 498.2686767578125\n",
      "Epoch 640/1000, Loss: 498.26873779296875\n",
      "Epoch 641/1000, Loss: 498.268798828125\n",
      "Epoch 642/1000, Loss: 498.2689514160156\n",
      "Epoch 643/1000, Loss: 498.2691955566406\n",
      "Epoch 644/1000, Loss: 498.26971435546875\n",
      "Epoch 645/1000, Loss: 498.2706604003906\n",
      "Epoch 646/1000, Loss: 498.27215576171875\n",
      "Epoch 647/1000, Loss: 498.274658203125\n",
      "Epoch 648/1000, Loss: 498.2774353027344\n",
      "Epoch 649/1000, Loss: 498.2802734375\n",
      "Epoch 650/1000, Loss: 498.27984619140625\n",
      "Epoch 651/1000, Loss: 498.2769775390625\n",
      "Epoch 652/1000, Loss: 498.2730712890625\n",
      "Epoch 653/1000, Loss: 498.271484375\n",
      "Epoch 654/1000, Loss: 498.2735595703125\n",
      "Epoch 655/1000, Loss: 498.27728271484375\n",
      "Epoch 656/1000, Loss: 498.2794189453125\n",
      "Epoch 657/1000, Loss: 498.27825927734375\n",
      "Epoch 658/1000, Loss: 498.2743225097656\n",
      "Epoch 659/1000, Loss: 498.2715759277344\n",
      "Epoch 660/1000, Loss: 498.27142333984375\n",
      "Epoch 661/1000, Loss: 498.27227783203125\n",
      "Epoch 662/1000, Loss: 498.27178955078125\n",
      "Epoch 663/1000, Loss: 498.27056884765625\n",
      "Epoch 664/1000, Loss: 498.2702941894531\n",
      "Epoch 665/1000, Loss: 498.271484375\n",
      "Epoch 666/1000, Loss: 498.27191162109375\n",
      "Epoch 667/1000, Loss: 498.2706604003906\n",
      "Epoch 668/1000, Loss: 498.26885986328125\n",
      "Epoch 669/1000, Loss: 498.268798828125\n",
      "Epoch 670/1000, Loss: 498.2701721191406\n",
      "Epoch 671/1000, Loss: 498.27069091796875\n",
      "Epoch 672/1000, Loss: 498.269775390625\n",
      "Epoch 673/1000, Loss: 498.2686767578125\n",
      "Epoch 674/1000, Loss: 498.26885986328125\n",
      "Epoch 675/1000, Loss: 498.2694091796875\n",
      "Epoch 676/1000, Loss: 498.2694396972656\n",
      "Epoch 677/1000, Loss: 498.26898193359375\n",
      "Epoch 678/1000, Loss: 498.2687683105469\n",
      "Epoch 679/1000, Loss: 498.2689208984375\n",
      "Epoch 680/1000, Loss: 498.26898193359375\n",
      "Epoch 681/1000, Loss: 498.26873779296875\n",
      "Epoch 682/1000, Loss: 498.2686767578125\n",
      "Epoch 683/1000, Loss: 498.26873779296875\n",
      "Epoch 684/1000, Loss: 498.26873779296875\n",
      "Epoch 685/1000, Loss: 498.2686462402344\n",
      "Epoch 686/1000, Loss: 498.2685241699219\n",
      "Epoch 687/1000, Loss: 498.2685546875\n",
      "Epoch 688/1000, Loss: 498.2685546875\n",
      "Epoch 689/1000, Loss: 498.2684631347656\n",
      "Epoch 690/1000, Loss: 498.26837158203125\n",
      "Epoch 691/1000, Loss: 498.2684326171875\n",
      "Epoch 692/1000, Loss: 498.26849365234375\n",
      "Epoch 693/1000, Loss: 498.26849365234375\n",
      "Epoch 694/1000, Loss: 498.2683410644531\n",
      "Epoch 695/1000, Loss: 498.268310546875\n",
      "Epoch 696/1000, Loss: 498.268310546875\n",
      "Epoch 697/1000, Loss: 498.2684020996094\n",
      "Epoch 698/1000, Loss: 498.26837158203125\n",
      "Epoch 699/1000, Loss: 498.2682800292969\n",
      "Epoch 700/1000, Loss: 498.26824951171875\n",
      "Epoch 701/1000, Loss: 498.26824951171875\n",
      "Epoch 702/1000, Loss: 498.268310546875\n",
      "Epoch 703/1000, Loss: 498.26824951171875\n",
      "Epoch 704/1000, Loss: 498.26824951171875\n",
      "Epoch 705/1000, Loss: 498.2681884765625\n",
      "Epoch 706/1000, Loss: 498.26824951171875\n",
      "Epoch 707/1000, Loss: 498.2682800292969\n",
      "Epoch 708/1000, Loss: 498.2681884765625\n",
      "Epoch 709/1000, Loss: 498.2681884765625\n",
      "Epoch 710/1000, Loss: 498.2681884765625\n",
      "Epoch 711/1000, Loss: 498.26812744140625\n",
      "Epoch 712/1000, Loss: 498.2682189941406\n",
      "Epoch 713/1000, Loss: 498.2681579589844\n",
      "Epoch 714/1000, Loss: 498.2681884765625\n",
      "Epoch 715/1000, Loss: 498.2681579589844\n",
      "Epoch 716/1000, Loss: 498.2681579589844\n",
      "Epoch 717/1000, Loss: 498.26812744140625\n",
      "Epoch 718/1000, Loss: 498.26812744140625\n",
      "Epoch 719/1000, Loss: 498.2680969238281\n",
      "Epoch 720/1000, Loss: 498.26806640625\n",
      "Epoch 721/1000, Loss: 498.2680969238281\n",
      "Epoch 722/1000, Loss: 498.26806640625\n",
      "Epoch 723/1000, Loss: 498.2680969238281\n",
      "Epoch 724/1000, Loss: 498.26806640625\n",
      "Epoch 725/1000, Loss: 498.26806640625\n",
      "Epoch 726/1000, Loss: 498.2680969238281\n",
      "Epoch 727/1000, Loss: 498.26812744140625\n",
      "Epoch 728/1000, Loss: 498.2680969238281\n",
      "Epoch 729/1000, Loss: 498.2680969238281\n",
      "Epoch 730/1000, Loss: 498.26806640625\n",
      "Epoch 731/1000, Loss: 498.26806640625\n",
      "Epoch 732/1000, Loss: 498.2680969238281\n",
      "Epoch 733/1000, Loss: 498.26806640625\n",
      "Epoch 734/1000, Loss: 498.26806640625\n",
      "Epoch 735/1000, Loss: 498.2680358886719\n",
      "Epoch 736/1000, Loss: 498.2680358886719\n",
      "Epoch 737/1000, Loss: 498.26800537109375\n",
      "Epoch 738/1000, Loss: 498.2680358886719\n",
      "Epoch 739/1000, Loss: 498.2680358886719\n",
      "Epoch 740/1000, Loss: 498.26806640625\n",
      "Epoch 741/1000, Loss: 498.26806640625\n",
      "Epoch 742/1000, Loss: 498.2680969238281\n",
      "Epoch 743/1000, Loss: 498.26812744140625\n",
      "Epoch 744/1000, Loss: 498.2682189941406\n",
      "Epoch 745/1000, Loss: 498.2684020996094\n",
      "Epoch 746/1000, Loss: 498.268798828125\n",
      "Epoch 747/1000, Loss: 498.269287109375\n",
      "Epoch 748/1000, Loss: 498.2702941894531\n",
      "Epoch 749/1000, Loss: 498.27178955078125\n",
      "Epoch 750/1000, Loss: 498.2734375\n",
      "Epoch 751/1000, Loss: 498.2750244140625\n",
      "Epoch 752/1000, Loss: 498.2744140625\n",
      "Epoch 753/1000, Loss: 498.27227783203125\n",
      "Epoch 754/1000, Loss: 498.2694091796875\n",
      "Epoch 755/1000, Loss: 498.2681884765625\n",
      "Epoch 756/1000, Loss: 498.2691345214844\n",
      "Epoch 757/1000, Loss: 498.2705993652344\n",
      "Epoch 758/1000, Loss: 498.2710876464844\n",
      "Epoch 759/1000, Loss: 498.2699890136719\n",
      "Epoch 760/1000, Loss: 498.26885986328125\n",
      "Epoch 761/1000, Loss: 498.26861572265625\n",
      "Epoch 762/1000, Loss: 498.26934814453125\n",
      "Epoch 763/1000, Loss: 498.2702941894531\n",
      "Epoch 764/1000, Loss: 498.27056884765625\n",
      "Epoch 765/1000, Loss: 498.27056884765625\n",
      "Epoch 766/1000, Loss: 498.27093505859375\n",
      "Epoch 767/1000, Loss: 498.27191162109375\n",
      "Epoch 768/1000, Loss: 498.2732849121094\n",
      "Epoch 769/1000, Loss: 498.274658203125\n",
      "Epoch 770/1000, Loss: 498.27471923828125\n",
      "Epoch 771/1000, Loss: 498.2740478515625\n",
      "Epoch 772/1000, Loss: 498.272216796875\n",
      "Epoch 773/1000, Loss: 498.27032470703125\n",
      "Epoch 774/1000, Loss: 498.26898193359375\n",
      "Epoch 775/1000, Loss: 498.26849365234375\n",
      "Epoch 776/1000, Loss: 498.2686767578125\n",
      "Epoch 777/1000, Loss: 498.2692565917969\n",
      "Epoch 778/1000, Loss: 498.2699279785156\n",
      "Epoch 779/1000, Loss: 498.2702941894531\n",
      "Epoch 780/1000, Loss: 498.2698974609375\n",
      "Epoch 781/1000, Loss: 498.26904296875\n",
      "Epoch 782/1000, Loss: 498.2681579589844\n",
      "Epoch 783/1000, Loss: 498.267822265625\n",
      "Epoch 784/1000, Loss: 498.26824951171875\n",
      "Epoch 785/1000, Loss: 498.2687683105469\n",
      "Epoch 786/1000, Loss: 498.2690124511719\n",
      "Epoch 787/1000, Loss: 498.26885986328125\n",
      "Epoch 788/1000, Loss: 498.26837158203125\n",
      "Epoch 789/1000, Loss: 498.26806640625\n",
      "Epoch 790/1000, Loss: 498.2679443359375\n",
      "Epoch 791/1000, Loss: 498.2679748535156\n",
      "Epoch 792/1000, Loss: 498.26806640625\n",
      "Epoch 793/1000, Loss: 498.2681884765625\n",
      "Epoch 794/1000, Loss: 498.2682800292969\n",
      "Epoch 795/1000, Loss: 498.2682189941406\n",
      "Epoch 796/1000, Loss: 498.26806640625\n",
      "Epoch 797/1000, Loss: 498.2679138183594\n",
      "Epoch 798/1000, Loss: 498.267822265625\n",
      "Epoch 799/1000, Loss: 498.267822265625\n",
      "Epoch 800/1000, Loss: 498.2679443359375\n",
      "Epoch 801/1000, Loss: 498.26800537109375\n",
      "Epoch 802/1000, Loss: 498.26800537109375\n",
      "Epoch 803/1000, Loss: 498.2679443359375\n",
      "Epoch 804/1000, Loss: 498.2679138183594\n",
      "Epoch 805/1000, Loss: 498.2678527832031\n",
      "Epoch 806/1000, Loss: 498.26788330078125\n",
      "Epoch 807/1000, Loss: 498.2678527832031\n",
      "Epoch 808/1000, Loss: 498.2679443359375\n",
      "Epoch 809/1000, Loss: 498.2680358886719\n",
      "Epoch 810/1000, Loss: 498.26824951171875\n",
      "Epoch 811/1000, Loss: 498.26849365234375\n",
      "Epoch 812/1000, Loss: 498.268798828125\n",
      "Epoch 813/1000, Loss: 498.26934814453125\n",
      "Epoch 814/1000, Loss: 498.2702331542969\n",
      "Epoch 815/1000, Loss: 498.2713928222656\n",
      "Epoch 816/1000, Loss: 498.2732238769531\n",
      "Epoch 817/1000, Loss: 498.27520751953125\n",
      "Epoch 818/1000, Loss: 498.27752685546875\n",
      "Epoch 819/1000, Loss: 498.278564453125\n",
      "Epoch 820/1000, Loss: 498.2781982421875\n",
      "Epoch 821/1000, Loss: 498.2753601074219\n",
      "Epoch 822/1000, Loss: 498.27166748046875\n",
      "Epoch 823/1000, Loss: 498.2686767578125\n",
      "Epoch 824/1000, Loss: 498.26776123046875\n",
      "Epoch 825/1000, Loss: 498.26885986328125\n",
      "Epoch 826/1000, Loss: 498.2707214355469\n",
      "Epoch 827/1000, Loss: 498.27191162109375\n",
      "Epoch 828/1000, Loss: 498.2715148925781\n",
      "Epoch 829/1000, Loss: 498.27008056640625\n",
      "Epoch 830/1000, Loss: 498.2685546875\n",
      "Epoch 831/1000, Loss: 498.26776123046875\n",
      "Epoch 832/1000, Loss: 498.26806640625\n",
      "Epoch 833/1000, Loss: 498.26898193359375\n",
      "Epoch 834/1000, Loss: 498.26959228515625\n",
      "Epoch 835/1000, Loss: 498.26947021484375\n",
      "Epoch 836/1000, Loss: 498.26885986328125\n",
      "Epoch 837/1000, Loss: 498.26806640625\n",
      "Epoch 838/1000, Loss: 498.26763916015625\n",
      "Epoch 839/1000, Loss: 498.2677307128906\n",
      "Epoch 840/1000, Loss: 498.2681884765625\n",
      "Epoch 841/1000, Loss: 498.2685852050781\n",
      "Epoch 842/1000, Loss: 498.2685852050781\n",
      "Epoch 843/1000, Loss: 498.26824951171875\n",
      "Epoch 844/1000, Loss: 498.2677917480469\n",
      "Epoch 845/1000, Loss: 498.267578125\n",
      "Epoch 846/1000, Loss: 498.26763916015625\n",
      "Epoch 847/1000, Loss: 498.2678527832031\n",
      "Epoch 848/1000, Loss: 498.26806640625\n",
      "Epoch 849/1000, Loss: 498.26806640625\n",
      "Epoch 850/1000, Loss: 498.2679443359375\n",
      "Epoch 851/1000, Loss: 498.2676696777344\n",
      "Epoch 852/1000, Loss: 498.267578125\n",
      "Epoch 853/1000, Loss: 498.267578125\n",
      "Epoch 854/1000, Loss: 498.2676086425781\n",
      "Epoch 855/1000, Loss: 498.2676696777344\n",
      "Epoch 856/1000, Loss: 498.26776123046875\n",
      "Epoch 857/1000, Loss: 498.26776123046875\n",
      "Epoch 858/1000, Loss: 498.2677001953125\n",
      "Epoch 859/1000, Loss: 498.2675476074219\n",
      "Epoch 860/1000, Loss: 498.26751708984375\n",
      "Epoch 861/1000, Loss: 498.2674560546875\n",
      "Epoch 862/1000, Loss: 498.2675476074219\n",
      "Epoch 863/1000, Loss: 498.267578125\n",
      "Epoch 864/1000, Loss: 498.267578125\n",
      "Epoch 865/1000, Loss: 498.26763916015625\n",
      "Epoch 866/1000, Loss: 498.267578125\n",
      "Epoch 867/1000, Loss: 498.267578125\n",
      "Epoch 868/1000, Loss: 498.2675476074219\n",
      "Epoch 869/1000, Loss: 498.2674560546875\n",
      "Epoch 870/1000, Loss: 498.26751708984375\n",
      "Epoch 871/1000, Loss: 498.2674865722656\n",
      "Epoch 872/1000, Loss: 498.267578125\n",
      "Epoch 873/1000, Loss: 498.267578125\n",
      "Epoch 874/1000, Loss: 498.2676696777344\n",
      "Epoch 875/1000, Loss: 498.2677917480469\n",
      "Epoch 876/1000, Loss: 498.267822265625\n",
      "Epoch 877/1000, Loss: 498.26800537109375\n",
      "Epoch 878/1000, Loss: 498.268310546875\n",
      "Epoch 879/1000, Loss: 498.268798828125\n",
      "Epoch 880/1000, Loss: 498.26947021484375\n",
      "Epoch 881/1000, Loss: 498.2705993652344\n",
      "Epoch 882/1000, Loss: 498.27203369140625\n",
      "Epoch 883/1000, Loss: 498.27398681640625\n",
      "Epoch 884/1000, Loss: 498.27587890625\n",
      "Epoch 885/1000, Loss: 498.2776184082031\n",
      "Epoch 886/1000, Loss: 498.27752685546875\n",
      "Epoch 887/1000, Loss: 498.2760009765625\n",
      "Epoch 888/1000, Loss: 498.2727966308594\n",
      "Epoch 889/1000, Loss: 498.2698669433594\n",
      "Epoch 890/1000, Loss: 498.2684326171875\n",
      "Epoch 891/1000, Loss: 498.2689514160156\n",
      "Epoch 892/1000, Loss: 498.2703552246094\n",
      "Epoch 893/1000, Loss: 498.27154541015625\n",
      "Epoch 894/1000, Loss: 498.2716369628906\n",
      "Epoch 895/1000, Loss: 498.2705383300781\n",
      "Epoch 896/1000, Loss: 498.26910400390625\n",
      "Epoch 897/1000, Loss: 498.2680969238281\n",
      "Epoch 898/1000, Loss: 498.2679138183594\n",
      "Epoch 899/1000, Loss: 498.2682800292969\n",
      "Epoch 900/1000, Loss: 498.26873779296875\n",
      "Epoch 901/1000, Loss: 498.26898193359375\n",
      "Epoch 902/1000, Loss: 498.2687072753906\n",
      "Epoch 903/1000, Loss: 498.2682800292969\n",
      "Epoch 904/1000, Loss: 498.2679443359375\n",
      "Epoch 905/1000, Loss: 498.2677917480469\n",
      "Epoch 906/1000, Loss: 498.2679443359375\n",
      "Epoch 907/1000, Loss: 498.2680969238281\n",
      "Epoch 908/1000, Loss: 498.268310546875\n",
      "Epoch 909/1000, Loss: 498.2681884765625\n",
      "Epoch 910/1000, Loss: 498.2680358886719\n",
      "Epoch 911/1000, Loss: 498.2677001953125\n",
      "Epoch 912/1000, Loss: 498.2675476074219\n",
      "Epoch 913/1000, Loss: 498.26751708984375\n",
      "Epoch 914/1000, Loss: 498.26763916015625\n",
      "Epoch 915/1000, Loss: 498.26776123046875\n",
      "Epoch 916/1000, Loss: 498.26776123046875\n",
      "Epoch 917/1000, Loss: 498.2676696777344\n",
      "Epoch 918/1000, Loss: 498.2674560546875\n",
      "Epoch 919/1000, Loss: 498.2674255371094\n",
      "Epoch 920/1000, Loss: 498.26739501953125\n",
      "Epoch 921/1000, Loss: 498.2674255371094\n",
      "Epoch 922/1000, Loss: 498.2676086425781\n",
      "Epoch 923/1000, Loss: 498.2675476074219\n",
      "Epoch 924/1000, Loss: 498.2675476074219\n",
      "Epoch 925/1000, Loss: 498.2675476074219\n",
      "Epoch 926/1000, Loss: 498.2674865722656\n",
      "Epoch 927/1000, Loss: 498.2674560546875\n",
      "Epoch 928/1000, Loss: 498.2675476074219\n",
      "Epoch 929/1000, Loss: 498.2676696777344\n",
      "Epoch 930/1000, Loss: 498.2677917480469\n",
      "Epoch 931/1000, Loss: 498.2679443359375\n",
      "Epoch 932/1000, Loss: 498.26806640625\n",
      "Epoch 933/1000, Loss: 498.268310546875\n",
      "Epoch 934/1000, Loss: 498.2686462402344\n",
      "Epoch 935/1000, Loss: 498.26910400390625\n",
      "Epoch 936/1000, Loss: 498.2698669433594\n",
      "Epoch 937/1000, Loss: 498.2708435058594\n",
      "Epoch 938/1000, Loss: 498.2721252441406\n",
      "Epoch 939/1000, Loss: 498.2737121582031\n",
      "Epoch 940/1000, Loss: 498.2752380371094\n",
      "Epoch 941/1000, Loss: 498.27691650390625\n",
      "Epoch 942/1000, Loss: 498.2778015136719\n",
      "Epoch 943/1000, Loss: 498.2779235839844\n",
      "Epoch 944/1000, Loss: 498.276611328125\n",
      "Epoch 945/1000, Loss: 498.2743835449219\n",
      "Epoch 946/1000, Loss: 498.2719421386719\n",
      "Epoch 947/1000, Loss: 498.2698974609375\n",
      "Epoch 948/1000, Loss: 498.2690734863281\n",
      "Epoch 949/1000, Loss: 498.26922607421875\n",
      "Epoch 950/1000, Loss: 498.2701416015625\n",
      "Epoch 951/1000, Loss: 498.2710266113281\n",
      "Epoch 952/1000, Loss: 498.27142333984375\n",
      "Epoch 953/1000, Loss: 498.27081298828125\n",
      "Epoch 954/1000, Loss: 498.26959228515625\n",
      "Epoch 955/1000, Loss: 498.268310546875\n",
      "Epoch 956/1000, Loss: 498.2674865722656\n",
      "Epoch 957/1000, Loss: 498.2674560546875\n",
      "Epoch 958/1000, Loss: 498.26800537109375\n",
      "Epoch 959/1000, Loss: 498.2687072753906\n",
      "Epoch 960/1000, Loss: 498.2691955566406\n",
      "Epoch 961/1000, Loss: 498.2691650390625\n",
      "Epoch 962/1000, Loss: 498.2686462402344\n",
      "Epoch 963/1000, Loss: 498.2679443359375\n",
      "Epoch 964/1000, Loss: 498.26739501953125\n",
      "Epoch 965/1000, Loss: 498.26727294921875\n",
      "Epoch 966/1000, Loss: 498.267333984375\n",
      "Epoch 967/1000, Loss: 498.2677307128906\n",
      "Epoch 968/1000, Loss: 498.2679443359375\n",
      "Epoch 969/1000, Loss: 498.26800537109375\n",
      "Epoch 970/1000, Loss: 498.2677917480469\n",
      "Epoch 971/1000, Loss: 498.2675476074219\n",
      "Epoch 972/1000, Loss: 498.2673645019531\n",
      "Epoch 973/1000, Loss: 498.26727294921875\n",
      "Epoch 974/1000, Loss: 498.26727294921875\n",
      "Epoch 975/1000, Loss: 498.2673645019531\n",
      "Epoch 976/1000, Loss: 498.2674865722656\n",
      "Epoch 977/1000, Loss: 498.26751708984375\n",
      "Epoch 978/1000, Loss: 498.26751708984375\n",
      "Epoch 979/1000, Loss: 498.2674560546875\n",
      "Epoch 980/1000, Loss: 498.2673645019531\n",
      "Epoch 981/1000, Loss: 498.2672119140625\n",
      "Epoch 982/1000, Loss: 498.2672119140625\n",
      "Epoch 983/1000, Loss: 498.2672119140625\n",
      "Epoch 984/1000, Loss: 498.2672119140625\n",
      "Epoch 985/1000, Loss: 498.2673034667969\n",
      "Epoch 986/1000, Loss: 498.2673034667969\n",
      "Epoch 987/1000, Loss: 498.2673034667969\n",
      "Epoch 988/1000, Loss: 498.26727294921875\n",
      "Epoch 989/1000, Loss: 498.26727294921875\n",
      "Epoch 990/1000, Loss: 498.2672424316406\n",
      "Epoch 991/1000, Loss: 498.2671813964844\n",
      "Epoch 992/1000, Loss: 498.2671813964844\n",
      "Epoch 993/1000, Loss: 498.26715087890625\n",
      "Epoch 994/1000, Loss: 498.2672119140625\n",
      "Epoch 995/1000, Loss: 498.26715087890625\n",
      "Epoch 996/1000, Loss: 498.2672424316406\n",
      "Epoch 997/1000, Loss: 498.26727294921875\n",
      "Epoch 998/1000, Loss: 498.26727294921875\n",
      "Epoch 999/1000, Loss: 498.2673034667969\n",
      "Epoch 1000/1000, Loss: 498.2673034667969\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Hyperparameters\n",
    "input_size = X_train.shape[1]  # Size of your one-hot vector\n",
    "print(\"input_size : \", input_size)\n",
    "hidden_size = 300  # Can be any number\n",
    "output_size = y_train.shape[1]  # Number of classes\n",
    "num_epochs = 10\n",
    "learning_rate = 0.001\n",
    "\n",
    "# Initialize the RNN model, Loss function and Optimizer\n",
    "model = rnn_pytorch.RNN_Model(input_size, hidden_size, output_size)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# training_data = list(zip(concatinated_vector_train, sentences_diacritics))\n",
    "\n",
    "sequence_length = utils.get_max_len(sequences=sentences_without_diacritics)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(\"trainnig on : \", device)\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (sentences, labels) in enumerate(dataloader):  # Assuming you have a PyTorch DataLoader\n",
    "        # print(\"sentences : \", sentences) # ex : [[0,0,0,0,0,0,0,0,0,0,0,0,0,0,1], [0,0,0,0,0,0,0,0,0,0,0,0,0,0,1]]\n",
    "        # print(\"labels : \", labels) # ex : ['`', '``]\n",
    "        # print(\"sentences shape : \", sentences.shape) # ex : torch.Size([1, 15])\n",
    "        # sentences = sentences.float().reshape(-1, sequence_length, input_size).to(device)\n",
    "        # print(\"sentences shape : \", sentences.shape) # ex : torch.Size([1, 15])\n",
    "        # # labels = [character_encoding.DIACRITICS.index(char) for char in labels]\n",
    "        # # labels = labels.to(device)\n",
    "        #  # Convert one-hot encoded labels to character form\n",
    "        # labels_list = labels.tolist()[0]\n",
    "        # print(\"labels_list : \", labels_list)\n",
    "        # # print(\"labels_list : \", labels_list)\n",
    "        # # labels_char = [character_encoding.DIACRITICS[i] for i, val in enumerate(labels_list) if val == 1.]\n",
    "        # labels_char = []\n",
    "        # for i, val in enumerate(labels_list):\n",
    "        #     for j, v in enumerate(val):\n",
    "        #         if v == 1.:\n",
    "        #             labels_char.append(character_encoding.DIACRITICS[j])\n",
    "        # # print(\"labels_char : \", labels_char)\n",
    "        # # Find the index of the character in DIACRITICS\n",
    "        # labels_index = [character_encoding.DIACRITICS.index(char) for char in labels_char]\n",
    "        # labels_index = torch.tensor([labels_index]).to(device)\n",
    "        # print(\"labels_index : \", labels_index)\n",
    "\n",
    "        # # Forward pass\n",
    "        # outputs = model(sentences)\n",
    "        # loss = criterion(outputs, labels_index)\n",
    "\n",
    "        # # Backward and optimize\n",
    "        # optimizer.zero_grad()\n",
    "        # loss.backward()\n",
    "        # optimizer.step()\n",
    "        input = sentences.view(batch_size, -1,input_size).to(device)\n",
    "        # print(\"input shape : \", input.shape)\n",
    "        labels = labels.view(batch_size, -1, output_size)\n",
    "        _, labels_index = torch.max(labels, dim=1)\n",
    "        # transform to float32\n",
    "        labels_index = labels_index.type(torch.float32)\n",
    "        # print(\"labels_index : \", labels_index)\n",
    "        labels_index = labels_index.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        # forward pass\n",
    "        outputs = model(input)\n",
    "        # calculate the loss\n",
    "        loss = criterion(outputs, labels_index)\n",
    "        # backward pass\n",
    "        loss.backward()\n",
    "        # update the weights\n",
    "        optimizer.step()\n",
    "        \n",
    "\n",
    "    print(f'Epoch {epoch+1}/{num_epochs}, Loss: {loss.item()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Evaluation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preparing Validation data to be passed into the `model.evaluate()`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "testing on a given sentence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Predict the diacritics of the validation data\n",
    "# target_text = sentences[0]\n",
    "# _, original_diacritics = character_encoding.remove_diacritics(target_text, is_sentence=True)\n",
    "# sentence_test = sentences_without_diacritics[0]\n",
    "\n",
    "# sentences = utils.split_data_to_sentences(sentence_test)\n",
    "# # get one hot encoding for each character in sentence_to_test\n",
    "# x_test = []\n",
    "# batch_concatinated_vector_train = []\n",
    "# for i,sentence in enumerate([sentence_test]):\n",
    "#     sentence_vec = []\n",
    "#     words = split_data_to_words(sentence)\n",
    "#     for j, word in enumerate(words):\n",
    "#         try:\n",
    "#             word_vec = embedding_model.vector(word)\n",
    "#         except:\n",
    "#             print(\"word not found : \\\"\", word , \"\\\"\")\n",
    "        \n",
    "#         for k,c in enumerate(word):\n",
    "#             one_hot = character_encoding.CharToOneHOt(c)\n",
    "#             v = np.concatenate((word_vec, one_hot), axis=None)\n",
    "#             sentence_vec.append(v)\n",
    "\n",
    "#         # add space between words except for the last word\n",
    "#         if j != len(words) - 1:    \n",
    "#             one_hot = character_encoding.CharToOneHOt(' ')\n",
    "#             v = np.concatenate((word_vec, one_hot), axis = None)\n",
    "#             sentence_vec.append(v)\n",
    "#     batch_concatinated_vector_train.append(sentence_vec)\n",
    "\n",
    "# x_test = utils.padding(batch_concatinated_vector_train)\n",
    "# # print(\"x_test : \", x_test)\n",
    "# x_test = np.array(x_test)\n",
    "# print(\"x_test len : \", len(sentence_test))\n",
    "# print(\"X_test size : \", x_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_test len :  38\n",
      "X_test size :  (38, 48)\n"
     ]
    }
   ],
   "source": [
    "# Predict the diacritics of the validation data\n",
    "target_text = sentences[0]\n",
    "_, original_diacritics = character_encoding.remove_diacritics(target_text, is_sentence=True)\n",
    "sentence_test = sentences_without_diacritics[0]\n",
    "\n",
    "sentences = utils.split_data_to_sentences(sentence_test)\n",
    "# get one hot encoding for each character in sentence_to_test\n",
    "x_test = []\n",
    "for i,sentence in enumerate([sentence_test]):\n",
    "    words = split_data_to_words(sentence)\n",
    "    for j, word in enumerate(words):\n",
    "        try:\n",
    "            word_vec = embedding_model.vector(word)\n",
    "        except:\n",
    "            print(\"word not found : \\\"\", word , \"\\\"\")\n",
    "        \n",
    "        for k,c in enumerate(word):\n",
    "            one_hot = character_encoding.CharToOneHOt(c)\n",
    "            v = np.concatenate((word_vec, one_hot), axis=None)\n",
    "            x_test.append(v)\n",
    "\n",
    "        # add space between words except for the last word\n",
    "        if j != len(words) - 1:    \n",
    "            one_hot = character_encoding.CharToOneHOt(' ')\n",
    "            v = np.concatenate((word_vec, one_hot), axis = None)\n",
    "            x_test.append(v) \n",
    "\n",
    "# print(\"x_test : \", x_test)\n",
    "x_test = np.array(x_test)\n",
    "print(\"x_test len : \", len(sentence_test))\n",
    "print(\"X_test size : \", x_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2s/step\n"
     ]
    }
   ],
   "source": [
    "y_pred = model.predict(x_test)\n",
    "\n",
    "# # Make predictions at each timestamp\n",
    "# y_pred = []\n",
    "\n",
    "# # Iterate over each timestamp\n",
    "# for i in range(len(sentence_test)):\n",
    "#     # Get the prediction at the current timestamp\n",
    "#     predictions = model.predict(x_test[:, :i+1])\n",
    "#     y_pred.append(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X :  قوله أو قطع الأول يده إلخ قال الزركشي \n",
      "Len X  :  38\n",
      "Len Y predicted  :  38\n",
      "Original Text :  قَوْلُهُ أَوْ قَطَعَ الْأَوَّلُ يَدَهُ إلَخْ قَالَ الزَّرْكَشِيُّ \n",
      "Restored Text :  قَوْلُهُ أَوْ قَطَعَ الُأَوَّلُ يَدَهُ إلَخْ قَالَ الزَّرْكَشِيُّ \n"
     ]
    }
   ],
   "source": [
    "# something is wrong here\n",
    "print(\"X : \", sentence_test)\n",
    "print(\"Len X  : \", len(sentence_test))\n",
    "print(\"Len Y predicted  : \", len(y_pred))\n",
    "\n",
    "\n",
    "predicted_diacritics = []\n",
    "for i in range(len(y_pred)):\n",
    "    predicted_diacritics.append(character_encoding.DIACRITICS[np.argmax(y_pred[i])])\n",
    "\n",
    "# Print the predicted diacritics\n",
    "restored_text = character_encoding.restore_diacritics(sentence_test, predicted_diacritics)\n",
    "print(\"Original Text : \", target_text)\n",
    "print(\"Restored Text : \", restored_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original diacritics  :  ['FATHA', 'SUKUN', 'DAMMA', 'DAMMA', ' ', 'FATHA', 'SUKUN', ' ', 'FATHA', 'FATHA', 'FATHA', ' ', ' ', 'SUKUN', 'FATHA', 'SHADDA_FATHA', 'DAMMA', ' ', 'FATHA', 'FATHA', 'DAMMA', ' ', ' ', 'FATHA', 'SUKUN', ' ', 'FATHA', ' ', 'FATHA', ' ', ' ', ' ', 'SHADDA_FATHA', 'SUKUN', 'FATHA', 'KASRA', 'SHADDA_DAMMA', ' ']\n",
      "Predicted diacritics :  ['FATHA', 'SUKUN', 'DAMMA', 'DAMMA', ' ', 'FATHA', 'SUKUN', ' ', 'FATHA', 'FATHA', 'FATHA', ' ', ' ', 'DAMMA', 'FATHA', 'SHADDA_FATHA', 'DAMMA', ' ', 'FATHA', 'FATHA', 'DAMMA', ' ', ' ', 'FATHA', 'SUKUN', ' ', 'FATHA', ' ', 'FATHA', ' ', ' ', ' ', 'SHADDA_FATHA', 'SUKUN', 'FATHA', 'KASRA', 'SHADDA_DAMMA', ' ']\n"
     ]
    }
   ],
   "source": [
    "print(\"Original diacritics  : \", character_encoding.map_text_to_diacritic(original_diacritics))\n",
    "print(\"Predicted diacritics : \", character_encoding.map_text_to_diacritic(predicted_diacritics))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def diacritics_error_rate(original_diacritics, predicted_diacritics):\n",
    "    error = 0\n",
    "    for i in range(len(original_diacritics)):\n",
    "        if original_diacritics[i] != predicted_diacritics[i]:\n",
    "            error += 1\n",
    "    return error / len(original_diacritics) * 100, error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Diacritic Error Rate =  2.631578947368421 %\n",
      "Diacritic Correct Rate =  97.36842105263158 %\n",
      "Number of Misclassified =  1 out of 38\n"
     ]
    }
   ],
   "source": [
    "diacritic_error_rate, number_of_mis_classified = diacritics_error_rate(original_diacritics, predicted_diacritics)\n",
    "print(\"Diacritic Error Rate = \", diacritic_error_rate, \"%\")\n",
    "print(\"Diacritic Correct Rate = \", 100 - diacritic_error_rate, \"%\")\n",
    "print(\"Number of Misclassified = \", number_of_mis_classified, \"out of\", len(original_diacritics))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
