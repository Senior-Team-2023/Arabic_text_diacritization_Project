{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read data from train.txt and filter it from unwanted patterns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configurations : \n",
      "number_test_of_words : 10000\n",
      "number_validation_of_words : 1000\n",
      "classifier : lstm\n",
      "embedding : fasttext\n",
      "is_training : True\n",
      "word_embeddings : False\n",
      "character_embeddings : False\n",
      "embedding_vector_size : 100\n",
      "character_embedding_vector_size : 200\n",
      "batch_size : 64\n",
      "num_epochs : 7\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from keras.utils import to_categorical\n",
    "from Embeddings import Word2Vec, FastText\n",
    "from Preprocessing import utils, character_encoding\n",
    "from Models import rnn, lstm, bilstm\n",
    "import config as conf\n",
    "\n",
    "config = conf.ConfigLoader().load_config()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clean data from special characcters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training_set قَوْلُهُ : ( أَوْ قَطَعَ الْأَوَّلُ يَدَهُ إلَخْ ) قَالَ الزَّرْكَشِيُّ( 14 / 123 )\n",
      "ابْنُ عَرَفَةَ : قَوْلُهُ : بِلَفْظٍ يَقْتَضِيه كَإِنْكَارِ غَيْرِ حَدِيثٍ بِالْإِسْلَامِ وُجُوبَ مَا عُلِمَ وُجُوبُهُ مِنْ الدِّينِ ضَرُورَةً ( كَإِلْقَاءِ مُصْحَفٍ بِقَذَرٍ وَشَدِّ زُنَّارٍ ) ابْنُ عَرَفَةَ : قَوْلُ ابْنِ شَاسٍ : أَوْ بِفِعْلٍ يَتَضَمَّنُهُ هُوَ كَلُبْسِ الزُّنَّارِ وَإِلْقَاءِ الْمُصْحَفِ فِي صَرِيحِ النَّجَاسَةِ وَالسُّجُودِ لِلصَّنَمِ وَنَحْوِ ذَلِكَ ( وَسِحْرٍ ) مُحَمَّدٌ : قَوْلُ مَالِكٍ و\n",
      "\n",
      "\n",
      "filtered_training_set قَوْلُهُ أَوْ قَطَعَ الْأَوَّلُ يَدَهُ إلَخْ قَالَ الزَّرْكَشِيُّ \n",
      "ابْنُ عَرَفَةَ قَوْلُهُ بِلَفْظٍ يَقْتَضِيه كَإِنْكَارِ غَيْرِ حَدِيثٍ بِالْإِسْلَامِ وُجُوبَ مَا عُلِمَ وُجُوبُهُ مِنْ الدِّينِ ضَرُورَةً كَإِلْقَاءِ مُصْحَفٍ بِقَذَرٍ وَشَدِّ زُنَّارٍ ابْنُ عَرَفَةَ قَوْلُ ابْنِ شَاسٍ أَوْ بِفِعْلٍ يَتَضَمَّنُهُ هُوَ كَلُبْسِ الزُّنَّارِ وَإِلْقَاءِ الْمُصْحَفِ فِي صَرِيحِ النَّجَاسَةِ وَالسُّجُودِ لِلصَّنَمِ وَنَحْوِ ذَلِكَ وَسِحْرٍ مُحَمَّدٌ قَوْلُ مَالِكٍ وَأَصْحَابِهِ أَنَّ السَّاحِرَ كَافِ\n"
     ]
    }
   ],
   "source": [
    "training_set = utils.read_data(f\"./Dataset/train.txt\")\n",
    "print(\"training_set\", training_set[0:500])\n",
    "print('\\n')\n",
    "filtered_training_set = utils.filter_data(training_set)\n",
    "print(\"filtered_training_set\", filtered_training_set[0:500])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation_set ( 27 ) قَوْلُهُ : وَلَا تُكْرَهُ ضِيَافَتُهُ .\n",
      "( الْفَرْقُ الثَّالِثُ وَالثَّلَاثُونَ بَيْنَ قَاعِدَةِ تَقَدُّمِ الْحُكْمِ عَلَى سَبَبِهِ دُونَ شَرْطِهِ أَوْ شَرْطِهِ دُونَ سَبَبِهِ وَبَيْنَ قَاعِدَةِ تَقَدُّمِهِ عَلَى السَّبَبِ وَالشَّرْطِ جَمِيعًا ) وَتَحْرِيرُهُ أَنَّ الْحُكْمَ إنْ كَانَ لَهُ سَبَبٌ بِغَيْرِ شَرْطٍ فَتَقَدَّمَ عَلَيْهِ لَا يُعْتَبَرُ أَوْ كَانَ لَهُ سَبَبَانِ أَوْ أَسْبَابٌ فَتَقَدَّمَ عَلَى جَمِيعِهَا لَمْ يُعْتَبَرْ أَوْ عَلَى بَعْضِهَا دُونَ بَعْضٍ اُعْتُبِرَ بِنَاءً عَلَى\n",
      "\n",
      "\n",
      "filtered_validation_set  قَوْلُهُ وَلَا تُكْرَهُ ضِيَافَتُهُ \n",
      " الْفَرْقُ الثَّالِثُ وَالثَّلَاثُونَ بَيْنَ قَاعِدَةِ تَقَدُّمِ الْحُكْمِ عَلَى سَبَبِهِ دُونَ شَرْطِهِ أَوْ شَرْطِهِ دُونَ سَبَبِهِ وَبَيْنَ قَاعِدَةِ تَقَدُّمِهِ عَلَى السَّبَبِ وَالشَّرْطِ جَمِيعًا وَتَحْرِيرُهُ أَنَّ الْحُكْمَ إنْ كَانَ لَهُ سَبَبٌ بِغَيْرِ شَرْطٍ فَتَقَدَّمَ عَلَيْهِ لَا يُعْتَبَرُ أَوْ كَانَ لَهُ سَبَبَانِ أَوْ أَسْبَابٌ فَتَقَدَّمَ عَلَى جَمِيعِهَا لَمْ يُعْتَبَرْ أَوْ عَلَى بَعْضِهَا دُونَ بَعْضٍ اُعْتُبِرَ بِنَاءً عَلَى سَبَبِ الْخ\n"
     ]
    }
   ],
   "source": [
    "validation_set = utils.read_data(f\"./Dataset/val.txt\")\n",
    "print(\"validation_set\", validation_set[0:500])\n",
    "print('\\n')\n",
    "filtered_validation_set = utils.filter_data(validation_set)\n",
    "print(\"filtered_validation_set\", filtered_validation_set[0:500])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Splite Training data and Validation data into words then separate diacritics from each word\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparing Training Set\n",
    "words_set_train = utils.split_data_to_words(filtered_training_set)\n",
    "num_of_words_train = config['number_test_of_words']\n",
    "text_without_diacritics, diacritic_list = character_encoding.PrepareData(words_set_train[0:num_of_words_train])\n",
    "\n",
    "\n",
    "# Assume this is a validation set\n",
    "words_set_val = utils.split_data_to_words(filtered_validation_set)\n",
    "num_of_words_val = config['number_validation_of_words']\n",
    "text_without_diacritics_validation, diacritic_list_validation = character_encoding.PrepareData(words_set_val[0:num_of_words_val])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Extraction\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split training data to sentences and remove diacritics from each sentence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "sentences = utils.split_data_to_sentences(filtered_training_set)\n",
    "list_of_sentences, list_of_sentences_diacritics = character_encoding.RemoveDiacriticFromSentence(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "list_of_sentences( 50000 ) قوله أو قطع الأول يده إلخ قال الزركشي \n",
      "list_of_sentences_diacritics( 50000 ) ['FATHA', 'SUKUN', 'DAMMA', 'DAMMA', ' ', 'FATHA', 'SUKUN', ' ', 'FATHA', 'FATHA', 'FATHA', ' ', ' ', 'SUKUN', 'FATHA', 'SHADDA_FATHA', 'DAMMA', ' ', 'FATHA', 'FATHA', 'DAMMA', ' ', ' ', 'FATHA', 'SUKUN', ' ', 'FATHA', ' ', 'FATHA', ' ', ' ', ' ', 'SHADDA_FATHA', 'SUKUN', 'FATHA', 'KASRA', 'SHADDA_DAMMA', ' ']\n",
      "original_Text قَوْلُهُ أَوْ قَطَعَ الْأَوَّلُ يَدَهُ إلَخْ قَالَ الزَّرْكَشِيُّ \n",
      "restored_text قَوْلُهُ أَوْ قَطَعَ الْأَوَّلُ يَدَهُ إلَخْ قَالَ الزَّرْكَشِيُّ \n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"list_of_sentences(\",len(list_of_sentences),\")\", list_of_sentences[0])\n",
    "print(\"list_of_sentences_diacritics(\",len(list_of_sentences_diacritics),\")\", character_encoding.map_text_to_diacritic(list_of_sentences_diacritics[0]))\n",
    "\n",
    "# words = utils.split_data_to_words(list_of_sentences[0])\n",
    "restored_text = character_encoding.restore_diacritics(list_of_sentences[0], list_of_sentences_diacritics[0])\n",
    "print(\"original_Text\", sentences[0])\n",
    "print(\"restored_text\", restored_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Choose embedding model from `config.json file`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "character_embedding_model = None\n",
    "embedding_model = None\n",
    "if config[\"word_embeddings\"] == True:\n",
    "    # Choose embedding model from config file\n",
    "    if config[\"embedding\"] == \"word2vec\":\n",
    "        file_path = './Embeddings/word2vec_model.bin'\n",
    "        embedding_model = Word2Vec.W2V(list_of_sentences, vector_size = config[\"embedding_vector_size\"])\n",
    "        # if config[\"character_embeddings\"] == True:\n",
    "        #     character_embedding_model = Word2Vec.W2V(list_of_sentences, vector_size = config[\"character_embedding_vector_size\"], is_character = True)\n",
    "\n",
    "    elif config[\"embedding\"] == \"fasttext\":\n",
    "        file_path = './Embeddings/fasttext_model.bin'\n",
    "        embedding_model = FastText.FastTextEmbedding(list_of_sentences, vector_size = config[\"embedding_vector_size\"])\n",
    "        if config[\"character_embeddings\"] == True:\n",
    "            character_embedding_model = FastText.FastTextEmbedding(list_of_sentences, vector_size = config[\"character_embedding_vector_size\"], is_character = True)\n",
    "\n",
    "\n",
    "    else:\n",
    "        raise Exception(\"Invalid embedding type\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training the word embedding model or load it (if already saved)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if config[\"word_embeddings\"] == True:\n",
    "    is_training = config[\"is_training\"]    # Change this to False if you want to load the model and not train it again\n",
    "\n",
    "    if embedding_model.is_model_saved(file_path) and is_training == False:\n",
    "        embedding_model.load_model(file_path)\n",
    "    else:\n",
    "        embedding_model.train()\n",
    "        # embedding_model.save_model(file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training the Character embedding model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "if config[\"character_embeddings\"] == True:\n",
    "    character_embedding_model.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Concatinate **Training** Word embeddings + Characted Embedding\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "concatinated_vector_train, diacritic_list = utils.concatinate_word_char_embeddings(text_without_diacritics, diacritic_list, embedding_model = embedding_model, character_embedding_model = character_embedding_model)\n",
    "\n",
    "# calculate total character diacritic list for the assert\n",
    "count_train = 0\n",
    "for d in diacritic_list:\n",
    "    count_train += len(d)  \n",
    "assert (len(concatinated_vector_train) == count_train), f\"Error : Train Set Len ({len(concatinated_vector_train)}) != Len diacritic ({count_train}) list have different sizes, \"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Concatinate **Validation** Word embeddings + Characted Embedding\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "concatinated_vector_validation, diacritic_list_validation = utils.concatinate_word_char_embeddings(text_without_diacritics_validation, diacritic_list_validation, embedding_model = embedding_model, character_embedding_model = character_embedding_model)\n",
    "\n",
    "count_validation = 0\n",
    "for d in diacritic_list_validation:\n",
    "    count_validation += len(d) \n",
    "\n",
    "assert (len(concatinated_vector_validation) == count_validation), f\"Error : validation Set Len ({len(concatinated_vector_validation)}) != Len diacritic ({count_validation}) list have different sizes, \""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building The Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input size :  37\n",
      "output size :  15\n"
     ]
    }
   ],
   "source": [
    "input_size = len(concatinated_vector_train[0])\n",
    "output_size = len(character_encoding.DIACRITICS)\n",
    "print(\"input size : \", input_size)\n",
    "print(\"output size : \", output_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Choose Classification model from `config.json file`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "classifier :  lstm\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\marky\\miniconda3\\Lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:205: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    }
   ],
   "source": [
    "# Create an instance of the LSTM class\n",
    "print(\"classifier : \", config[\"classifier\"])\n",
    "if config[\"classifier\"] == \"lstm\":\n",
    "    model = lstm.LSTM_Model(input_shape=(None, 1), output_shape = output_size)\n",
    "\n",
    "elif config[\"classifier\"] == \"bilstm\":\n",
    "    model = bilstm.BiLSTM_Model(input_shape=(None, 1), output_shape = output_size)\n",
    "\n",
    "elif config[\"classifier\"] == \"rnn\":\n",
    "    model = rnn.RNN(input_shape=(None, 1), output_shape = output_size)\n",
    "    \n",
    "else:\n",
    "    raise Exception(\"Invalid model type\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preparing Training data to be passed into the `model.train()`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train size :  (39522, 37)\n",
      "y_train size :  (39522, 15)\n"
     ]
    }
   ],
   "source": [
    "# Convert the training data to the required format\n",
    "X_train = concatinated_vector_train # np.array([[[character_encoding.CharToOneHOt(char)]] for word in text_without_diacritics for char in word])\n",
    "\n",
    "y_train = []\n",
    "for word_diacritic in diacritic_list:\n",
    "    for diacritic in word_diacritic:\n",
    "        #print(utils.map_text_to_diacritic(diacritic))\n",
    "        index = character_encoding.DIACRITICS.index(diacritic)\n",
    "        y_train.append(to_categorical(index, num_classes=output_size))\n",
    "y_train = np.array(y_train)\n",
    "X_train = np.array(X_train)\n",
    "print(\"X_train size : \", X_train.shape)\n",
    "print(\"y_train size : \", y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/7\n",
      "\u001b[1m618/618\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m136s\u001b[0m 211ms/step - accuracy: 0.3508 - loss: 1.8318\n",
      "Epoch 2/7\n",
      "\u001b[1m618/618\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m124s\u001b[0m 200ms/step - accuracy: 0.4740 - loss: 1.5383\n",
      "Epoch 3/7\n",
      "\u001b[1m618/618\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m124s\u001b[0m 200ms/step - accuracy: 0.4757 - loss: 1.5327\n",
      "Epoch 4/7\n",
      "\u001b[1m618/618\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m119s\u001b[0m 193ms/step - accuracy: 0.4755 - loss: 1.5242\n",
      "Epoch 5/7\n",
      "\u001b[1m618/618\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m122s\u001b[0m 197ms/step - accuracy: 0.4729 - loss: 1.5382\n",
      "Epoch 6/7\n",
      "\u001b[1m618/618\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m133s\u001b[0m 216ms/step - accuracy: 0.4741 - loss: 1.5326\n",
      "Epoch 7/7\n",
      "\u001b[1m618/618\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m149s\u001b[0m 241ms/step - accuracy: 0.4787 - loss: 1.5242\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "model.train(X_train, y_train, epochs = config[\"num_epochs\"], batch_size = config[\"batch_size\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Evaluation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preparing Validation data to be passed into the `model.evaluate()`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_validation size :  (39522, 37)\n",
      "y_validation size :  (39522, 15)\n"
     ]
    }
   ],
   "source": [
    "# Convert the validation data to the required format\n",
    "X_validation = concatinated_vector_validation # np.array([[[character_encoding.CharToOneHOt(char)]] for word in text_without_diacritics_validation for char in word])\n",
    "y_validation = []\n",
    "for word_diacritic in diacritic_list_validation:\n",
    "    for diacritic in word_diacritic:\n",
    "        index = character_encoding.DIACRITICS.index(diacritic)\n",
    "        y_validation.append(to_categorical(index, num_classes=output_size))\n",
    "\n",
    "y_validation = np.array(y_train)\n",
    "X_validation = np.array(X_train)\n",
    "print(\"X_validation size : \", X_validation.shape)\n",
    "print(\"y_validation size : \", y_validation.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1236/1236\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m64s\u001b[0m 51ms/step - accuracy: 0.4721 - loss: 1.5410\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model\n",
    "lost , accuracy = model.evaluate(X_validation, y_validation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "testing on a given sentence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_test len :  85\n",
      "X_test size :  (70, 37)\n"
     ]
    }
   ],
   "source": [
    "# Predict the diacritics of the validation data\n",
    "target_text = \"يَأْخُذُونَ بَعْضَ مَا تَيَسَّرَ لَهُمْ أَخْذُهُ فَيَخْتَلِسُونَهُ وَيَجْعَلُونَهُ تَحْتَهُمْ حَتَّى إذَا رَجَعُوا إلَى بُيُوتِهِمْ أَخْرَجُوهُ\"\n",
    "sentence_test = \"يأخذون بعض ما تيسر لهم أخذه فيختلسونه و يجعلونه تحتهم حتى إذا رجعوا إلى بيوتهم أخرجوه\"\n",
    "sentence_words = utils.split_data_to_words(sentence_test)\n",
    "\n",
    "x_test, _ = utils.concatinate_word_char_embeddings(sentence_words, diacritic_list, embedding_model = embedding_model, character_embedding_model = character_embedding_model)\n",
    "# print(\"x_test : \", x_test)\n",
    "x_test = np.array(x_test)\n",
    "print(\"x_test len : \", len(sentence_test))\n",
    "print(\"X_test size : \", x_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 572ms/step\n"
     ]
    }
   ],
   "source": [
    "y_pred = model.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X :  يأخذون بعض ما تيسر لهم أخذه فيختلسونه و يجعلونه تحتهم حتى إذا رجعوا إلى بيوتهم أخرجوه\n",
      "Len X  :  85\n",
      "Len Y predicted  :  70\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[20], line 13\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m# Print the predicted diacritics\u001b[39;00m\n\u001b[0;32m     12\u001b[0m diacritics_pred \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m---> 13\u001b[0m restored_text \u001b[38;5;241m=\u001b[39m \u001b[43mcharacter_encoding\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrestore_diacritics\u001b[49m\u001b[43m(\u001b[49m\u001b[43msentence_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpredicted_diacritics\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOriginal Text : \u001b[39m\u001b[38;5;124m\"\u001b[39m, target_text)\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRestored Text : \u001b[39m\u001b[38;5;124m\"\u001b[39m, restored_text)\n",
      "File \u001b[1;32md:\\Engineering\\Fourth Year\\First Semester\\Natural Language Processing\\Project\\NLP-Project\\Preprocessing\\character_encoding.py:120\u001b[0m, in \u001b[0;36mrestore_diacritics\u001b[1;34m(sentence, diacritic_list)\u001b[0m\n\u001b[0;32m    118\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(sentence)):\n\u001b[0;32m    119\u001b[0m     text \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m sentence[i]\n\u001b[1;32m--> 120\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[43mdiacritic_list\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m):\n\u001b[0;32m    121\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m diacritic \u001b[38;5;129;01min\u001b[39;00m diacritic_list[i]:\n\u001b[0;32m    122\u001b[0m             text \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m diacritic\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "# something is wrong here\n",
    "print(\"X : \", sentence_test)\n",
    "print(\"Len X  : \", len(sentence_test))\n",
    "print(\"Len Y predicted  : \", len(y_pred))\n",
    "\n",
    "\n",
    "predicted_diacritics = []\n",
    "for i in range(len(y_pred)):\n",
    "    predicted_diacritics.append(character_encoding.DIACRITICS[np.argmax(y_pred[i])])\n",
    "\n",
    "# Print the predicted diacritics\n",
    "diacritics_pred = []\n",
    "restored_text = character_encoding.restore_diacritics(sentence_test, predicted_diacritics)\n",
    "print(\"Original Text : \", target_text)\n",
    "print(\"Restored Text : \", restored_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def remove_diacritics(text):\n",
    "    return ''.join(char for char in text if char not in character_encoding.DIACRITICS)\n",
    "\n",
    "def diacritics_error_rate(target_text, predicted_text):\n",
    "    target_clean = remove_diacritics(target_text)\n",
    "    predicted_clean = remove_diacritics(predicted_text)\n",
    "\n",
    "    total_diacritics = sum(1 for c in target_text if c in character_encoding.DIACRITICS) + sum(1 for c in predicted_text if c in character_encoding.DIACRITICS)\n",
    "    error_count = sum(1 for c1, c2 in zip(target_clean, predicted_clean) if c1 != c2 and c1 in character_encoding.ARABIC_ALPHABIT and c2 in character_encoding.ARABIC_ALPHABIT)\n",
    "\n",
    "    if total_diacritics == 0:\n",
    "        return 0.0  # No diacritics, so DER is 0\n",
    "    else:\n",
    "        return error_count / total_diacritics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total_diacritics :  74\n",
      "Diacritic Error Rate =  0.40540540540540543 %\n"
     ]
    }
   ],
   "source": [
    "diacritic_error_rate = diacritics_error_rate(target_text, restored_text)\n",
    "print(\"Diacritic Error Rate = \", diacritic_error_rate, \"%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
