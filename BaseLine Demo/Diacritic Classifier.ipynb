{"cells":[{"cell_type":"markdown","metadata":{},"source":["### Before running the code make sure that you have baseline model saved in Folder ./Models\n","\n","Then add its name down here:\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["model_name = \"\"  # \"model_baseline_EPOCH10\" \n","\n","if model_name == \"\":\n","    print(\"Please specify model name\")\n","    exit(1)"]},{"cell_type":"code","execution_count":81,"metadata":{"executionInfo":{"elapsed":4858,"status":"ok","timestamp":1704134214692,"user":{"displayName":"Mark Yasser","userId":"04355184956843458843"},"user_tz":-120},"id":"WR6a6DkN0d-3"},"outputs":[],"source":["import numpy as np\n","import pandas as pd\n","from tqdm import tqdm\n","import torch\n","from torch import nn\n","import re"]},{"cell_type":"code","execution_count":82,"metadata":{},"outputs":[],"source":["test_set = input(\"Enter the test name : \\n\")"]},{"cell_type":"code","execution_count":84,"metadata":{},"outputs":[],"source":["# Define Diacritics\n","KASRA = \"\\u0650\"\n","DAMMA = \"\\u064F\"\n","FATHA = \"\\u064E\"\n","KASRATAN = \"\\u064D\"\n","DAMMATAN = \"\\u064C\"\n","FATHATAN = \"\\u064B\"\n","SUKUN = \"\\u0652\"\n","SHADDA = \"\\u0651\"\n","DAMMA_SHADDA = DAMMA + SHADDA\n","SHADDA_DAMMA = SHADDA + DAMMA\n","FATHA_SHADDA = FATHA + SHADDA\n","SHADDA_FATHA = SHADDA + FATHA\n","KASRA_SHADDA = KASRA + SHADDA\n","SHADDA_KASRA = SHADDA + KASRA\n","DAMMATAN_SHADDA = DAMMATAN + SHADDA\n","SHADDA_DAMMATAN = SHADDA + DAMMATAN\n","FATHATAN_SHADDA = FATHATAN + SHADDA\n","SHADDA_FATHATAN = SHADDA + FATHATAN\n","KASRATAN_SHADDA = KASRATAN + SHADDA\n","SHADDA_KASRATAN = SHADDA + KASRATAN\n","EMPTY = \"_\"\n","PAD_LABLE = \"pad\"\n","DIACRITICS = [\n","    KASRA,\n","    DAMMA,\n","    FATHA,\n","    KASRATAN,\n","    DAMMATAN,\n","    FATHATAN,\n","    SUKUN,\n","    SHADDA,\n","    DAMMA_SHADDA,\n","    SHADDA_DAMMA,\n","    FATHA_SHADDA,\n","    SHADDA_FATHA,\n","    KASRA_SHADDA,\n","    SHADDA_KASRA,\n","    DAMMATAN_SHADDA,\n","    SHADDA_DAMMATAN,\n","    FATHATAN_SHADDA,\n","    SHADDA_FATHATAN,\n","    KASRATAN_SHADDA,\n","    SHADDA_KASRATAN,\n","    EMPTY,\n","    PAD_LABLE,\n","]\n","\n","DIACRITICS_MAP = [\n","    FATHA,\n","    FATHATAN,\n","    DAMMA,\n","    DAMMATAN,\n","    KASRA,\n","    KASRATAN,\n","    SUKUN,\n","    SHADDA,\n","    SHADDA_FATHA,\n","    SHADDA_FATHATAN,\n","    SHADDA_DAMMA,\n","    SHADDA_DAMMATAN,\n","    SHADDA_KASRA,\n","    SHADDA_KASRATAN,\n","    EMPTY,\n","    PAD_LABLE,\n","]"]},{"cell_type":"code","execution_count":85,"metadata":{},"outputs":[],"source":["# This function is responsible for mapping diacritics to their corresponding strings\n","def diacritic_to_str(diacritic):\n","    if diacritic == SHADDA:\n","        diacritic = \"SHADDA\"\n","    elif diacritic == KASRA:\n","        diacritic = \"KASRA\"\n","    elif diacritic == DAMMA:\n","        diacritic = \"DAMMA\"\n","    elif diacritic == FATHA:\n","        diacritic = \"FATHA\"\n","    elif diacritic == KASRATAN:\n","        diacritic = \"KASRATAN\"\n","    elif diacritic == DAMMATAN:\n","        diacritic = \"DAMMATAN\"\n","    elif diacritic == FATHATAN:\n","        diacritic = \"FATHATAN\"\n","    elif diacritic == SUKUN:\n","        diacritic = \"SUKUN\"\n","    elif diacritic == DAMMA_SHADDA or diacritic == SHADDA_DAMMA:\n","        diacritic = \"SHADDA_DAMMA\"\n","    elif diacritic == FATHA_SHADDA or diacritic == SHADDA_FATHA:\n","        diacritic = \"SHADDA_FATHA\"\n","    elif diacritic == KASRA_SHADDA or diacritic == SHADDA_KASRA:\n","        diacritic = \"SHADDA_KASRA\"\n","    elif diacritic == DAMMATAN_SHADDA or diacritic == SHADDA_DAMMATAN:\n","        diacritic = \"SHADDA_DAMMATAN\"\n","    elif diacritic == FATHATAN_SHADDA or diacritic == SHADDA_FATHATAN:\n","        diacritic = \"SHADDA_FATHATAN\"\n","    elif diacritic == KASRATAN_SHADDA or diacritic == SHADDA_KASRATAN:\n","        diacritic = \"SHADDA_KASRATAN\"\n","    elif diacritic==\"pad\":\n","      diacritic = \"pad\"\n","    else:  # EMPTY\n","        diacritic = \"_\"\n","    return diacritic"]},{"cell_type":"code","execution_count":86,"metadata":{},"outputs":[],"source":["# filter data takes a list of strings and removes unwanted patterns\n","def filter_data(data: str) -> str:\n","    # data = re.sub(r\"\\( \\d+ (/ \\d+)? \\)\", \"\", data)\n","    # remove all numbers\n","    data = re.sub(r\"\\d+\", \"\", data)\n","    # regex to remove all special characters\n","    data = re.sub(r\"[][//,;\\?؟()$:\\-{}_*؛:«»`–\\\"~!']\", \"\", data)\n","    # remove all english letters\n","    data = re.sub(r\"[a-zA-Z]\", \"\", data)\n","    # Substituting multiple spaces with single space\n","    data = re.sub(r\"([^\\S\\n])+\", \" \", data, flags=re.I)\n","    return data"]},{"cell_type":"code","execution_count":87,"metadata":{},"outputs":[],"source":["def get_data_chars(data_set):\n","    original_labels = [\"\" for _ in range(len(data_set))]\n","    sentences = [\"\" for _ in range(len(data_set))]\n","    for i in range(len(data_set)):\n","        line = data_set[i]\n","        line_without_diacritics = []\n","        line_labels = []\n","        for word in line.split():\n","            if word == \"\":\n","                continue\n","            j = 0\n","            while j < len(word):\n","                if (\n","                    j <= len(word) - 2\n","                    and word[j] in DIACRITICS\n","                    and word[j + 1] in DIACRITICS\n","                ):\n","                    line_labels.pop()\n","                    line_labels.append(\n","                        diacritic_to_str(word[j] + word[j + 1])\n","                    )  # lable of the word\n","                    j += 1\n","                else:\n","                    if word[j] in DIACRITICS:\n","                        # pop\n","                        line_labels.pop()\n","                        line_labels.append(diacritic_to_str(word[j]))  # lable of the word\n","                    else:\n","                        line_labels.append(diacritic_to_str(word[j]))  # lable of the word\n","                        line_without_diacritics.append(word[j])\n","\n","                j += 1\n","\n","        sentences[i] = \" \".join(line_without_diacritics)\n","        original_labels[i] = \" \".join(line_labels)\n","    sentences = list(filter(None, sentences))\n","    original_labels = list(filter(None, original_labels))\n","\n","    return sentences, original_labels"]},{"cell_type":"code","execution_count":88,"metadata":{},"outputs":[],"source":["def get_vocab(vocab_path, tags_path):\n","    vocab = {}\n","    with open(vocab_path) as f:\n","        for i, l in enumerate(f.read().splitlines()):\n","            vocab[l] = i  # to avoid the 0\n","        # loading tags (we require this to map tags to their indices)\n","    vocab['<PAD>'] = len(vocab) # 35180\n","    tag_map = {}\n","    with open(tags_path) as f:\n","        for i, t in enumerate(f.read().splitlines()):\n","            tag_map[t] = i\n","\n","    return vocab, tag_map\n","\n","def get_params(vocab, tag_map, sentences_file, labels_file):\n","    sentences = []\n","    labels = []\n","\n","    for sentence in sentences_file:\n","        # replace each token by its index if it is in vocab\n","        # else use index of UNK_WORD\n","        s = [vocab[token] if token in vocab\n","                else vocab['UNK']\n","                for token in sentence.split(' ')]\n","        sentences.append(s)\n","\n","    for sentence in labels_file:\n","        # replace each label by its index\n","        s = sentence.split(' ')\n","        # remove empty strings\n","        s = list(filter(None, s))\n","        l = [tag_map[label] for label in s] # I added plus 1 here\n","        labels.append(l)\n","    return sentences, labels, len(sentences)\n"]},{"cell_type":"code","execution_count":89,"metadata":{},"outputs":[],"source":["# filter the data\n","test_set = filter_data(test_set)\n","# get space indecies\n","space_indecies = [m.start() for m in re.finditer(\" \", test_set)]\n","# split the data into lines\n","test_set = re.split(r\"[.،]\", test_set)\n","# remove empty lines\n","test_set = list(filter(None, test_set))"]},{"cell_type":"code","execution_count":90,"metadata":{},"outputs":[],"source":["test_chars, test_labels_chars = get_data_chars(test_set)"]},{"cell_type":"markdown","metadata":{"id":"alY6U9G-dBSM"},"source":["# Constants\n"]},{"cell_type":"code","execution_count":91,"metadata":{"executionInfo":{"elapsed":20,"status":"ok","timestamp":1704134214695,"user":{"displayName":"Mark Yasser","userId":"04355184956843458843"},"user_tz":-120},"id":"CL1Yi6fYdAyA"},"outputs":[],"source":["MODEL = \"BI_LSTM\"\n","NUM_LAYERS = 2\n","EMBEDDING_SIZE = 300"]},{"cell_type":"markdown","metadata":{"id":"_44BK5K82YwF"},"source":["# Importing and discovering the data\n"]},{"cell_type":"code","execution_count":92,"metadata":{"executionInfo":{"elapsed":7567,"status":"ok","timestamp":1704134222244,"user":{"displayName":"Mark Yasser","userId":"04355184956843458843"},"user_tz":-120},"id":"ulSik2Sv1p1G"},"outputs":[],"source":["vocab, tag_map = get_vocab('./Dataset/unique_chars.txt', './Dataset/unique_labels.txt')\n","test_sentences1, test_labels1, test_size1 = get_params(vocab, tag_map, test_chars, test_labels_chars)"]},{"cell_type":"markdown","metadata":{"id":"wt3e4nxjFT3O"},"source":["# DiacritiserDataset\n","\n","The class that impelements the dataset for Diacritization\n"]},{"cell_type":"code","execution_count":93,"metadata":{"executionInfo":{"elapsed":7,"status":"ok","timestamp":1704134224152,"user":{"displayName":"Mark Yasser","userId":"04355184956843458843"},"user_tz":-120},"id":"29iM0u4-4YOV"},"outputs":[],"source":["class DiacritiserDataset(torch.utils.data.Dataset):\n","\n","  def __init__(self, x, y, pad):\n","    self.x = nn.utils.rnn.pad_sequence([torch.tensor(i) for i in x], padding_value=pad,batch_first = True)\n","    self.y = nn.utils.rnn.pad_sequence([torch.tensor(i) for i in y], padding_value=tag_map[\"pad\"],batch_first = True)\n","    print('The max length of the sentences is', self.x.shape[1])\n","    print('The max length of the labels is', self.y.shape[1])\n","  def __len__(self):\n","    return len(self.x)\n","\n","  def __getitem__(self, idx):\n","    return self.x[idx], self.y[idx]"]},{"cell_type":"markdown","metadata":{"id":"CQB6O7I7FbUh"},"source":["# Classifiers\n","\n","The class that implementss the pytorch model for arabic diacritic classification\n"]},{"cell_type":"code","execution_count":94,"metadata":{"executionInfo":{"elapsed":5,"status":"ok","timestamp":1704134224152,"user":{"displayName":"Mark Yasser","userId":"04355184956843458843"},"user_tz":-120},"id":"xHeJcz1JuhYa"},"outputs":[],"source":["class ArabicDiacriticsClassifier(nn.Module):\n","  def __init__(self, vocab_size = 128137, num_layers = 3, embedding_dim = 512, hidden_size=256, n_classes=len(tag_map)):\n","    super(ArabicDiacriticsClassifier, self).__init__()\n","    # (1) Create the embedding layer\n","    self.embedding = nn.Embedding(vocab_size, embedding_dim)\n","    # import gensim.downloader as api\n","    # self.embedding.weight.data.copy_(torch.from_numpy(api.load('word2vec-google-news-300').vectors[:1000])) WORD2VEc\n","    # import fasttext.util\n","    # self.embedding.weight.data.copy_(torch.from_numpy(fasttext.util.download_model('en', if_exists='ignore').get_input_matrix()[:1000]))\n","\n","    # (2) Create an LSTM layer with hidden size = hidden_size and batch_first = True\n","    self.num_layers = num_layers\n","    self.hidden_size = hidden_size\n","\n","\n","    self.lstm = nn.LSTM(embedding_dim, hidden_size,num_layers, batch_first=True, bidirectional=True)\n","    self.linear = nn.Linear(2 * hidden_size, n_classes)\n","\n","    # (3) Create a linear layer with number of neorons = n_classes\n","    # self.linear = nn.Linear(hidden_size, n_classes)\n","\n","\n","  def forward(self, sentences):\n","    embeddings = self.embedding(sentences)\n","\n","    # BIDIRECTIONAL\n","    # Initialize hidden states for bidirectional LSTM\n","    # h0 = torch.zeros(self.num_layers*2, embeddings.size(0), self.hidden_size).to(sentences.device)\n","    # c0 = torch.zeros(self.num_layers*2, embeddings.size(0), self.hidden_size).to(sentences.device)\n","    # lstm_out, (a, b) = self.lstm(embeddings, (h0, c0))\n","\n","    # LSTM\n","    lstm1_out, (a,b) = self.lstm(embeddings)\n","    final_output = self.linear(lstm1_out)\n","    # final_output = self.linear(lstm_out[:, -1, :])\n","    return final_output"]},{"cell_type":"code","execution_count":95,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":702,"status":"ok","timestamp":1704134224849,"user":{"displayName":"Mark Yasser","userId":"04355184956843458843"},"user_tz":-120},"id":"lJJJF-qQA_wk","outputId":"1bfbf6b0-f9ff-446d-cdf9-58a726f86976"},"outputs":[{"name":"stdout","output_type":"stream","text":["ArabicDiacriticsClassifier(\n","  (embedding): Embedding(128137, 512)\n","  (lstm): LSTM(512, 256, num_layers=3, batch_first=True, bidirectional=True)\n","  (linear): Linear(in_features=512, out_features=16, bias=True)\n",")\n"]}],"source":["model = ArabicDiacriticsClassifier()\n","print(model)"]},{"cell_type":"code","execution_count":96,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":847,"status":"ok","timestamp":1704134282600,"user":{"displayName":"Mark Yasser","userId":"04355184956843458843"},"user_tz":-120},"id":"3BI7_ANkLf7G","outputId":"540db405-f874-4e1c-ab12-1c1c5f9007b7"},"outputs":[{"name":"stdout","output_type":"stream","text":["The max length of the sentences is 20\n","The max length of the labels is 20\n"]}],"source":["test_dataset = DiacritiserDataset(test_sentences1, test_labels1, vocab['<PAD>'])"]},{"cell_type":"code","execution_count":97,"metadata":{"executionInfo":{"elapsed":2,"status":"ok","timestamp":1704134282600,"user":{"displayName":"Mark Yasser","userId":"04355184956843458843"},"user_tz":-120},"id":"b2Lj2SneWZUj"},"outputs":[],"source":["def load_model(model, model_name):\n","  model.load_state_dict(torch.load(f'./Models/{model_name}'))\n","  return model"]},{"cell_type":"code","execution_count":98,"metadata":{"executionInfo":{"elapsed":622,"status":"ok","timestamp":1704134283220,"user":{"displayName":"Mark Yasser","userId":"04355184956843458843"},"user_tz":-120},"id":"NtFqGvvgZVv-"},"outputs":[],"source":["model = load_model(model, model_name)"]},{"cell_type":"markdown","metadata":{"id":"TWJNO6mUXPRI"},"source":["# Evaluation\n"]},{"cell_type":"code","execution_count":99,"metadata":{"executionInfo":{"elapsed":2,"status":"ok","timestamp":1704134283220,"user":{"displayName":"Mark Yasser","userId":"04355184956843458843"},"user_tz":-120},"id":"Gz5mxUAJM1xS"},"outputs":[],"source":["diacritic_results = []\n","gold_results = []\n","test_input_list = []\n","\n","def evaluate(model, test_dataset, batch_size=64):\n","  # (1) create the test data loader\n","  test_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size)\n","\n","  # GPU Configuration\n","  use_cuda = torch.cuda.is_available()\n","  device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n","  if use_cuda:\n","    model = model.cuda()\n","\n","  # (2) disable gradients\n","  with torch.no_grad():\n","\n","    for test_input, test_label in tqdm(test_dataloader):\n","      # (3) move the test input to the device\n","      test_label = test_label.to(device)\n","\n","      # (4) move the test label to the device\n","      test_input = test_input.to(device)\n","\n","      # (5) do the forward pass\n","      output = model(test_input)\n","      prediction = output.argmax(2)\n","\n","\n","      diacritic_results.extend(np.array(prediction.cpu().data).flatten())\n","      gold_results.extend(np.array(test_label.cpu().data).flatten())\n","      test_input_list.extend(np.array(test_input.cpu().data).flatten())"]},{"cell_type":"markdown","metadata":{"id":"DMsZQ4ITWAEl"},"source":["### Evaluatio and Save\n"]},{"cell_type":"code","execution_count":100,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["100%|██████████| 1/1 [00:00<00:00, 58.39it/s]\n"]}],"source":["evaluate(model, test_dataset)"]},{"cell_type":"code","execution_count":101,"metadata":{"executionInfo":{"elapsed":2,"status":"ok","timestamp":1704134283220,"user":{"displayName":"Mark Yasser","userId":"04355184956843458843"},"user_tz":-120},"id":"bHY019YvKejN"},"outputs":[],"source":["def DER():\n","  der = 0\n","  total_size = 0\n","  for i in range(len(diacritic_results)):\n","    if test_input_list[i] != vocab['<PAD>']: # Do not include padding in DER calculations\n","      if diacritic_results[i] != gold_results[i] : # Miss Classification\n","        der += 1\n","      total_size += 1\n","  der /= total_size\n","  der *= 100\n","  print(\"DER = \",der,\"%\")\n","  print(\"Accuracy = \",100 - der,\"%\")"]},{"cell_type":"code","execution_count":102,"metadata":{"executionInfo":{"elapsed":422,"status":"ok","timestamp":1704134284234,"user":{"displayName":"Mark Yasser","userId":"04355184956843458843"},"user_tz":-120},"id":"LLtlWNm6YVG2"},"outputs":[],"source":["filtered_diacritic_results = [] # diacrtic results without paddings\n","filtered_inputs = [] # inputs without paddings\n","LIST_OF_DIACRITICS = [\n","    \"FATHA\",\n","    \"FATHATAN\",\n","    \"DAMMA\",\n","    \"DAMMATAN\",\n","    \"KASRA\",\n","    \"KASRATAN\",\n","    \"SUKUN\",\n","    \"SHADDA\",\n","    \"SHADDA_FATHA\",\n","    \"SHADDA_FATHATAN\",\n","    \"SHADDA_DAMMA\",\n","    \"SHADDA_DAMMATAN\",\n","    \"SHADDA_KASRA\",\n","    \"SHADDA_KASRATAN\",\n","    \"_\"\n","]\n","LIST_OF_ARABIC_LETTERS = list(vocab.keys())\n","\n","\n","\n","for i in range(len(diacritic_results)):\n","  if test_input_list[i] != vocab['<PAD>']:\n","    filtered_diacritic_results.append(diacritic_results[i])\n","    filtered_inputs.append(test_input_list[i])\n","\n","index = len(filtered_diacritic_results)\n","\n","inputs = [LIST_OF_ARABIC_LETTERS[filtered_inputs[i]] for i in range(index)]\n","model_prediction = [LIST_OF_DIACRITICS[filtered_diacritic_results[i]] for i in range(index)]\n"]},{"cell_type":"code","execution_count":103,"metadata":{"executionInfo":{"elapsed":3,"status":"ok","timestamp":1704134284234,"user":{"displayName":"Mark Yasser","userId":"04355184956843458843"},"user_tz":-120},"id":"vzd36snRjME9"},"outputs":[],"source":["data_length = len(filtered_diacritic_results)\n","df = pd.DataFrame(\n","    {\n","    'ID': range(len(filtered_diacritic_results[0: data_length])),\n","    'label': filtered_diacritic_results[0: data_length],\n","    'input':  inputs,\n","    'diactric':  model_prediction,\n","    })\n","\n","df.to_csv(f'./Results/result_{model_name}.csv', index=False)"]},{"cell_type":"code","execution_count":105,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["قَالَ الزَّرْكَشِيُّ رَضِيَ اللَّهُ عَنْهُ\n"]}],"source":["output = \"\"\n","output_len = 0\n","j = 0\n","for i in range(len(diacritic_results)):\n","    if test_input_list[i] != vocab['<PAD>']:\n","        output += LIST_OF_ARABIC_LETTERS[test_input_list[i]]\n","        diacritic =  DIACRITICS_MAP[diacritic_results[i]] if DIACRITICS_MAP[diacritic_results[i]] != \"_\" else \"\"\n","        output += diacritic\n","        output_len += 2\n","        if j < len(space_indecies) and output_len // 2 == space_indecies[j]:\n","            j += 1\n","            output += \" \"\n","            output_len += 2 \n","print(output)"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.5"}},"nbformat":4,"nbformat_minor":0}
