{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read data from train.txt and filter it from unwanted patterns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from keras.utils import to_categorical\n",
    "from Embeddings import Word2Vec\n",
    "from Preprocessing import utils, character_encoding\n",
    "from Models import rnn\n",
    "# from CharacterEmbeddings import TF_IDF, WordToVec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training_set قَوْلُهُ : ( أَوْ قَطَعَ الْأَوَّلُ يَدَهُ إلَخْ ) قَالَ الزَّرْكَشِيُّ( 14 / 123 )\n",
      "ابْنُ عَرَفَةَ : قَوْلُهُ : بِلَفْظٍ يَقْتَضِيه كَإِنْكَارِ غَيْرِ حَدِيثٍ بِالْإِسْلَامِ وُجُوبَ مَا عُلِمَ وُجُوبُهُ مِنْ الدِّينِ ضَرُورَةً ( كَإِلْقَاءِ مُصْحَفٍ بِقَذَرٍ وَشَدِّ زُنَّارٍ ) ابْنُ عَرَفَةَ : قَوْلُ ابْنِ شَاسٍ : أَوْ بِفِعْلٍ يَتَضَمَّنُهُ هُوَ كَلُبْسِ الزُّنَّارِ وَإِلْقَاءِ الْمُصْحَفِ فِي صَرِيحِ النَّجَاسَةِ وَالسُّجُودِ لِلصَّنَمِ وَنَحْوِ ذَلِكَ ( وَسِحْرٍ ) مُحَمَّدٌ : قَوْلُ مَالِكٍ و\n",
      "filtered_training_set قَوْلُهُ أَوْ قَطَعَ الْأَوَّلُ يَدَهُ إلَخْ قَالَ الزَّرْكَشِيُّ \n",
      "ابْنُ عَرَفَةَ قَوْلُهُ بِلَفْظٍ يَقْتَضِيه كَإِنْكَارِ غَيْرِ حَدِيثٍ بِالْإِسْلَامِ وُجُوبَ مَا عُلِمَ وُجُوبُهُ مِنْ الدِّينِ ضَرُورَةً كَإِلْقَاءِ مُصْحَفٍ بِقَذَرٍ وَشَدِّ زُنَّارٍ ابْنُ عَرَفَةَ قَوْلُ ابْنِ شَاسٍ أَوْ بِفِعْلٍ يَتَضَمَّنُهُ هُوَ كَلُبْسِ الزُّنَّارِ وَإِلْقَاءِ الْمُصْحَفِ فِي صَرِيحِ النَّجَاسَةِ وَالسُّجُودِ لِلصَّنَمِ وَنَحْوِ ذَلِكَ وَسِحْرٍ مُحَمَّدٌ قَوْلُ مَالِكٍ وَأَصْحَابِهِ أَنَّ السَّاحِرَ كَافِ\n"
     ]
    }
   ],
   "source": [
    "training_set = utils.read_data(f\"./Dataset/train.txt\")\n",
    "print(\"training_set\", training_set[0:500])\n",
    "filtered_training_set = utils.filter_data(training_set)\n",
    "print(\"filtered_training_set\", filtered_training_set[0:500])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_set = utils.split_data_to_words(filtered_training_set)\n",
    "text_without_diacritics = []\n",
    "diacritic_list = []\n",
    "# Preparing Training Set\n",
    "text_without_diacritics, diacritic_list = character_encoding.PrepareTrainData(words_set[0:10000])\n",
    "\n",
    "# Assume this is a test set\n",
    "text_without_diacritics_test = character_encoding.PrepareTestData(words_set[10000:15000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# chracter_set = utils.char_tokenizer(filtered_training_set)\n",
    "# print(\"chracter_set\", chracter_set[0:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Extraction\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data to sentences and remove diacritics from each sentence  \n",
    "sentences = utils.split_data_to_sentences(filtered_training_set)\n",
    "list_of_sentences = character_encoding.RemoveDiacriticFromSentence(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Word2Vec model\n",
    "w2v = Word2Vec.W2V(list_of_sentences, vector_size = 50)\n",
    "w2v.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: في  - Vector: [-0.19084334  0.33991933  0.8920253   2.038869   -3.0456183   1.2804062\n",
      "  1.0829108   1.3845755  -1.8281728   0.3677872   2.6614974   0.10976142\n",
      " -1.3255929   1.7847508  -3.8392553  -1.8737712  -0.29812673  0.8716539\n",
      " -1.2923402  -0.18115766  0.19548194  0.8251692   1.6408223   1.3376774\n",
      "  0.8235652  -0.40330458  1.4933072   0.7136417  -3.1075103  -0.11752446\n",
      " -0.91862994  3.4307263  -0.38698995 -2.2465723  -0.38528436  3.9813666\n",
      "  3.671119   -1.7181312  -0.3021689  -3.2318962  -1.2201558   1.0487772\n",
      "  1.3038967  -0.9528995   2.709921    1.9757184  -1.2047977   0.89363015\n",
      "  1.336037    0.5780968 ]\n",
      "1: من  - Vector: [-2.951654    0.04070311  0.7041647  -0.22683853  0.15740763  1.086609\n",
      "  0.47842246  2.594803   -2.088505   -1.1702888   0.66025573  1.3864099\n",
      " -1.771956    0.47065756 -1.8496472   2.1469395   0.44570535  1.8917934\n",
      " -0.7909366   1.0787189  -0.75279987  2.1971307   2.3592613   0.1185462\n",
      "  0.82584196 -0.5784354   2.4660666   0.1270661   1.0497872   1.7568791\n",
      " -0.7737577   3.3410127   1.4091253  -0.50893754  0.09015384  2.4894593\n",
      "  2.4860492   0.7620801  -0.8441853   1.1713561  -1.5535039  -1.7629514\n",
      "  2.1279976  -1.2056074   3.678881    0.6252466  -3.1569815  -1.8500032\n",
      "  2.5965998   2.002053  ]\n"
     ]
    }
   ],
   "source": [
    "# Print first 2 words and their vectors\n",
    "embeddings = list(w2v.Word_embeddings().key_to_index.keys())\n",
    "for i, word in enumerate(embeddings[0:2]):\n",
    "    vector = w2v.vector(word)\n",
    "    print(f\"{i}: {word}  - Vector: {vector}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"Key 'والشفعةوإن' not present\"",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[74], line 5\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m word \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m      4\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m word_vector \u001b[38;5;241m=\u001b[39m \u001b[43mw2v\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvector\u001b[49m\u001b[43m(\u001b[49m\u001b[43mword\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m j, char \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(word):\n\u001b[0;32m      7\u001b[0m     char_vector \u001b[38;5;241m=\u001b[39m character_encoding\u001b[38;5;241m.\u001b[39mCharToOneHOt(char)\n",
      "File \u001b[1;32md:\\Engineering\\Fourth Year\\First Semester\\Natural Language Processing\\Project\\NLP-Project\\Embeddings\\Word2Vec.py:24\u001b[0m, in \u001b[0;36mW2V.vector\u001b[1;34m(self, word)\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mvector\u001b[39m(\u001b[38;5;28mself\u001b[39m, word):\n\u001b[1;32m---> 24\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mword_embeddings\u001b[49m\u001b[43m[\u001b[49m\u001b[43mword\u001b[49m\u001b[43m]\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\marky\\miniconda3\\Lib\\site-packages\\gensim\\models\\keyedvectors.py:403\u001b[0m, in \u001b[0;36mKeyedVectors.__getitem__\u001b[1;34m(self, key_or_keys)\u001b[0m\n\u001b[0;32m    389\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Get vector representation of `key_or_keys`.\u001b[39;00m\n\u001b[0;32m    390\u001b[0m \n\u001b[0;32m    391\u001b[0m \u001b[38;5;124;03mParameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    400\u001b[0m \n\u001b[0;32m    401\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    402\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key_or_keys, _KEY_TYPES):\n\u001b[1;32m--> 403\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_vector\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey_or_keys\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    405\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m vstack([\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_vector(key) \u001b[38;5;28;01mfor\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m key_or_keys])\n",
      "File \u001b[1;32mc:\\Users\\marky\\miniconda3\\Lib\\site-packages\\gensim\\models\\keyedvectors.py:446\u001b[0m, in \u001b[0;36mKeyedVectors.get_vector\u001b[1;34m(self, key, norm)\u001b[0m\n\u001b[0;32m    422\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_vector\u001b[39m(\u001b[38;5;28mself\u001b[39m, key, norm\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[0;32m    423\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Get the key's vector, as a 1D numpy array.\u001b[39;00m\n\u001b[0;32m    424\u001b[0m \n\u001b[0;32m    425\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    444\u001b[0m \n\u001b[0;32m    445\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 446\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_index\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    447\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m norm:\n\u001b[0;32m    448\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfill_norms()\n",
      "File \u001b[1;32mc:\\Users\\marky\\miniconda3\\Lib\\site-packages\\gensim\\models\\keyedvectors.py:420\u001b[0m, in \u001b[0;36mKeyedVectors.get_index\u001b[1;34m(self, key, default)\u001b[0m\n\u001b[0;32m    418\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m default\n\u001b[0;32m    419\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 420\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mKey \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m not present\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mKeyError\u001b[0m: \"Key 'والشفعةوإن' not present\""
     ]
    }
   ],
   "source": [
    "concatinated_vector = []\n",
    "for i, word in enumerate(text_without_diacritics):\n",
    "    # if word does not have corresponding embedding don't add it to the training set and remove its corresponding diacritic list\n",
    "    if word == \"\":\n",
    "        continue\n",
    "    word_vector = w2v.vector(word)\n",
    "    for j, char in enumerate(word):\n",
    "        char_vector = character_encoding.CharToOneHOt(char)\n",
    "        concatinated_vector.append(np.concatenate((word_vector, char_vector), axis=None))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building The Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input size :  36\n",
      "output size :  15\n"
     ]
    }
   ],
   "source": [
    "input_size = len(concatinated_vector[0])\n",
    "output_size = len(character_encoding.DIACRITICS)\n",
    "print(\"input size : \", input_size)\n",
    "print(\"output size : \", output_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an instance of the LSTM class\n",
    "model = rnn.RNN(input_shape=(None,input_size), output_shape = output_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the training data to the required format\n",
    "X_train = concatinated_vector # np.array([[[character_encoding.CharToOneHOt(char)]] for word in text_without_diacritics for char in word])\n",
    "\n",
    "y_train = []\n",
    "for word_diacritic in diacritic_list:\n",
    "    for diacritic in word_diacritic:\n",
    "        #print(utils.map_text_to_diacritic(diacritic))\n",
    "        index = character_encoding.DIACRITICS.index(diacritic)\n",
    "        y_train.append(to_categorical(index, num_classes=output_size))\n",
    "y_train = np.array(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "\u001b[1m1236/1236\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - accuracy: 0.5202 - loss: 1.2761\n",
      "Epoch 2/5\n",
      "\u001b[1m1236/1236\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - accuracy: 0.5175 - loss: 1.2825\n",
      "Epoch 3/5\n",
      "\u001b[1m1236/1236\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - accuracy: 0.5218 - loss: 1.2774\n",
      "Epoch 4/5\n",
      "\u001b[1m1236/1236\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - accuracy: 0.5194 - loss: 1.2801\n",
      "Epoch 5/5\n",
      "\u001b[1m1236/1236\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - accuracy: 0.5191 - loss: 1.2807\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "model.train(X_train, y_train, epochs = 5, batch_size = 64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m611/611\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step\n"
     ]
    }
   ],
   "source": [
    "# Convert the test data to the required format\n",
    "X_test = np.array([[character_encoding.CharToOneHOt(char)] for word in text_without_diacritics_test for char in word])\n",
    "\n",
    "# Predict the diacritics of the test data\n",
    "y_pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Evaluation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m611/611\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 1.0000 - loss: 1.2780\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model\n",
    "lost , accuracy = model.evaluate(X_test, y_pred)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
