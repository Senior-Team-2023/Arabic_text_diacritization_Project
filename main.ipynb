{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read data from train.txt and filter it from unwanted patterns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "Configurations : \n",
      "number_test_of_words : 20000\n",
      "number_validation_of_words : 3000\n",
      "classifier : lstm\n",
      "embedding : fasttext\n",
      "is_training : True\n",
      "embedding_vector_size : 200\n",
      "batch_size : 64\n",
      "num_epochs : 7\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from keras.utils import to_categorical\n",
    "from Embeddings import Word2Vec, FastText\n",
    "from Preprocessing import utils, character_encoding\n",
    "from Models import rnn,lstm\n",
    "import config as conf\n",
    "\n",
    "config = conf.ConfigLoader().load_config()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clean data from special characcters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training_set قَوْلُهُ : ( أَوْ قَطَعَ الْأَوَّلُ يَدَهُ إلَخْ ) قَالَ الزَّرْكَشِيُّ( 14 / 123 )\n",
      "ابْنُ عَرَفَةَ : قَوْلُهُ : بِلَفْظٍ يَقْتَضِيه كَإِنْكَارِ غَيْرِ حَدِيثٍ بِالْإِسْلَامِ وُجُوبَ مَا عُلِمَ وُجُوبُهُ مِنْ الدِّينِ ضَرُورَةً ( كَإِلْقَاءِ مُصْحَفٍ بِقَذَرٍ وَشَدِّ زُنَّارٍ ) ابْنُ عَرَفَةَ : قَوْلُ ابْنِ شَاسٍ : أَوْ بِفِعْلٍ يَتَضَمَّنُهُ هُوَ كَلُبْسِ الزُّنَّارِ وَإِلْقَاءِ الْمُصْحَفِ فِي صَرِيحِ النَّجَاسَةِ وَالسُّجُودِ لِلصَّنَمِ وَنَحْوِ ذَلِكَ ( وَسِحْرٍ ) مُحَمَّدٌ : قَوْلُ مَالِكٍ و\n",
      "\n",
      "\n",
      "filtered_training_set قَوْلُهُ أَوْ قَطَعَ الْأَوَّلُ يَدَهُ إلَخْ قَالَ الزَّرْكَشِيُّ \n",
      "ابْنُ عَرَفَةَ قَوْلُهُ بِلَفْظٍ يَقْتَضِيه كَإِنْكَارِ غَيْرِ حَدِيثٍ بِالْإِسْلَامِ وُجُوبَ مَا عُلِمَ وُجُوبُهُ مِنْ الدِّينِ ضَرُورَةً كَإِلْقَاءِ مُصْحَفٍ بِقَذَرٍ وَشَدِّ زُنَّارٍ ابْنُ عَرَفَةَ قَوْلُ ابْنِ شَاسٍ أَوْ بِفِعْلٍ يَتَضَمَّنُهُ هُوَ كَلُبْسِ الزُّنَّارِ وَإِلْقَاءِ الْمُصْحَفِ فِي صَرِيحِ النَّجَاسَةِ وَالسُّجُودِ لِلصَّنَمِ وَنَحْوِ ذَلِكَ وَسِحْرٍ مُحَمَّدٌ قَوْلُ مَالِكٍ وَأَصْحَابِهِ أَنَّ السَّاحِرَ كَافِ\n"
     ]
    }
   ],
   "source": [
    "training_set = utils.read_data(f\"./Dataset/train.txt\")\n",
    "print(\"training_set\", training_set[0:500])\n",
    "print('\\n')\n",
    "filtered_training_set = utils.filter_data(training_set)\n",
    "print(\"filtered_training_set\", filtered_training_set[0:500])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation_set ( 27 ) قَوْلُهُ : وَلَا تُكْرَهُ ضِيَافَتُهُ .\n",
      "( الْفَرْقُ الثَّالِثُ وَالثَّلَاثُونَ بَيْنَ قَاعِدَةِ تَقَدُّمِ الْحُكْمِ عَلَى سَبَبِهِ دُونَ شَرْطِهِ أَوْ شَرْطِهِ دُونَ سَبَبِهِ وَبَيْنَ قَاعِدَةِ تَقَدُّمِهِ عَلَى السَّبَبِ وَالشَّرْطِ جَمِيعًا ) وَتَحْرِيرُهُ أَنَّ الْحُكْمَ إنْ كَانَ لَهُ سَبَبٌ بِغَيْرِ شَرْطٍ فَتَقَدَّمَ عَلَيْهِ لَا يُعْتَبَرُ أَوْ كَانَ لَهُ سَبَبَانِ أَوْ أَسْبَابٌ فَتَقَدَّمَ عَلَى جَمِيعِهَا لَمْ يُعْتَبَرْ أَوْ عَلَى بَعْضِهَا دُونَ بَعْضٍ اُعْتُبِرَ بِنَاءً عَلَى\n",
      "\n",
      "\n",
      "filtered_validation_set  قَوْلُهُ وَلَا تُكْرَهُ ضِيَافَتُهُ \n",
      " الْفَرْقُ الثَّالِثُ وَالثَّلَاثُونَ بَيْنَ قَاعِدَةِ تَقَدُّمِ الْحُكْمِ عَلَى سَبَبِهِ دُونَ شَرْطِهِ أَوْ شَرْطِهِ دُونَ سَبَبِهِ وَبَيْنَ قَاعِدَةِ تَقَدُّمِهِ عَلَى السَّبَبِ وَالشَّرْطِ جَمِيعًا وَتَحْرِيرُهُ أَنَّ الْحُكْمَ إنْ كَانَ لَهُ سَبَبٌ بِغَيْرِ شَرْطٍ فَتَقَدَّمَ عَلَيْهِ لَا يُعْتَبَرُ أَوْ كَانَ لَهُ سَبَبَانِ أَوْ أَسْبَابٌ فَتَقَدَّمَ عَلَى جَمِيعِهَا لَمْ يُعْتَبَرْ أَوْ عَلَى بَعْضِهَا دُونَ بَعْضٍ اُعْتُبِرَ بِنَاءً عَلَى سَبَبِ الْخ\n"
     ]
    }
   ],
   "source": [
    "validation_set = utils.read_data(f\"./Dataset/val.txt\")\n",
    "print(\"validation_set\", validation_set[0:500])\n",
    "print('\\n')\n",
    "filtered_validation_set = utils.filter_data(validation_set)\n",
    "print(\"filtered_validation_set\", filtered_validation_set[0:500])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Splite Training data and Validation data into words then separate diacritics from each word\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparing Training Set\n",
    "words_set_train = utils.split_data_to_words(filtered_training_set)\n",
    "num_of_words_train = config['number_test_of_words']\n",
    "text_without_diacritics, diacritic_list = character_encoding.PrepareData(words_set_train[0:num_of_words_train])\n",
    "\n",
    "\n",
    "# Assume this is a validation set\n",
    "words_set_val = utils.split_data_to_words(filtered_validation_set)\n",
    "num_of_words_val = config['number_validation_of_words']\n",
    "text_without_diacritics_validation, diacritic_list_validation = character_encoding.PrepareData(words_set_val[0:num_of_words_val])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Extraction\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split training data to sentences and remove diacritics from each sentence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = utils.split_data_to_sentences(filtered_training_set)\n",
    "list_of_sentences = character_encoding.RemoveDiacriticFromSentence(sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Choose embedding model from `config.json file`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose embedding model from config file\n",
    "if config[\"embedding\"] == \"word2vec\":\n",
    "    file_path = './Embeddings/word2vec_model.bin'\n",
    "    embedding_model = Word2Vec.W2V(list_of_sentences, vector_size = config[\"embedding_vector_size\"])\n",
    "\n",
    "elif config[\"embedding\"] == \"fasttext\":\n",
    "    file_path = './Embeddings/fasttext_model.bin'\n",
    "    embedding_model = FastText.FastTextEmbedding(list_of_sentences, vector_size = config[\"embedding_vector_size\"])\n",
    "\n",
    "else:\n",
    "    raise Exception(\"Invalid embedding type\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training the word embedding model or load it (if already saved)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FastText model saved to:  ./Embeddings/fasttext_model.bin\n"
     ]
    }
   ],
   "source": [
    "\n",
    "is_training = config[\"is_training\"]    # Change this to False if you want to load the model and not train it again\n",
    "\n",
    "if embedding_model.is_model_saved(file_path) and is_training == False:\n",
    "    embedding_model.load_model(file_path)\n",
    "else:\n",
    "    embedding_model.train()\n",
    "    embedding_model.save_model(file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Concatinate **Training** Word embeddings + Characted Embedding\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "concatinated_vector_train, diacritic_list = utils.concatinate_word_char_embeddings(text_without_diacritics, diacritic_list, embedding_model = embedding_model)\n",
    "\n",
    "# calculate total character diacritic list for the assert\n",
    "count_train = 0\n",
    "for d in diacritic_list:\n",
    "    count_train += len(d)  \n",
    "assert (len(concatinated_vector_train) == count_train), f\"Error : Train Set Len ({len(concatinated_vector_train)}) != Len diacritic ({count_train}) list have different sizes, \"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Concatinate **Validation** Word embeddings + Characted Embedding\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "concatinated_vector_validation, diacritic_list_validation = utils.concatinate_word_char_embeddings(text_without_diacritics_validation, diacritic_list_validation, embedding_model = embedding_model)\n",
    "\n",
    "count_validation = 0\n",
    "for d in diacritic_list_validation:\n",
    "    count_validation += len(d) \n",
    "\n",
    "assert (len(concatinated_vector_validation) == count_validation), f\"Error : validation Set Len ({len(concatinated_vector_validation)}) != Len diacritic ({count_validation}) list have different sizes, \""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building The Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input size :  236\n",
      "output size :  16\n"
     ]
    }
   ],
   "source": [
    "input_size = len(concatinated_vector_train[0])\n",
    "output_size = len(character_encoding.DIACRITICS)\n",
    "print(\"input size : \", input_size)\n",
    "print(\"output size : \", output_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Choose Classification model from `config.json file`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\marky\\miniconda3\\Lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:205: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    }
   ],
   "source": [
    "# Create an instance of the LSTM class\n",
    "if config[\"classifier\"] == \"lstm\":\n",
    "    model = lstm.LSTM_Model(input_shape=(None, 1), output_shape = output_size)\n",
    "\n",
    "elif config[\"classifier\"] == \"rnn\":\n",
    "    model = rnn.RNN(input_shape=(None, 1), output_shape = output_size)\n",
    "    \n",
    "else:\n",
    "    raise Exception(\"Invalid model type\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preparing Training data to be passed into the `model.train()`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train size :  (78956, 236)\n",
      "y_train size :  (78956, 16)\n"
     ]
    }
   ],
   "source": [
    "# Convert the training data to the required format\n",
    "X_train = concatinated_vector_train # np.array([[[character_encoding.CharToOneHOt(char)]] for word in text_without_diacritics for char in word])\n",
    "\n",
    "y_train = []\n",
    "for word_diacritic in diacritic_list:\n",
    "    for diacritic in word_diacritic:\n",
    "        #print(utils.map_text_to_diacritic(diacritic))\n",
    "        index = character_encoding.DIACRITICS.index(diacritic)\n",
    "        y_train.append(to_categorical(index, num_classes=output_size))\n",
    "y_train = np.array(y_train)\n",
    "X_train = np.array(X_train)\n",
    "print(\"X_train size : \", X_train.shape)\n",
    "print(\"y_train size : \", y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/7\n",
      "\u001b[1m1234/1234\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m100s\u001b[0m 76ms/step - accuracy: 0.3440 - loss: 1.8620\n",
      "Epoch 2/7\n",
      "\u001b[1m1234/1234\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 68ms/step - accuracy: 0.3818 - loss: 1.6758\n",
      "Epoch 3/7\n",
      "\u001b[1m1234/1234\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 68ms/step - accuracy: 0.3992 - loss: 1.5976\n",
      "Epoch 4/7\n",
      "\u001b[1m1234/1234\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m84s\u001b[0m 68ms/step - accuracy: 0.3967 - loss: 1.5677\n",
      "Epoch 5/7\n",
      "\u001b[1m1234/1234\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 67ms/step - accuracy: 0.4020 - loss: 1.5433\n",
      "Epoch 6/7\n",
      "\u001b[1m1234/1234\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m84s\u001b[0m 68ms/step - accuracy: 0.4078 - loss: 1.5310\n",
      "Epoch 7/7\n",
      "\u001b[1m1234/1234\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m85s\u001b[0m 69ms/step - accuracy: 0.4044 - loss: 1.5152\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "model.train(X_train, y_train, epochs = config[\"num_epochs\"], batch_size = config[\"batch_size\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Evaluation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preparing Validation data to be passed into the `model.evaluate()`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_validation size :  (78956, 236)\n",
      "y_validation size :  (78956, 16)\n"
     ]
    }
   ],
   "source": [
    "# Convert the validation data to the required format\n",
    "X_validation = concatinated_vector_validation # np.array([[[character_encoding.CharToOneHOt(char)]] for word in text_without_diacritics_validation for char in word])\n",
    "y_validation = []\n",
    "for word_diacritic in diacritic_list_validation:\n",
    "    for diacritic in word_diacritic:\n",
    "        index = character_encoding.DIACRITICS.index(diacritic)\n",
    "        y_validation.append(to_categorical(index, num_classes=output_size))\n",
    "\n",
    "y_validation = np.array(y_train)\n",
    "X_validation = np.array(X_train)\n",
    "print(\"X_validation size : \", X_validation.shape)\n",
    "print(\"y_validation size : \", y_validation.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m2468/2468\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m53s\u001b[0m 21ms/step - accuracy: 0.4110 - loss: 1.5017\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model\n",
    "lost , accuracy = model.evaluate(X_validation, y_validation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "testing on a given sentence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_test len :  85\n",
      "X_test size :  (70, 236)\n"
     ]
    }
   ],
   "source": [
    "# Predict the diacritics of the validation data\n",
    "sentence_test = \"يأخذون بعض ما تيسر لهم أخذه فيختلسونه و يجعلونه تحتهم حتى إذا رجعوا إلى بيوتهم أخرجوه\"\n",
    "sentence_words = utils.split_data_to_words(sentence_test)\n",
    "\n",
    "x_test, _ = utils.concatinate_word_char_embeddings(sentence_words, diacritic_list, embedding_model = embedding_model)\n",
    "# print(\"x_test : \", x_test)\n",
    "x_test = np.array(x_test)\n",
    "print(\"x_test len : \", len(sentence_test))\n",
    "print(\"X_test size : \", x_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 90ms/step\n"
     ]
    }
   ],
   "source": [
    "y_pred = model.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X :  يأخذون بعض ما تيسر لهم أخذه فيختلسونه و يجعلونه تحتهم حتى إذا رجعوا إلى بيوتهم أخرجوه\n",
      "Len X  :  85\n",
      "Len Y predicted  :  70\n",
      "FATHA\n",
      "FATHA\n",
      "FATHA\n",
      "FATHA\n",
      "FATHA\n",
      "FATHA\n",
      "FATHA\n",
      "FATHA\n",
      "FATHA\n",
      "FATHA\n",
      "FATHA\n",
      "FATHA\n",
      "FATHA\n",
      "FATHA\n",
      "FATHA\n",
      "SUKUN\n",
      "SUKUN\n",
      "SUKUN\n",
      "FATHA\n",
      "FATHA\n",
      "FATHA\n",
      "FATHA\n",
      "FATHA\n",
      "FATHA\n",
      "FATHA\n",
      "FATHA\n",
      "FATHA\n",
      "FATHA\n",
      "FATHA\n",
      "FATHA\n",
      "FATHA\n",
      "FATHA\n",
      "FATHA\n",
      "FATHA\n",
      "FATHA\n",
      "FATHA\n",
      "FATHA\n",
      "FATHA\n",
      "FATHA\n",
      "KASRA\n",
      "KASRA\n",
      "KASRA\n",
      "KASRA\n",
      "KASRA\n",
      "SHADDA_FATHA\n",
      "SHADDA_FATHA\n",
      "SHADDA_FATHA\n",
      " \n",
      " \n",
      " \n",
      "FATHA\n",
      "FATHA\n",
      "FATHA\n",
      "FATHA\n",
      "FATHA\n",
      " \n",
      " \n",
      " \n",
      "KASRA\n",
      "KASRA\n",
      "KASRA\n",
      "KASRA\n",
      "KASRA\n",
      "KASRA\n",
      "FATHA\n",
      "FATHA\n",
      "FATHA\n",
      "FATHA\n",
      "FATHA\n",
      "FATHA\n"
     ]
    }
   ],
   "source": [
    "# something is wrong here\n",
    "print(\"X : \", sentence_test)\n",
    "print(\"Len X  : \", len(sentence_test))\n",
    "print(\"Len Y predicted  : \", len(y_pred))\n",
    "\n",
    "# Print the predicted diacritics\n",
    "diacritics_pred = []\n",
    "for pred in y_pred:\n",
    "    index = np.argmax(pred)\n",
    "    diac = character_encoding.DIACRITICS[index]\n",
    "    print(character_encoding.diacritic_to_str(diac))\n",
    "    diacritics_pred.append(character_encoding.diacritic_to_str(diac))\n",
    "\n",
    "# character_encoding.print_text_to_diacritic_mapping(sentence_test, diacritics_pred, True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
