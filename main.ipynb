{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read data from train.txt and filter it from unwanted patterns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configurations Read Successfully\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from keras.utils import to_categorical\n",
    "from Embeddings import Word2Vec\n",
    "from Preprocessing import utils, character_encoding\n",
    "from Models import rnn\n",
    "import config as conf\n",
    "\n",
    "config = conf.ConfigLoader().load_config()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training_set قَوْلُهُ : ( أَوْ قَطَعَ الْأَوَّلُ يَدَهُ إلَخْ ) قَالَ الزَّرْكَشِيُّ( 14 / 123 )\n",
      "ابْنُ عَرَفَةَ : قَوْلُهُ : بِلَفْظٍ يَقْتَضِيه كَإِنْكَارِ غَيْرِ حَدِيثٍ بِالْإِسْلَامِ وُجُوبَ مَا عُلِمَ وُجُوبُهُ مِنْ الدِّينِ ضَرُورَةً ( كَإِلْقَاءِ مُصْحَفٍ بِقَذَرٍ وَشَدِّ زُنَّارٍ ) ابْنُ عَرَفَةَ : قَوْلُ ابْنِ شَاسٍ : أَوْ بِفِعْلٍ يَتَضَمَّنُهُ هُوَ كَلُبْسِ الزُّنَّارِ وَإِلْقَاءِ الْمُصْحَفِ فِي صَرِيحِ النَّجَاسَةِ وَالسُّجُودِ لِلصَّنَمِ وَنَحْوِ ذَلِكَ ( وَسِحْرٍ ) مُحَمَّدٌ : قَوْلُ مَالِكٍ و\n",
      "filtered_training_set قَوْلُهُ أَوْ قَطَعَ الْأَوَّلُ يَدَهُ إلَخْ قَالَ الزَّرْكَشِيُّ \n",
      "ابْنُ عَرَفَةَ قَوْلُهُ بِلَفْظٍ يَقْتَضِيه كَإِنْكَارِ غَيْرِ حَدِيثٍ بِالْإِسْلَامِ وُجُوبَ مَا عُلِمَ وُجُوبُهُ مِنْ الدِّينِ ضَرُورَةً كَإِلْقَاءِ مُصْحَفٍ بِقَذَرٍ وَشَدِّ زُنَّارٍ ابْنُ عَرَفَةَ قَوْلُ ابْنِ شَاسٍ أَوْ بِفِعْلٍ يَتَضَمَّنُهُ هُوَ كَلُبْسِ الزُّنَّارِ وَإِلْقَاءِ الْمُصْحَفِ فِي صَرِيحِ النَّجَاسَةِ وَالسُّجُودِ لِلصَّنَمِ وَنَحْوِ ذَلِكَ وَسِحْرٍ مُحَمَّدٌ قَوْلُ مَالِكٍ وَأَصْحَابِهِ أَنَّ السَّاحِرَ كَافِ\n"
     ]
    }
   ],
   "source": [
    "training_set = utils.read_data(f\"./Dataset/train.txt\")\n",
    "print(\"training_set\", training_set[0:500])\n",
    "filtered_training_set = utils.filter_data(training_set)\n",
    "print(\"filtered_training_set\", filtered_training_set[0:500])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_set = utils.split_data_to_words(filtered_training_set)\n",
    "text_without_diacritics = []\n",
    "diacritic_list = []\n",
    "# Preparing Training Set\n",
    "text_without_diacritics, diacritic_list = character_encoding.PrepareData(words_set[0:10000])\n",
    "\n",
    "# Assume this is a test set\n",
    "text_without_diacritics_test, diacritic_list_test = character_encoding.PrepareData(words_set[10000:15000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"size of text without diacritics\", len(text_without_diacritics))\n",
    "# print(\"size of diacritic list\", len(diacritic_list))\n",
    "# print(\"size of original words\", len(words_set))\n",
    "# for i in range(len(diacritic_list)):\n",
    "#     print(i,\"- original : \",words_set[i] ,\"   - without : \",text_without_diacritics[i] , \"   - diacritic : \", character_encoding.map_text_to_diacritic(diacritic_list[i]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Extraction\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data to sentences and remove diacritics from each sentence  \n",
    "sentences = utils.split_data_to_sentences(filtered_training_set)\n",
    "list_of_sentences = character_encoding.RemoveDiacriticFromSentence(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = './Embeddings/word2vec_model.bin'\n",
    "# Create Word Embedding model\n",
    "embedding_model = Word2Vec.W2V(list_of_sentences, vector_size = config[\"embedding_vector_size\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word2Vec model saved to :  ./Embeddings/word2vec_model.bin\n"
     ]
    }
   ],
   "source": [
    "\n",
    "is_training = config[\"is_training\"]    # Change this to False if you want to load the model and not train it again\n",
    "\n",
    "if embedding_model.is_model_saved(file_path) and is_training == False:\n",
    "    embedding_model.load_model(file_path)\n",
    "else:\n",
    "    embedding_model.train()\n",
    "    embedding_model.save_model(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "concatinated_vector_train, diacritic_list = utils.concatinate_word_char_embeddings(text_without_diacritics, diacritic_list, embedding_model = embedding_model)\n",
    "concatinated_vector_test, diacritic_list_test = utils.concatinate_word_char_embeddings(text_without_diacritics_test, diacritic_list_test, embedding_model = embedding_model)\n",
    "\n",
    "\n",
    "# calculate total character diacritic list for the assert\n",
    "count_train = 0\n",
    "for d in diacritic_list:\n",
    "    count_train += len(d)\n",
    "count_test = 0\n",
    "for d in diacritic_list_test:\n",
    "    count_test += len(d)   \n",
    "assert (len(concatinated_vector_train) == count_train), f\"Error : Train Set Len ({len(concatinated_vector_train)}) != Len diacritic ({count_train}) list have different sizes, \"\n",
    "assert (len(concatinated_vector_test) == count_test), f\"Error : Test Set Len ({len(concatinated_vector_test)}) != Len diacritic ({count_test}) list have different sizes, \""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building The Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input size :  86\n",
      "output size :  15\n"
     ]
    }
   ],
   "source": [
    "input_size = len(concatinated_vector_train[0])\n",
    "output_size = len(character_encoding.DIACRITICS)\n",
    "print(\"input size : \", input_size)\n",
    "print(\"output size : \", output_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\marky\\miniconda3\\Lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:205: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    }
   ],
   "source": [
    "# Create an instance of the LSTM class\n",
    "model = rnn.RNN(input_shape=(None, 1), output_shape = output_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train size :  (39493, 86)\n",
      "y_train size :  (39493, 15)\n"
     ]
    }
   ],
   "source": [
    "# Convert the training data to the required format\n",
    "X_train = concatinated_vector_train # np.array([[[character_encoding.CharToOneHOt(char)]] for word in text_without_diacritics for char in word])\n",
    "\n",
    "y_train = []\n",
    "for word_diacritic in diacritic_list:\n",
    "    for diacritic in word_diacritic:\n",
    "        #print(utils.map_text_to_diacritic(diacritic))\n",
    "        index = character_encoding.DIACRITICS.index(diacritic)\n",
    "        y_train.append(to_categorical(index, num_classes=output_size))\n",
    "y_train = np.array(y_train)\n",
    "X_train = np.array(X_train)\n",
    "print(\"X_train size : \", X_train.shape)\n",
    "print(\"y_train size : \", y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "\u001b[1m618/618\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 34ms/step - accuracy: 0.3568 - loss: 1.8937\n",
      "Epoch 2/5\n",
      "\u001b[1m618/618\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 42ms/step - accuracy: 0.3567 - loss: 1.7723\n",
      "Epoch 3/5\n",
      "\u001b[1m618/618\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 49ms/step - accuracy: 0.4068 - loss: 1.6606\n",
      "Epoch 4/5\n",
      "\u001b[1m618/618\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 40ms/step - accuracy: 0.4911 - loss: 1.4324\n",
      "Epoch 5/5\n",
      "\u001b[1m618/618\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 42ms/step - accuracy: 0.5026 - loss: 1.3901\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "model.train(X_train, y_train, epochs = config[\"num_epochs\"], batch_size = config[\"batch_size\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_test size :  (39493, 86)\n",
      "y_test size :  (39493, 15)\n"
     ]
    }
   ],
   "source": [
    "# Convert the test data to the required format\n",
    "X_test = concatinated_vector_test # np.array([[[character_encoding.CharToOneHOt(char)]] for word in text_without_diacritics_test for char in word])\n",
    "\n",
    "y_test = []\n",
    "for word_diacritic in diacritic_list:\n",
    "    for diacritic in word_diacritic:\n",
    "        index = character_encoding.DIACRITICS.index(diacritic)\n",
    "        y_test.append(to_categorical(index, num_classes=output_size))\n",
    "\n",
    "y_test = np.array(y_train)\n",
    "X_test = np.array(X_train)\n",
    "print(\"X_test size : \", X_test.shape)\n",
    "print(\"y_test size : \", y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1235/1235\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 14ms/step\n"
     ]
    }
   ],
   "source": [
    "# Predict the diacritics of the test data\n",
    "y_pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Evaluation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1235/1235\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 14ms/step - accuracy: 0.5169 - loss: 1.3566\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model\n",
    "lost , accuracy = model.evaluate(X_test, y_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
