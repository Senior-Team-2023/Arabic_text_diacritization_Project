{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read data from train.txt and filter it from unwanted patterns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configurations : \n",
      "number_test_of_words : 10000\n",
      "number_validation_of_words : 1000\n",
      "classifier : lstm\n",
      "embedding : fasttext\n",
      "is_training : True\n",
      "word_embeddings : False\n",
      "character_embeddings : False\n",
      "embedding_vector_size : 100\n",
      "character_embedding_vector_size : 200\n",
      "batch_size : 64\n",
      "num_epochs : 7\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from keras.utils import to_categorical\n",
    "from Embeddings import Word2Vec, FastText\n",
    "from Preprocessing import utils, character_encoding\n",
    "from Models import rnn, lstm, bilstm\n",
    "import config as conf\n",
    "\n",
    "config = conf.ConfigLoader().load_config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VECTOR_SIZE = config['embedding_vector_size ']\n",
    "VECTOR_SIZE = 10\n",
    "NUM_TRAIN_LINES = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clean data from special characcters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training_set قَوْلُهُ : ( أَوْ قَطَعَ الْأَوَّلُ يَدَهُ إلَخْ ) قَالَ الزَّرْكَشِيُّ( 14 / 123 )\n",
      "ابْنُ عَرَفَةَ : قَوْلُهُ : بِلَفْظٍ يَقْتَضِيه كَإِنْكَارِ غَيْرِ حَدِيثٍ بِالْإِسْلَامِ وُجُوبَ مَا عُلِمَ وُجُوبُهُ مِنْ الدِّينِ ضَرُورَةً ( كَإِلْقَاءِ مُصْحَفٍ بِقَذَرٍ وَشَدِّ زُنَّارٍ ) ابْنُ عَرَفَةَ : قَوْلُ ابْنِ شَاسٍ : أَوْ بِفِعْلٍ يَتَضَمَّنُهُ هُوَ كَلُبْسِ الزُّنَّارِ وَإِلْقَاءِ الْمُصْحَفِ فِي صَرِيحِ النَّجَاسَةِ وَالسُّجُودِ لِلصَّنَمِ وَنَحْوِ ذَلِكَ ( وَسِحْرٍ ) مُحَمَّدٌ : قَوْلُ مَالِكٍ و\n",
      "\n",
      "\n",
      "filtered_training_set قَوْلُهُ أَوْ قَطَعَ الْأَوَّلُ يَدَهُ إلَخْ قَالَ الزَّرْكَشِيُّ \n",
      "ابْنُ عَرَفَةَ قَوْلُهُ بِلَفْظٍ يَقْتَضِيه كَإِنْكَارِ غَيْرِ حَدِيثٍ بِالْإِسْلَامِ وُجُوبَ مَا عُلِمَ وُجُوبُهُ مِنْ الدِّينِ ضَرُورَةً كَإِلْقَاءِ مُصْحَفٍ بِقَذَرٍ وَشَدِّ زُنَّارٍ ابْنُ عَرَفَةَ قَوْلُ ابْنِ شَاسٍ أَوْ بِفِعْلٍ يَتَضَمَّنُهُ هُوَ كَلُبْسِ الزُّنَّارِ وَإِلْقَاءِ الْمُصْحَفِ فِي صَرِيحِ النَّجَاسَةِ وَالسُّجُودِ لِلصَّنَمِ وَنَحْوِ ذَلِكَ وَسِحْرٍ مُحَمَّدٌ قَوْلُ مَالِكٍ وَأَصْحَابِهِ أَنَّ السَّاحِرَ كَافِ\n"
     ]
    }
   ],
   "source": [
    "training_set = utils.read_data(f\"./Dataset/train.txt\")\n",
    "print(\"training_set\", training_set[0:500])\n",
    "print('\\n')\n",
    "filtered_training_set = utils.filter_data(training_set)\n",
    "print(\"filtered_training_set\", filtered_training_set[0:500])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation_set ( 27 ) قَوْلُهُ : وَلَا تُكْرَهُ ضِيَافَتُهُ .\n",
      "( الْفَرْقُ الثَّالِثُ وَالثَّلَاثُونَ بَيْنَ قَاعِدَةِ تَقَدُّمِ الْحُكْمِ عَلَى سَبَبِهِ دُونَ شَرْطِهِ أَوْ شَرْطِهِ دُونَ سَبَبِهِ وَبَيْنَ قَاعِدَةِ تَقَدُّمِهِ عَلَى السَّبَبِ وَالشَّرْطِ جَمِيعًا ) وَتَحْرِيرُهُ أَنَّ الْحُكْمَ إنْ كَانَ لَهُ سَبَبٌ بِغَيْرِ شَرْطٍ فَتَقَدَّمَ عَلَيْهِ لَا يُعْتَبَرُ أَوْ كَانَ لَهُ سَبَبَانِ أَوْ أَسْبَابٌ فَتَقَدَّمَ عَلَى جَمِيعِهَا لَمْ يُعْتَبَرْ أَوْ عَلَى بَعْضِهَا دُونَ بَعْضٍ اُعْتُبِرَ بِنَاءً عَلَى\n",
      "\n",
      "\n",
      "filtered_validation_set  قَوْلُهُ وَلَا تُكْرَهُ ضِيَافَتُهُ \n",
      " الْفَرْقُ الثَّالِثُ وَالثَّلَاثُونَ بَيْنَ قَاعِدَةِ تَقَدُّمِ الْحُكْمِ عَلَى سَبَبِهِ دُونَ شَرْطِهِ أَوْ شَرْطِهِ دُونَ سَبَبِهِ وَبَيْنَ قَاعِدَةِ تَقَدُّمِهِ عَلَى السَّبَبِ وَالشَّرْطِ جَمِيعًا وَتَحْرِيرُهُ أَنَّ الْحُكْمَ إنْ كَانَ لَهُ سَبَبٌ بِغَيْرِ شَرْطٍ فَتَقَدَّمَ عَلَيْهِ لَا يُعْتَبَرُ أَوْ كَانَ لَهُ سَبَبَانِ أَوْ أَسْبَابٌ فَتَقَدَّمَ عَلَى جَمِيعِهَا لَمْ يُعْتَبَرْ أَوْ عَلَى بَعْضِهَا دُونَ بَعْضٍ اُعْتُبِرَ بِنَاءً عَلَى سَبَبِ الْخ\n"
     ]
    }
   ],
   "source": [
    "validation_set = utils.read_data(f\"./Dataset/val.txt\")\n",
    "print(\"validation_set\", validation_set[0:500])\n",
    "print('\\n')\n",
    "filtered_validation_set = utils.filter_data(validation_set)\n",
    "print(\"filtered_validation_set\", filtered_validation_set[0:500])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Splite Training data and Validation data into words then separate diacritics from each word\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Extraction\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split training data to sentences and remove diacritics from each sentence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "قَوْلُهُ أَوْ قَطَعَ الْأَوَّلُ يَدَهُ إلَخْ قَالَ الزَّرْكَشِيُّ \n"
     ]
    }
   ],
   "source": [
    "\n",
    "sentences = utils.split_data_to_sentences(filtered_training_set)[0:NUM_TRAIN_LINES]\n",
    "max_sentence_length = utils.get_max_len(sequences=[sentences])\n",
    "sentences_without_diacritics, sentences_diacritics = character_encoding.RemoveDiacriticFromSentence(sentences)\n",
    "print(sentences[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "testing new approach\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_model = FastText.FastTextEmbedding(sentences_without_diacritics, vector_size = VECTOR_SIZE)\n",
    "embedding_model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sentences_without_diacritics( 38 ) قوله أو قطع الأول يده إلخ قال الزركشي \n",
      "sentences_diacritics( 38 ) ['FATHA', 'SUKUN', 'DAMMA', 'DAMMA', ' ', 'FATHA', 'SUKUN', ' ', 'FATHA', 'FATHA', 'FATHA', ' ', ' ', 'SUKUN', 'FATHA', 'SHADDA_FATHA', 'DAMMA', ' ', 'FATHA', 'FATHA', 'DAMMA', ' ', ' ', 'FATHA', 'SUKUN', ' ', 'FATHA', ' ', 'FATHA', ' ', ' ', ' ', 'SHADDA_FATHA', 'SUKUN', 'FATHA', 'KASRA', 'SHADDA_DAMMA', ' ']\n",
      "original_Text قَوْلُهُ أَوْ قَطَعَ الْأَوَّلُ يَدَهُ إلَخْ قَالَ الزَّرْكَشِيُّ \n",
      "restored_text قَوْلُهُ أَوْ قَطَعَ الْأَوَّلُ يَدَهُ إلَخْ قَالَ الزَّرْكَشِيُّ \n"
     ]
    }
   ],
   "source": [
    "index = 0\n",
    "sentence_to_test = sentences_without_diacritics[index]\n",
    "diacritic_list_to_test = sentences_diacritics[index]\n",
    "\n",
    "print(\"sentences_without_diacritics(\",len(sentence_to_test),\")\", sentence_to_test)\n",
    "print(\"sentences_diacritics(\",len(diacritic_list_to_test),\")\", character_encoding.map_text_to_diacritic(diacritic_list_to_test))\n",
    "\n",
    "# words = utils.split_data_to_words(sentence_to_test)\n",
    "restored_text = character_encoding.restore_diacritics(sentence_to_test, diacritic_list_to_test)\n",
    "print(\"original_Text\", sentences[index])\n",
    "print(\"restored_text\", restored_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_spaces(text, diacritic_list):\n",
    "    text = list(text)\n",
    "    copy_diacritic_list = diacritic_list.copy()\n",
    "    for i,c in enumerate(text):\n",
    "        if c.isspace():\n",
    "            text.pop(i)\n",
    "            copy_diacritic_list.pop(i)\n",
    "    return text, copy_diacritic_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def split_data_to_words(data: str) -> list:\n",
    "    words = re.split(r\"[^\\S\\n]+\", data)\n",
    "    # # remove empty words\n",
    "    # words = [word for word in words if word]\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # input is one hot incodeing including spaces and with word embedding - WITH BATCHES\n",
    "\n",
    "# batch_concatinated_vector_train = []\n",
    "# for i,sentence in enumerate(sentences_without_diacritics):\n",
    "#     sentence_vec = []\n",
    "#     words = split_data_to_words(sentence)\n",
    "#     for j, word in enumerate(words):\n",
    "#         try:\n",
    "#             word_vec = embedding_model.vector(word)\n",
    "#         except:\n",
    "#             print(\"word not found : \\\"\", word , \"\\\"\")\n",
    "        \n",
    "#         for k,c in enumerate(word):\n",
    "#             one_hot = character_encoding.CharToOneHOt(c)\n",
    "#             v = np.concatenate((word_vec, one_hot), axis=None)\n",
    "#             sentence_vec.append(v)\n",
    "\n",
    "#         # add space between words except for the last word\n",
    "#         if j != len(words) - 1:    \n",
    "#             one_hot = character_encoding.CharToOneHOt(' ')\n",
    "#             v = np.concatenate((word_vec, one_hot), axis = None)\n",
    "#             sentence_vec.append(v)\n",
    "#     batch_concatinated_vector_train.append(sentence_vec)\n",
    "\n",
    "# padded_sequences = utils.padding(batch_concatinated_vector_train)\n",
    "\n",
    "# c = 0\n",
    "# for diacritics in sentences_diacritics:\n",
    "#     for diacritic in diacritics:\n",
    "#         c += 1\n",
    "# # print(\"vector example :\", v)\n",
    "# print(\"len concatinated :\", len(batch_concatinated_vector_train[0]))\n",
    "# print(\"len diacritics   :\", c)\n",
    "# print(\"Diff = \", len(batch_concatinated_vector_train[0]) - c)\n",
    "# print(\"Original sentence :\", sentences[0])\n",
    "# print(\"First sentence    :\", sentences_without_diacritics[0])\n",
    "# print(\"First diacritics  :\", character_encoding.map_text_to_diacritic(sentences_diacritics[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len concatinated : 38\n",
      "len diacritics   : 38\n",
      "Diff =  0\n",
      "Original sentence : قَوْلُهُ أَوْ قَطَعَ الْأَوَّلُ يَدَهُ إلَخْ قَالَ الزَّرْكَشِيُّ \n",
      "First sentence    : قوله أو قطع الأول يده إلخ قال الزركشي \n",
      "First diacritics  : ['FATHA', 'SUKUN', 'DAMMA', 'DAMMA', ' ', 'FATHA', 'SUKUN', ' ', 'FATHA', 'FATHA', 'FATHA', ' ', ' ', 'SUKUN', 'FATHA', 'SHADDA_FATHA', 'DAMMA', ' ', 'FATHA', 'FATHA', 'DAMMA', ' ', ' ', 'FATHA', 'SUKUN', ' ', 'FATHA', ' ', 'FATHA', ' ', ' ', ' ', 'SHADDA_FATHA', 'SUKUN', 'FATHA', 'KASRA', 'SHADDA_DAMMA', ' ']\n"
     ]
    }
   ],
   "source": [
    "# input is one hot incodeing including spaces and with word embedding\n",
    "concatinated_vector_train = []\n",
    "for i,sentence in enumerate(sentences_without_diacritics):\n",
    "    words = split_data_to_words(sentence)\n",
    "    for j, word in enumerate(words):\n",
    "        try:\n",
    "            word_vec = embedding_model.vector(word)\n",
    "        except:\n",
    "            print(\"word not found : \\\"\", word , \"\\\"\")\n",
    "        \n",
    "        for k,c in enumerate(word):\n",
    "            one_hot = character_encoding.CharToOneHOt(c)\n",
    "            v = np.concatenate((word_vec, one_hot), axis=None)\n",
    "            concatinated_vector_train.append(v)\n",
    "\n",
    "        # add space between words except for the last word\n",
    "        if j != len(words) - 1:    \n",
    "            one_hot = character_encoding.CharToOneHOt(' ')\n",
    "            v = np.concatenate((word_vec, one_hot), axis = None)\n",
    "            concatinated_vector_train.append(v) \n",
    "\n",
    "\n",
    "c = 0\n",
    "for diacritics in sentences_diacritics:\n",
    "    for diacritic in diacritics:\n",
    "        c += 1\n",
    "# print(\"vector example :\", v)\n",
    "print(\"len concatinated :\", len(concatinated_vector_train))\n",
    "print(\"len diacritics   :\", c)\n",
    "print(\"Diff = \", len(concatinated_vector_train) - c)\n",
    "print(\"Original sentence :\", sentences[0])\n",
    "print(\"First sentence    :\", sentences_without_diacritics[0])\n",
    "print(\"First diacritics  :\", character_encoding.map_text_to_diacritic(sentences_diacritics[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # input is one hot incodeing including spaces\n",
    "# concatinated_vector_train = []\n",
    "# for i,sentence in enumerate(sentences_without_diacritics):\n",
    "#     for j,c in enumerate(sentence):\n",
    "#         one_hot = character_encoding.CharToOneHOt(c)\n",
    "#         concatinated_vector_train.append(one_hot)\n",
    "\n",
    "# print(\"len sentence \", len(sentences_without_diacritics[-1]))\n",
    "# print(\"len diacritics \", len(sentences_diacritics))\n",
    "# print(\"sentence \", sentences_without_diacritics[-1])\n",
    "# print(\"diacritics \", sentences_diacritics[-1])\n",
    "# print(\"Original sentence :\", sentences[0])\n",
    "# print(\"First sentence    :\", sentences_without_diacritics[0])\n",
    "# print(\"First diacritics  :\", character_encoding.map_text_to_diacritic(sentences_diacritics[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # input is one hot including without spaces\n",
    "# concatinated_vector_train = []\n",
    "# train_diactrics = []\n",
    "# for i,sentence in enumerate(sentences_without_diacritics):\n",
    "#     sentence, diacritics = remove_spaces(sentence, sentences_diacritics[i])\n",
    "    \n",
    "#     for j,c in enumerate(sentence):\n",
    "#         one_hot = character_encoding.CharToOneHOt(c)\n",
    "#         train_diactrics.append(diacritics[j])\n",
    "#         concatinated_vector_train.append(one_hot)\n",
    "\n",
    "# print(\"len sentence \", len(sentence))\n",
    "# print(\"len diacritics \", len(diacritics))\n",
    "# print(\"sentence \", sentence)\n",
    "# print(\"diacritics \", diacritics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # input is one hot incodeing without spaces and with word embedding\n",
    "# concatinated_vector_train = []\n",
    "# train_diactrics = []\n",
    "# ## need to be fixed\n",
    "# for i,sentence in enumerate(sentences_without_diacritics):\n",
    "#     _, diacritics = remove_spaces(sentence, sentences_diacritics[i])\n",
    "#     words = utils.split_data_to_words(sentence)\n",
    "#     j = 0\n",
    "#     for word in words:\n",
    "#         try:\n",
    "#             word_vec = embedding_model.vector(word)\n",
    "#         except:\n",
    "#             # print(\"word not found in embedding model\", word)\n",
    "#             word_vec = np.ones(150)\n",
    "#         for c in word:\n",
    "#             one_hot = character_encoding.CharToOneHOt(c)\n",
    "#             v = np.concatenate((one_hot, word_vec), axis=None)\n",
    "#             concatinated_vector_train.append(v)\n",
    "#             train_diactrics.append(diacritics[j])\n",
    "#             j += 1\n",
    "\n",
    "# print(\"len sentence \", len(concatinated_vector_train))\n",
    "# print(\"len diacritics \", len(train_diactrics))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building The Model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preparing Training data to be passed into the `model.train()`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train size :  (38, 48)\n",
      "y_train size :  (38, 15)\n"
     ]
    }
   ],
   "source": [
    "# Convert the training data to the required format\n",
    "output_size = len(character_encoding.DIACRITICS)\n",
    "X_train = concatinated_vector_train # Original\n",
    "# X_train = padded_sequences\n",
    "\n",
    "y_train = []\n",
    "for diacritics in sentences_diacritics:\n",
    "    for diacritic in diacritics:\n",
    "        index = character_encoding.DIACRITICS.index(diacritic)\n",
    "        y_train.append(to_categorical(index, num_classes = output_size))\n",
    "\n",
    "# for diacritic in sentences_diacritics:\n",
    "#     index = character_encoding.DIACRITICS.index(diacritic)\n",
    "#     y_train.append(to_categorical(index, num_classes=output_size))\n",
    "        \n",
    "y_train = np.array(y_train)\n",
    "X_train = np.array(X_train)\n",
    "print(\"X_train size : \", X_train.shape)\n",
    "print(\"y_train size : \", y_train.shape)\n",
    "\n",
    "# assert X_train.shape[0] == y_train.shape[0] # Original\n",
    "assert X_train.shape[0] == y_train.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input size   :  (38, 48)\n",
      "output size  :  15\n"
     ]
    }
   ],
   "source": [
    "print(\"input size   : \", X_train.shape)\n",
    "print(\"output size  : \", output_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Choose Classification model from `config.json file`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = rnn.RNN_Model(input_shape=(X_train.shape[1], X_train.shape[2]), output_shape = output_size, num_lstm_layers=2, hidden_size=128) # batches\n",
    "model = rnn.RNN_Model(input_shape=(X_train.shape[1], 1), output_shape = output_size, num_lstm_layers=3, hidden_size=128) # original\n",
    "# model = rnn.RNN_Model(input_shape=X_train.shape[1], output_shape = output_size,hidden_size=X_train.shape[0]) # pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 127ms/step - accuracy: 0.1469 - loss: 2.4650\n",
      "Epoch 2/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 119ms/step - accuracy: 0.2412 - loss: 2.0484\n",
      "Epoch 3/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - accuracy: 0.6398 - loss: 1.5418\n",
      "Epoch 4/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 95ms/step - accuracy: 0.6749 - loss: 1.2141\n",
      "Epoch 5/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 88ms/step - accuracy: 0.6957 - loss: 1.1185\n",
      "Epoch 6/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 113ms/step - accuracy: 0.6749 - loss: 0.8579\n",
      "Epoch 7/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 73ms/step - accuracy: 0.7763 - loss: 0.7110\n",
      "Epoch 8/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 74ms/step - accuracy: 0.8427 - loss: 0.6764\n",
      "Epoch 9/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 144ms/step - accuracy: 0.8251 - loss: 0.5841\n",
      "Epoch 10/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 76ms/step - accuracy: 0.8602 - loss: 0.5064\n",
      "Epoch 11/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 77ms/step - accuracy: 0.8986 - loss: 0.4831\n",
      "Epoch 12/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step - accuracy: 0.8427 - loss: 0.4070 \n",
      "Epoch 13/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - accuracy: 0.8706 - loss: 0.3730\n",
      "Epoch 14/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - accuracy: 0.8706 - loss: 0.4092\n",
      "Epoch 15/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 71ms/step - accuracy: 0.8531 - loss: 0.4642\n",
      "Epoch 16/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - accuracy: 0.8986 - loss: 0.3327\n",
      "Epoch 17/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step - accuracy: 0.8986 - loss: 0.3237 \n",
      "Epoch 18/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step - accuracy: 0.8882 - loss: 0.2791 \n",
      "Epoch 19/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step - accuracy: 0.9545 - loss: 0.3061 \n",
      "Epoch 20/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 71ms/step - accuracy: 0.9545 - loss: 0.2652\n",
      "Epoch 21/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step - accuracy: 0.9720 - loss: 0.2166 \n",
      "Epoch 22/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 76ms/step - accuracy: 0.9161 - loss: 0.1911\n",
      "Epoch 23/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step - accuracy: 0.8706 - loss: 0.3015 \n",
      "Epoch 24/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step - accuracy: 0.8706 - loss: 0.2515 \n",
      "Epoch 25/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step - accuracy: 0.9545 - loss: 0.2073 \n",
      "Epoch 26/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step - accuracy: 0.9545 - loss: 0.2106 \n",
      "Epoch 27/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - accuracy: 0.9441 - loss: 0.2118\n",
      "Epoch 28/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step - accuracy: 0.9441 - loss: 0.2195 \n",
      "Epoch 29/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - accuracy: 0.9441 - loss: 0.1649\n",
      "Epoch 30/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - accuracy: 0.9441 - loss: 0.1560\n",
      "Epoch 31/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step - accuracy: 0.9441 - loss: 0.1370 \n",
      "Epoch 32/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - accuracy: 0.9265 - loss: 0.1617\n",
      "Epoch 33/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 211ms/step - accuracy: 0.9441 - loss: 0.1327\n",
      "Epoch 34/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - accuracy: 0.9825 - loss: 0.1883\n",
      "Epoch 35/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - accuracy: 0.9720 - loss: 0.1460\n",
      "Epoch 36/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step - accuracy: 0.9720 - loss: 0.1005 \n",
      "Epoch 37/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step - accuracy: 0.9720 - loss: 0.1193 \n",
      "Epoch 38/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step - accuracy: 0.9720 - loss: 0.1228 \n",
      "Epoch 39/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - accuracy: 0.9720 - loss: 0.0884 \n",
      "Epoch 40/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step - accuracy: 0.9720 - loss: 0.0971\n",
      "Epoch 41/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step - accuracy: 0.9720 - loss: 0.0875 \n",
      "Epoch 42/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 54ms/step - accuracy: 0.9720 - loss: 0.1060 \n",
      "Epoch 43/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step - accuracy: 0.9720 - loss: 0.0953 \n",
      "Epoch 44/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step - accuracy: 0.9720 - loss: 0.0882 \n",
      "Epoch 45/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step - accuracy: 0.9720 - loss: 0.0702 \n",
      "Epoch 46/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step - accuracy: 0.9825 - loss: 0.1252 \n",
      "Epoch 47/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step - accuracy: 0.9545 - loss: 0.1211 \n",
      "Epoch 48/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step - accuracy: 0.9720 - loss: 0.0634 \n",
      "Epoch 49/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 54ms/step - accuracy: 0.9720 - loss: 0.0573 \n",
      "Epoch 50/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step - accuracy: 0.9720 - loss: 0.0613 \n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "model.train(X_train, y_train, epochs = 50)\n",
    "# model.train_batch(X_train, y_train, epochs = 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Evaluation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preparing Validation data to be passed into the `model.evaluate()`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "testing on a given sentence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Predict the diacritics of the validation data\n",
    "# target_text = sentences[0]\n",
    "# _, original_diacritics = character_encoding.remove_diacritics(target_text, is_sentence=True)\n",
    "# sentence_test = sentences_without_diacritics[0]\n",
    "\n",
    "# sentences = utils.split_data_to_sentences(sentence_test)\n",
    "# # get one hot encoding for each character in sentence_to_test\n",
    "# x_test = []\n",
    "# batch_concatinated_vector_train = []\n",
    "# for i,sentence in enumerate([sentence_test]):\n",
    "#     sentence_vec = []\n",
    "#     words = split_data_to_words(sentence)\n",
    "#     for j, word in enumerate(words):\n",
    "#         try:\n",
    "#             word_vec = embedding_model.vector(word)\n",
    "#         except:\n",
    "#             print(\"word not found : \\\"\", word , \"\\\"\")\n",
    "        \n",
    "#         for k,c in enumerate(word):\n",
    "#             one_hot = character_encoding.CharToOneHOt(c)\n",
    "#             v = np.concatenate((word_vec, one_hot), axis=None)\n",
    "#             sentence_vec.append(v)\n",
    "\n",
    "#         # add space between words except for the last word\n",
    "#         if j != len(words) - 1:    \n",
    "#             one_hot = character_encoding.CharToOneHOt(' ')\n",
    "#             v = np.concatenate((word_vec, one_hot), axis = None)\n",
    "#             sentence_vec.append(v)\n",
    "#     batch_concatinated_vector_train.append(sentence_vec)\n",
    "\n",
    "# x_test = utils.padding(batch_concatinated_vector_train)\n",
    "# # print(\"x_test : \", x_test)\n",
    "# x_test = np.array(x_test)\n",
    "# print(\"x_test len : \", len(sentence_test))\n",
    "# print(\"X_test size : \", x_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_test len :  38\n",
      "X_test size :  (38, 48)\n"
     ]
    }
   ],
   "source": [
    "# Predict the diacritics of the validation data\n",
    "target_text = sentences[0]\n",
    "_, original_diacritics = character_encoding.remove_diacritics(target_text, is_sentence=True)\n",
    "sentence_test = sentences_without_diacritics[0]\n",
    "\n",
    "sentences = utils.split_data_to_sentences(sentence_test)\n",
    "# get one hot encoding for each character in sentence_to_test\n",
    "x_test = []\n",
    "for i,sentence in enumerate([sentence_test]):\n",
    "    words = split_data_to_words(sentence)\n",
    "    for j, word in enumerate(words):\n",
    "        try:\n",
    "            word_vec = embedding_model.vector(word)\n",
    "        except:\n",
    "            print(\"word not found : \\\"\", word , \"\\\"\")\n",
    "        \n",
    "        for k,c in enumerate(word):\n",
    "            one_hot = character_encoding.CharToOneHOt(c)\n",
    "            v = np.concatenate((word_vec, one_hot), axis=None)\n",
    "            x_test.append(v)\n",
    "\n",
    "        # add space between words except for the last word\n",
    "        if j != len(words) - 1:    \n",
    "            one_hot = character_encoding.CharToOneHOt(' ')\n",
    "            v = np.concatenate((word_vec, one_hot), axis = None)\n",
    "            x_test.append(v) \n",
    "\n",
    "# print(\"x_test : \", x_test)\n",
    "x_test = np.array(x_test)\n",
    "print(\"x_test len : \", len(sentence_test))\n",
    "print(\"X_test size : \", x_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2s/step\n"
     ]
    }
   ],
   "source": [
    "y_pred = model.predict(x_test)\n",
    "\n",
    "# # Make predictions at each timestamp\n",
    "# y_pred = []\n",
    "\n",
    "# # Iterate over each timestamp\n",
    "# for i in range(len(sentence_test)):\n",
    "#     # Get the prediction at the current timestamp\n",
    "#     predictions = model.predict(x_test[:, :i+1])\n",
    "#     y_pred.append(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X :  قوله أو قطع الأول يده إلخ قال الزركشي \n",
      "Len X  :  38\n",
      "Len Y predicted  :  38\n",
      "Original Text :  قَوْلُهُ أَوْ قَطَعَ الْأَوَّلُ يَدَهُ إلَخْ قَالَ الزَّرْكَشِيُّ \n",
      "Restored Text :  قَوْلُهُ أَوْ قَطَعَ الُأَوَّلُ يَدَهُ إلَخْ قَالَ الزَّرْكَشِيُّ \n"
     ]
    }
   ],
   "source": [
    "# something is wrong here\n",
    "print(\"X : \", sentence_test)\n",
    "print(\"Len X  : \", len(sentence_test))\n",
    "print(\"Len Y predicted  : \", len(y_pred))\n",
    "\n",
    "\n",
    "predicted_diacritics = []\n",
    "for i in range(len(y_pred)):\n",
    "    predicted_diacritics.append(character_encoding.DIACRITICS[np.argmax(y_pred[i])])\n",
    "\n",
    "# Print the predicted diacritics\n",
    "restored_text = character_encoding.restore_diacritics(sentence_test, predicted_diacritics)\n",
    "print(\"Original Text : \", target_text)\n",
    "print(\"Restored Text : \", restored_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original diacritics  :  ['FATHA', 'SUKUN', 'DAMMA', 'DAMMA', ' ', 'FATHA', 'SUKUN', ' ', 'FATHA', 'FATHA', 'FATHA', ' ', ' ', 'SUKUN', 'FATHA', 'SHADDA_FATHA', 'DAMMA', ' ', 'FATHA', 'FATHA', 'DAMMA', ' ', ' ', 'FATHA', 'SUKUN', ' ', 'FATHA', ' ', 'FATHA', ' ', ' ', ' ', 'SHADDA_FATHA', 'SUKUN', 'FATHA', 'KASRA', 'SHADDA_DAMMA', ' ']\n",
      "Predicted diacritics :  ['FATHA', 'SUKUN', 'DAMMA', 'DAMMA', ' ', 'FATHA', 'SUKUN', ' ', 'FATHA', 'FATHA', 'FATHA', ' ', ' ', 'DAMMA', 'FATHA', 'SHADDA_FATHA', 'DAMMA', ' ', 'FATHA', 'FATHA', 'DAMMA', ' ', ' ', 'FATHA', 'SUKUN', ' ', 'FATHA', ' ', 'FATHA', ' ', ' ', ' ', 'SHADDA_FATHA', 'SUKUN', 'FATHA', 'KASRA', 'SHADDA_DAMMA', ' ']\n"
     ]
    }
   ],
   "source": [
    "print(\"Original diacritics  : \", character_encoding.map_text_to_diacritic(original_diacritics))\n",
    "print(\"Predicted diacritics : \", character_encoding.map_text_to_diacritic(predicted_diacritics))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "def diacritics_error_rate(original_diacritics, predicted_diacritics):\n",
    "    error = 0\n",
    "    for i in range(len(original_diacritics)):\n",
    "        if original_diacritics[i] != predicted_diacritics[i]:\n",
    "            error += 1\n",
    "    return error / len(original_diacritics) * 100, error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Diacritic Error Rate =  2.631578947368421 %\n",
      "Diacritic Correct Rate =  97.36842105263158 %\n",
      "Number of Misclassified =  1 out of 38\n"
     ]
    }
   ],
   "source": [
    "diacritic_error_rate, number_of_mis_classified = diacritics_error_rate(original_diacritics, predicted_diacritics)\n",
    "print(\"Diacritic Error Rate = \", diacritic_error_rate, \"%\")\n",
    "print(\"Diacritic Correct Rate = \", 100 - diacritic_error_rate, \"%\")\n",
    "print(\"Number of Misclassified = \", number_of_mis_classified, \"out of\", len(original_diacritics))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
