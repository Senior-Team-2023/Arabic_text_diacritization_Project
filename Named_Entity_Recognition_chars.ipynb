{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":28877,"status":"ok","timestamp":1703090226499,"user":{"displayName":"Peter Atef Fathi Zaki","userId":"08833456164249872975"},"user_tz":-120},"id":"4FmiJ0pSimk2","outputId":"8b491507-07f6-4ceb-dad5-87bc5a581374"},"outputs":[],"source":["# from google.colab import drive\n","# drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":267,"status":"ok","timestamp":1703090233330,"user":{"displayName":"Peter Atef Fathi Zaki","userId":"08833456164249872975"},"user_tz":-120},"id":"8Lgy0bfSiqLu","outputId":"f74611e2-8815-4573-9063-71fa3ed1ab91"},"outputs":[],"source":["# %cd ./drive/MyDrive/Colab\\ Notebooks/NLP_Project/"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":4186,"status":"ok","timestamp":1703090238634,"user":{"displayName":"Peter Atef Fathi Zaki","userId":"08833456164249872975"},"user_tz":-120},"id":"WR6a6DkN0d-3"},"outputs":[],"source":["import os\n","import numpy as np\n","import pandas as pd\n","from tqdm import tqdm\n","import torch\n","from torch import nn\n","import random as rnd"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":10,"status":"ok","timestamp":1703090238635,"user":{"displayName":"Peter Atef Fathi Zaki","userId":"08833456164249872975"},"user_tz":-120},"id":"jS2AXWkhigQt"},"outputs":[],"source":["def get_vocab(vocab_path, tags_path):\n","    vocab = {}\n","    with open(vocab_path) as f:\n","        for i, l in enumerate(f.read().splitlines()):\n","            vocab[l] = i  # to avoid the 0\n","        # loading tags (we require this to map tags to their indices)\n","    vocab['<PAD>'] = len(vocab) # 35180\n","    tag_map = {}\n","    with open(tags_path) as f:\n","        for i, t in enumerate(f.read().splitlines()):\n","            tag_map[t] = i\n","\n","    return vocab, tag_map\n","\n","def get_params(vocab, tag_map, sentences_file, labels_file):\n","    sentences = []\n","    labels = []\n","\n","    with open(sentences_file) as f:\n","        for sentence in f.read().splitlines():\n","            # replace each token by its index if it is in vocab\n","            # else use index of UNK_WORD\n","            s = [vocab[token] if token in vocab\n","                 else vocab['UNK']\n","                 for token in sentence.split(' ')]\n","            sentences.append(s)\n","\n","    with open(labels_file) as f:\n","        for sentence in f.read().splitlines():\n","            # replace each label by its index\n","            s = sentence.split(' ')\n","            # remove empty strings\n","            s = list(filter(None, s))\n","            l = [tag_map[label] for label in s] # I added plus 1 here\n","            labels.append(l)\n","    return sentences, labels, len(sentences)\n"]},{"cell_type":"markdown","metadata":{"id":"_44BK5K82YwF"},"source":["# Importing and discovering the data"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":4171,"status":"ok","timestamp":1703090407825,"user":{"displayName":"Peter Atef Fathi Zaki","userId":"08833456164249872975"},"user_tz":-120},"id":"ulSik2Sv1p1G"},"outputs":[],"source":["vocab, tag_map = get_vocab('./Dataset/characters/unique_chars.txt', './Dataset/characters/unique_labels.txt')\n","t_sentences, t_labels, t_size = get_params(vocab, tag_map, './Dataset/characters/t_chars.txt', './Dataset/characters/t_labels.txt')\n","v_sentences, v_labels, v_size = get_params(vocab, tag_map, './Dataset/characters/v_chars.txt', './Dataset/characters/v_labels.txt')\n","test_sentences, test_labels, test_size = get_params(vocab, tag_map, './Dataset/characters/test_chars.txt', './Dataset/characters/test_labels.txt')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":556,"status":"ok","timestamp":1703090412253,"user":{"displayName":"Peter Atef Fathi Zaki","userId":"08833456164249872975"},"user_tz":-120},"id":"IB_1MhtP1rLL","outputId":"ed950913-5a9a-4886-cbf1-e23d3e79ceed"},"outputs":[],"source":["# vocab translates from a word to a unique number\n","print('vocab[\"الأعم\"]:', vocab[\"الأعم\"])\n","# Pad token\n","print('padded token:', vocab['<PAD>'])"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":7,"status":"ok","timestamp":1703090412701,"user":{"displayName":"Peter Atef Fathi Zaki","userId":"08833456164249872975"},"user_tz":-120},"id":"PagjN4rl22Fr","outputId":"5023e281-9467-45ad-865a-3db4b34948ec"},"outputs":[],"source":["# The possible tags\n","print(tag_map)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":259,"status":"ok","timestamp":1703090415506,"user":{"displayName":"Peter Atef Fathi Zaki","userId":"08833456164249872975"},"user_tz":-120},"id":"oWLR2Oxp28K6","outputId":"4ae33707-b1be-40ca-96fa-514b447cf622"},"outputs":[],"source":["# Exploring information about the data\n","print('The number of outputs is tag_map', len(tag_map))\n","# The number of vocabulary tokens (including <PAD>)\n","g_vocab_size = len(vocab)\n","print(f\"Num of vocabulary words: {g_vocab_size}\")\n","print('The vocab size is', len(vocab))\n","print('The training size is', t_size)\n","print('The validation size is', v_size)\n","print('An example of the first sentence is', t_sentences[0])\n","print('An example of its corresponding label is', t_labels[0])\n","len(t_sentences[0])==len( t_labels[0])"]},{"cell_type":"markdown","metadata":{"id":"wt3e4nxjFT3O"},"source":["# NERDataset\n","The class that impelements the dataset for NER"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":365,"status":"ok","timestamp":1703090418575,"user":{"displayName":"Peter Atef Fathi Zaki","userId":"08833456164249872975"},"user_tz":-120},"id":"29iM0u4-4YOV"},"outputs":[],"source":["class NERDataset(torch.utils.data.Dataset):\n","\n","  def __init__(self, x, y, pad):\n","    \"\"\"\n","    This is the constructor of the NERDataset\n","    Inputs:\n","    - x: a list of lists where each list contains the ids of the tokens\n","    - y: a list of lists where each list contains the label of each token in the sentence\n","    - pad: the id of the <PAD> token (to be used for padding all sentences and labels to have the same length)\n","    \"\"\"\n","    #####################  create two tensors one for x and the other for labels ###############################\n","    self.x = nn.utils.rnn.pad_sequence([torch.tensor(i) for i in x], padding_value=pad,batch_first = True)\n","    self.y = nn.utils.rnn.pad_sequence([torch.tensor(i) for i in y], padding_value=10,batch_first = True)\n","    #################################################################################################################\n","    # print the max length of the sentences\n","    print('The max length of the sentences is', self.x.shape[1])\n","    print('The max length of the labels is', self.y.shape[1])\n","  def __len__(self):\n","    \"\"\"\n","    This function should return the length of the dataset (the number of sentences)\n","    \"\"\"\n","    ######################  return the length of the dataset #############################\n","    return len(self.x)\n","    ###########################################################################################\n","\n","  def __getitem__(self, idx):\n","    \"\"\"\n","    This function returns a subset of the whole dataset\n","    \"\"\"\n","    ######################  return a tuple of x and y ###################################\n","    return self.x[idx], self.y[idx]\n","    ##########################################################################################"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4,"status":"ok","timestamp":1703090418950,"user":{"displayName":"Peter Atef Fathi Zaki","userId":"08833456164249872975"},"user_tz":-120},"id":"sz-saCtRs7Pz","outputId":"0c9330f1-8fcb-4bc5-92cf-d3c78c45c920"},"outputs":[],"source":["batch_size = 5\n","mini_sentences = t_sentences[0: 8]\n","mini_labels = t_labels[0: 8]\n","# print(mini_labels)\n","mini_dataset = NERDataset(mini_sentences, mini_labels, vocab['<PAD>'])\n","dummy_dataloader = torch.utils.data.DataLoader(mini_dataset, batch_size=5)\n","dg = iter(dummy_dataloader)\n","X1, Y1 = next(dg)\n","X2, Y2 = next(dg)\n","print(Y1.shape, X1.shape, Y2.shape, X2.shape)\n","print(X1[0][:], \"\\n\", Y1[0][:])"]},{"cell_type":"markdown","metadata":{"id":"CQB6O7I7FbUh"},"source":["# NER\n","The class that implementss the pytorch model for NER"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":1032,"status":"ok","timestamp":1703090422533,"user":{"displayName":"Peter Atef Fathi Zaki","userId":"08833456164249872975"},"user_tz":-120},"id":"xHeJcz1JuhYa"},"outputs":[],"source":["class NER(nn.Module):\n","  def __init__(self, vocab_size=len(t_sentences) + len(test_sentences) + len(v_sentences), embedding_dim=300, hidden_size=50, n_classes=len(tag_map)):\n","    \"\"\"\n","    The constructor of our NER model\n","    Inputs:\n","    - vacab_size: the number of unique words\n","    - embedding_dim: the embedding dimension\n","    - n_classes: the number of final classes (tags)\n","    \"\"\"\n","    super(NER, self).__init__()\n","    #######################  Create the layers of your model #######################################\n","    # (1) Create the embedding layer\n","    self.embedding = nn.Embedding(vocab_size, embedding_dim)\n","\n","    # (2) Create an LSTM layer with hidden size = hidden_size and batch_first = True\n","    self.lstm = nn.LSTM(embedding_dim, hidden_size, batch_first=True)\n","\n","    # (3) Create a linear layer with number of neorons = n_classes\n","    self.linear = nn.Linear(hidden_size, n_classes)\n","    #####################################################################################################\n","\n","  def forward(self, sentences):\n","    \"\"\"\n","    This function does the forward pass of our model\n","    Inputs:\n","    - sentences: tensor of shape (batch_size, max_length)\n","\n","    Returns:\n","    - final_output: tensor of shape (batch_size, max_length, n_classes)\n","    \"\"\"\n","\n","    final_output = None\n","    #########################  implement the forward pass ####################################\n","    embeddings = self.embedding(sentences)\n","    lstm_out, (a,b) = self.lstm(embeddings)\n","    # print(\"lstm_out.size\",lstm_out.size())\n","    # print(\"a.size\",a.size())\n","    # print(\"b.size\",b.size())\n","    final_output = self.linear(lstm_out)\n","    ###############################################################################################\n","    return final_output"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":351,"status":"ok","timestamp":1703090423267,"user":{"displayName":"Peter Atef Fathi Zaki","userId":"08833456164249872975"},"user_tz":-120},"id":"lJJJF-qQA_wk","outputId":"8b42fc31-7345-45a5-f676-fa3d489c9f5a"},"outputs":[],"source":["model = NER()\n","print(model)"]},{"cell_type":"markdown","metadata":{"id":"PLHx_oHpFlSX"},"source":["# Training"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":4,"status":"ok","timestamp":1703090423268,"user":{"displayName":"Peter Atef Fathi Zaki","userId":"08833456164249872975"},"user_tz":-120},"id":"-yvaq8i2CCLD"},"outputs":[],"source":["def train(model, train_dataset, batch_size=512, epochs=5, learning_rate=0.01):\n","  \"\"\"\n","  This function implements the training logic\n","  Inputs:\n","  - model: the model ot be trained\n","  - train_dataset: the training set of type NERDataset\n","  - batch_size: integer represents the number of examples per step\n","  - epochs: integer represents the total number of epochs (full training pass)\n","  - learning_rate: the learning rate to be used by the optimizer\n","  \"\"\"\n","\n","  ##############################  replace the Nones in the following code ##################################\n","\n","  # (1) create the dataloader of the training set (make the shuffle=True)\n","  train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n","\n","  # (2) make the criterion cross entropy loss\n","  criterion = torch.nn.CrossEntropyLoss()\n","\n","  # (3) create the optimizer (Adam)\n","  optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n","\n","  # GPU configuration\n","  use_cuda = torch.cuda.is_available()\n","  device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n","  if use_cuda:\n","    model = model.cuda()\n","    criterion = criterion.cuda()\n","\n","  for epoch_num in range(epochs):\n","    total_acc_train = 0\n","    total_loss_train = 0\n","\n","    for train_input, train_label in tqdm(train_dataloader):\n","\n","      # (4) move the train input to the device\n","      train_label = train_label.to(device)\n","\n","      # (5) move the train label to the device\n","      train_input = train_input.to(device)\n","\n","\n","      # (6) do the forward pass\n","      output = model(train_input)\n","      print(\"output.shape\",output.shape)\n","      print(\"train_label.shape\",train_label.shape)\n","      # (7) loss calculation (you need to think in this part how to calculate the loss correctly)\n","      batch_loss = criterion(output.view(-1, output.shape[-1]), train_label.view(-1))\n","\n","      # (8) append the batch loss to the total_loss_train\n","      total_loss_train += batch_loss\n","\n","      # (9) calculate the batch accuracy (just add the number of correct predictions)\n","      acc = (output.argmax(2) == train_label).sum().item()\n","\n","      total_acc_train += acc\n","\n","      # (10) zero your gradients\n","      optimizer.zero_grad()\n","\n","\n","      # (11) do the backward pass\n","      batch_loss.backward()\n","\n","\n","      # (12) update the weights with your optimizer\n","      optimizer.step()\n","\n","    # epoch loss\n","    epoch_loss = total_loss_train / len(train_dataset)\n","\n","    # (13) calculate the accuracy\n","    epoch_acc = total_acc_train / (len(train_dataset) * train_dataset[0][0].shape[0])\n","    print(\n","        f'Epochs: {epoch_num + 1} | Train Loss: {epoch_loss} \\\n","        | Train Accuracy: {epoch_acc}\\n')\n","\n","  ##############################################################################################################"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4552,"status":"ok","timestamp":1703090427815,"user":{"displayName":"Peter Atef Fathi Zaki","userId":"08833456164249872975"},"user_tz":-120},"id":"3BI7_ANkLf7G","outputId":"13b61d5a-dbd5-428f-8241-77433d6e5ae3"},"outputs":[],"source":["train_dataset = NERDataset(t_sentences, t_labels, vocab['<PAD>'])\n","val_dataset = NERDataset(v_sentences, v_labels, vocab['<PAD>'])\n","test_dataset = NERDataset(test_sentences, test_labels, vocab['<PAD>'])"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":93209,"status":"ok","timestamp":1703090521018,"user":{"displayName":"Peter Atef Fathi Zaki","userId":"08833456164249872975"},"user_tz":-120},"id":"LMXjDv51LU6k","outputId":"3ba48f78-6f2d-4ce2-ea70-c1abcacda1e0"},"outputs":[],"source":["train(model, train_dataset)"]},{"cell_type":"markdown","metadata":{"id":"TWJNO6mUXPRI"},"source":["# Evaluation"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":8,"status":"ok","timestamp":1703090521309,"user":{"displayName":"Peter Atef Fathi Zaki","userId":"08833456164249872975"},"user_tz":-120},"id":"Gz5mxUAJM1xS"},"outputs":[],"source":["def evaluate(model, test_dataset, batch_size=512):\n","  \"\"\"\n","  This function takes a NER model and evaluates its performance (accuracy) on a test data\n","  Inputs:\n","  - model: a NER model\n","  - test_dataset: dataset of type NERDataset\n","  \"\"\"\n","  ###########################  Replace the Nones in the following code ##########################\n","\n","  # (1) create the test data loader\n","  test_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size)\n","\n","  # GPU Configuration\n","  use_cuda = torch.cuda.is_available()\n","  device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n","  if use_cuda:\n","    model = model.cuda()\n","\n","  total_acc_test = 0\n","\n","  # (2) disable gradients\n","  with torch.no_grad():\n","\n","    for test_input, test_label in tqdm(test_dataloader):\n","      # (3) move the test input to the device\n","      test_label = test_label.to(device)\n","\n","      # (4) move the test label to the device\n","      test_input = test_input.to(device)\n","\n","      # (5) do the forward pass\n","      output = model(test_input)\n","\n","      # accuracy calculation (just add the correct predicted items to total_acc_test)\n","      acc = (output.argmax(2) == test_label).sum().item()\n","      total_acc_test += acc\n","\n","    # (6) calculate the over all accuracy\n","    total_acc_test /= (len(test_dataset) * test_dataset[0][0].shape[0])\n","  ##################################################################################################\n","\n","\n","  print(f'\\nTest Accuracy: {total_acc_test}')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":7,"status":"ok","timestamp":1703090521309,"user":{"displayName":"Peter Atef Fathi Zaki","userId":"08833456164249872975"},"user_tz":-120},"id":"6FD8JNcHWmMY","outputId":"9b51d9bf-bd94-40af-b0ac-461ecc1751c8"},"outputs":[],"source":["evaluate(model, test_dataset)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["torch.save(model.state_dict(), f'./SavedModels/model_lstm_linearNN')"]}],"metadata":{"accelerator":"GPU","colab":{"provenance":[]},"gpuClass":"standard","kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.5"}},"nbformat":4,"nbformat_minor":0}
